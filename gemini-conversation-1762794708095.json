[
  {
    "role": "user",
    "parts": [
      {
        "text": "This is the Gemini CLI. We are setting up the context for our chat.\nToday's date is Monday, November 10, 2025 (formatted according to the user's locale).\nMy operating system is: linux\nI'm currently working in the directory: /home/diannenewton170/formation-sat-2\nHere is the folder structure of the current working directories:\n\nShowing up to 200 items (files + folders). Folders or files indicated with ... contain more items not shown, were ignored, or the display limit (200 items) was reached.\n\n/home/diannenewton170/formation-sat-2/\n├───.env\n├───.gitattributes\n├───.gitignore\n├───AGENTS.md\n├───artefacts-documentation.md\n├───debug.txt\n├───determining_orbital_elements_for_repeat_ground_track_orbits.md\n├───GEMINI.md\n├───Makefile\n├───proposal.md\n├───pyproject.toml\n├───README.md\n├───requirements.txt\n├───run_debug.py\n├───run.py\n├───.git/...\n├───.github/\n│   └───workflows/\n│       ├───ci.yml\n│       ├───gemini-dispatch.yml\n│       ├───gemini-invoke.yml\n│       ├───gemini-review.yml\n│       ├───gemini-scheduled-triage.yml\n│       └───gemini-triage.yml\n├───.venv/...\n├───artefacts/\n│   ├───conops_baseline/\n│   │   ├───conops_link.md\n│   │   └───README.md\n│   ├───debug/\n│   │   ├───run_20250930_1718Z.md\n│   │   ├───run_20250930_1833Z.md\n│   │   ├───run_20250930_1858Z.md\n│   │   ├───20250930T171815Z/\n│   │   ├───20250930T171818Z/\n│   │   ├───20250930T183303Z/\n│   │   ├───20250930T183307Z/\n│   │   ├───20250930T185811Z/\n│   │   ├───20250930T185815Z/\n│   │   ├───20251103T152521Z/\n│   │   ├───20251103T152745Z/\n│   │   ├───20251103T152938Z/\n│   │   ├───20251103T194049Z/\n│   │   ├───20251105T193220Z/\n│   │   └───20251110T143857Z/\n│   ├───example_stk_export/\n│   │   ├───Sat1_Chief.sa\n│   │   ├───Sat1_Chief.sa3\n│   │   ├───Sat1_Cone.sn\n│   │   ├───Sat1_Cone.sn3\n│   │   ├───Sat2_Cone.sn\n│   │   ├───Sat2_Cone.sn3\n│   │   ├───Sat2_Dep_A.sa\n│   │   ├───Sat2_Dep_A.sa3\n│   │   ├───Sat3_Cone.sn\n│   │   ├───Sat3_Cone.sn3\n│   │   ├───Sat3_Dep_B.e\n│   │   ├───Sat3_Dep_B.sa\n│   │   ├───Sat3_Dep_B.sa3\n│   │   ├───Tehran_Target.f\n│   │   ├───Tehran_Target.f3\n│   │   ├───Tehran_Transient_Triangle STK ScenarioWB.wsp\n│   │   ├───Tehran_Transient_Triangle STK ScenarioWB.wsptb\n│   │   ├───Tehran_Transient_Triangle.atmosphere.flight\n│   │   ├───Tehran_Transient_Triangle.atmosphere.guid\n│   │   ├───Tehran_Transient_Triangle.sc\n│   │   ├───Tehran_Transient_Triangle.sc3\n│   │   ├───Tehran_Transient_Triangle.sca\n│   │   ├───Tehran_Transient_Triangle.SQR.flight\n│   │   ├───Tehran_Transient_Triangle.SQR.guid\n│   │   ├───Tehran_Transient_Triangle.wind.flight\n│   │   ├───Tehran_Transient_Triangle.wind.guid\n│   │   └───Tehran_Transient_TriangleTimelinePrefs.xml\n│   ├───gemini_run_14day_rgt_verification/\n│   │   ├───altitudes_m.csv\n│   │   ├───command_windows.csv\n│   │   ├───drag_dispersion.csv\n│   │   ├───formation_windows.csv\n│   │   ├───ground_ranges.csv\n│   │   ├───injection_recovery_cdf.svg\n│   │   ├───injection_recovery.csv\n│   │   ├───latitudes_rad.csv\n│   │   ├───...\n│   │   └───...\n│   ├───gemini_run_14day_rgt_verification_corrected/\n│   ├───gemini_run_20251106/\n│   ├───my_triangle_run/\n│   ├───run_20251018_1207Z/\n│   ├───run_20251018_1308Z_tehran_daily_pass/\n│   ├───run_20251018_1345Z/\n│   ├───run_20251018_1424Z/\n│   ├───run_20251020_0813Z_tehran_daily_pass/\n│   ├───run_20251020_1900Z_tehran_daily_pass_locked/\n│   ├───run_20251101_0803Z/\n│   ├───run_20251109_0638Z/\n│   ├───run_20251109_0719Z/\n│   ├───run_20251109_0916Z/\n│   ├───run_20260321_0740Z/\n│   ├───run_20260321_0740Z_tehran_daily_pass_locked/\n│   ├───run_20260321_0740Z_tehran_daily_pass_resampled/\n│   ├───sweeps/\n│   ├───triangle/\n│   ├───triangle_campaign/\n│   ├───triangle_run/\n│   └───web_runs/\n├───config/\n│   ├───project.yaml\n│   ├───README.md\n│   └───scenarios/\n├───data/\n│   └───tehran_boundary.geojson\n├───docs/\n│   ├───_authoritative_runs.md\n│   ├───AGENTS.md\n│   ├───automated_plotting.md\n│   ├───compliance_matrix.md\n│   ├───concept_of_operations.md\n│   ├───final_delivery_manifest.md\n│   ├───how_to_import_tehran_daily_pass_into_stk.md\n│   ├───how-it-works.md\n│   ├───interactive_execution_guide.md\n│   ├───long_duration_ground_track_generation.md\n│   ├───mission_requirements.md\n│   ├───project_overview.md\n│   ├───project_roadmap.md\n│   ├───rgt_orbit_report.md\n│   ├───run_20251020_daily_pass_locked_report.md\n│   ├───run_20251101_0803Z_triangle_report.md\n│   ├───run_execution_modes.md\n│   ├───stk_export.md\n│   ├───system_requirements.md\n│   ├───tehran_daily_pass_scenario.md\n│   ├───tehran_triangle_walkthrough.md\n│   ├───triangle_formation_results.md\n│   ├───verification_plan.md\n│   └───web_interface.md\n├───final/\n│   ├───PROMPTS/\n│   └───RESULTS/\n├───formation_sat_tools.egg-info/\n│   ├───dependency_links.txt\n│   ├───PKG-INFO\n│   ├───requires.txt\n│   ├───SOURCES.txt\n│   └───top_level.txt\n├───formation-sat-2/\n│   ├───.env\n│   ├───.gitattributes\n│   ├───.gitignore\n│   ├───AGENTS.md\n│   ├───artefacts-documentation.md\n│   ├───determining_orbital_elements_for_repeat_ground_track_orbits.md\n│   ├───GEMINI.md\n│   ├───Makefile\n│   ├───proposal.md\n│   ├───pyproject.toml\n│   ├───README.md\n│   ├───requirements.txt\n│   ├───run_debug.py\n│   ├───run.py\n│   ├───.git/...\n│   ├───.github/\n│   ├───artefacts/\n│   ├───config/\n│   ├───data/\n│   ├───docs/\n│   ├───final/\n│   ├───formation_sat_tools.egg-info/\n│   ├───sim/\n│   ├───src/\n│   ├───tests/\n│   └───tools/\n├───sim/\n│   ├───formation/\n│   └───scripts/\n├───src/\n│   └───constellation/\n├───tests/\n│   ├───baseline_compare.py\n│   ├───conftest.py\n│   ├───README.md\n│   ├───test_documentation_consistency.py\n│   ├───test_orbit_geodetic.py\n│   ├───test_stk_export.py\n│   ├───data/\n│   ├───integration/\n│   └───unit/\n└───tools/\n    ├───__init__.py\n    ├───calculate_rgt_orbit.py\n    ├───generate_docs_summary.py\n    ├───generate_triangle_report.py\n    ├───probe_analysis_interfaces.py\n    ├───propagate_long_duration.py\n    ├───render_debug_plots.py\n    ├───render_ground_track.py\n    ├───render_scenario_formation.py\n    ├───render_scenario_plots.py\n    ├───run_simulation_stub.py\n    ├───stk_export.py\n    ├───stk_tehran_daily_pass_runner.py\n    └───__pycache__/\n\nReminder: Do not return an empty response when a tool call is required.\n\nMy setup is complete. I will provide my first command in the next turn."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "به انگلیسی بهم جواب بده، بخون کل پروژه رو و ببین توی کد هایی که داره ران میشه این انجام شده یا نه؟ باید دو صفحه مداری تکرار پذیر پیدا بشه که روی یکیش دو ماهواره قرار بگیره، و روی دیگری یک ماهواره دیگه قرار بگیره و بالای تهران این سه ماهواره فورمیشن مثلثی تشکیل بد. یعنی سه ماهواره برای مدت حداقل 90 ثانیه بالای تهران روزانه فورمیشن فلای مثلثی داشته باشن"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Of course. I will help you with that. I will start by analyzing the project's documentation and code to determine if the scenario you described is already implemented.\n\nBased on my initial review of the project overview, the project aims to model a triangular satellite formation over Tehran. I will now investigate the specifics of the implementation to see if it matches your requirements.\n\nI will start by examining the documentation to understand the implemented scenarios. I will then look at the simulation code and configuration files to confirm the details.\n\nI will keep you updated on my findings."
      },
      {
        "functionCall": {
          "name": "read_many_files",
          "args": {
            "paths": [
              "docs/tehran_triangle_walkthrough.md",
              "docs/tehran_daily_pass_scenario.md",
              "docs/triangle_formation_results.md"
            ]
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_many_files-1762791186391-34bee9c542a1",
          "name": "read_many_files",
          "response": {
            "output": "Tool execution succeeded."
          }
        }
      },
      {
        "text": "--- /home/diannenewton170/formation-sat-2/docs/tehran_daily_pass_scenario.md ---\n\n# Tehran Daily Pass Scenario Overview\n\n## Context\nThe Tehran daily pass scenario now reflects the right-ascension-of-the-ascending-node (RAAN) alignment recovered by the dedicated solver introduced in the simulation pipeline, ensuring that the constellation centroid remains within \\(\\pm30\\,\\text{km}\\) of the Tehran reference point at the midpoint of the morning imaging opportunity.[Ref1][Ref6] The configuration remains compatible with the Systems Tool Kit (STK 11.2) export workflow so that analysts can continue to regenerate contact catalogues and orbital ephemerides without bespoke tailoring.[Ref2][Ref5] Morning imaging is still paired with an evening downlink window to preserve the operational cadence supporting the infrastructure resilience monitoring mission.\n\n## Configuration Linkage\nThe authoritative machine-readable description held in [`config/scenarios/tehran_daily_pass.json`](../config/scenarios/tehran_daily_pass.json) records the optimised RAAN solution of \\(350.7885044642857^{\\circ}\\) at the \\(2026\\) vernal equinox epoch alongside the refined 07:39:25–07:40:55Z access window.[Ref1] These elements remain tied to the one-day repeat ground-track regime and retain the payload, thermal, and data-handling constraints that feed directly into simulation and onboard scheduling analyses. The metadata block logs the alignment validation run identifier (`run_20251020_1900Z_tehran_daily_pass_locked`) and associated epoch, preserving traceability to the evidence set baselined for compliance oversight.[Ref1][Ref7] The canonical run ledger maintained in [`docs/_authoritative_runs.md`](./_authoritative_runs.md) summarises every referenced identifier so that downstream documents remain synchronised with the authoritative evidence set.\n\nThe archived solver catalogue retains both the pre-optimisation seed (\\(350.9838169642857^{\\circ}\\)) and the converged value cited above so that auditors can see the progression captured in `scenario_summary.json`.[Ref2]\n\n*Note:* Analysts repeating the optimisation should document the recovered RAAN to a precision of \\(10^{-3}\\,\\text{deg}\\). Small deviations from \\(350.7885^{\\circ}\\) arise from epoch selection and \\(J_2\\)-driven nodal drift; any larger discrepancy must be reconciled before the result is deemed configuration-controlled.\n\n## Alignment Solution and Simulation Evidence\nThe locked alignment campaign is archived under `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/`, capturing the deterministic alignment solution and the supporting Monte Carlo catalogue.[Ref2][Ref3][Ref4]\n\n1. Initialise the optimisation sweep using `sim/scripts/run_scenario.py`, which now evaluates candidate RAAN values before executing the high-fidelity propagation with \\(J_2\\) and drag perturbations to select the centroid-minimising configuration.[Ref6]\n2. Propagate the deterministic case across the 90-second morning window, confirming a centroid cross-track magnitude of \\(12.14\\,\\text{km}\\) and a worst-vehicle absolute offset of \\(27.76\\,\\text{km}\\) at 07:40:10Z, comfortably within the \\(\\pm30\\,\\text{km}\\) primary tolerance and the \\(\\pm70\\,\\text{km}\\) waiver ceiling.[Ref3]\n3. Archive the solver settings, deterministic and Monte Carlo CSV catalogues, and refreshed STK exports alongside the run directory so that downstream analyses can reuse the data products without re-running the optimisation campaign.[Ref2]\n\n*Note:* `run_20260321_0740Z_tehran_daily_pass_resampled` remains available as an exploratory rerun that exercised the resampling workflow while reproducing the locked geometry, `stk_export/` ephemeris suite, and Monte Carlo statistics for comparison studies rather than baseline compliance.[Ref8][Ref9]\n\n## STK Validation Status\nThe regenerated STK package embedded within `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/` was exported directly by the scenario runner, yielding ephemeris (`.e`), satellite (`.sat`), ground-track (`.gt`), and contact interval (`.int`) files aligned with the updated geometry.[Ref2] Import testing via the established STK guidance confirmed that the morning imaging and evening downlink windows reproduce the documented UTC spans and that the Tehran/Svalbard facilities retain their expected geodetic coordinates, maintaining the validated-against-export status recorded in the configuration metadata.[Ref2][Ref5]\n\n## Compliance Assessment\nThe deterministic centroid cross-track evidence produced by `run_20251020_1900Z_tehran_daily_pass_locked` closes the MR-2 and SRD-P-001 requirements by demonstrating that the constellation centroid and all vehicles remain within the \\(\\pm30\\,\\text{km}\\) tolerance at the midpoint of the daily access window, with additional headroom beneath the \\(\\pm70\\,\\text{km}\\) waiver ceiling.[Ref3] The accompanying Monte Carlo catalogue sustains 100% primary and waiver compliance while holding the centroid \\(p_{95}\\) magnitude to \\(24.18\\,\\text{km}\\), now exposed directly through the `centroid_abs_cross_track_km_p95` metric to support rapid citation in compliance dossiers.[Ref4] The compliance matrix identifies this run as the authoritative evidence set so that reviewers can immediately confirm provenance during audits.[Ref7]\n\n## Validation Artefacts and Next Steps\n1. Regenerate the STK package or rerun the RAAN alignment process as required by executing `python -m sim.scripts.run_scenario tehran_daily_pass --output-dir artefacts/run_YYYYMMDD_hhmmZ`, ensuring the output directory follows the repository naming convention before committing results.[Ref6]\n2. Use the [STK validation guide](how_to_import_tehran_daily_pass_into_stk.md) to document animation captures, access reports, and quantitative checks for each rerun. Archive SVG evidence alongside the run directory to maintain traceability.[Ref5]\n3. Reference the Monte Carlo `centroid_abs_cross_track_km_p95` field when drafting probabilistic compliance statements so that deterministic and statistical evidence remain aligned during future reviews.[Ref4]\n\n## References\n- [Ref1] `config/scenarios/tehran_daily_pass.json` – Machine-readable Tehran daily pass configuration with RAAN alignment metadata.\n- [Ref2] `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/scenario_summary.json` – Scenario-level metadata, STK export catalogue, and access window timings for the compliant geometry.\n- [Ref3] `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/deterministic_summary.json` – Deterministic centroid cross-track metrics demonstrating compliance with MR-2 and SRD-P-001.\n- [Ref4] `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/monte_carlo_summary.json` – Monte Carlo statistics quantifying centroid and worst-spacecraft dispersion for the locked geometry.\n- [Ref5] `docs/how_to_import_tehran_daily_pass_into_stk.md` – Analyst workflow for STK ingestion and evidence capture.\n- [Ref6] `sim/scripts/run_scenario.py` – Scenario runner integrating the RAAN alignment solver and STK export workflow.\n- [Ref7] `docs/compliance_matrix.md` – Repository compliance ledger designating the authoritative evidence set for MR-2 and SRD-P-001.\n- [Ref8] `artefacts/run_20260321_0740Z_tehran_daily_pass_resampled/scenario_summary.json` – Exploratory rerun capturing the resampled scenario metadata, export catalogue, and access timings.\n- [Ref9] `artefacts/run_20260321_0740Z_tehran_daily_pass_resampled/monte_carlo_summary.json` – Monte Carlo statistics retained for the exploratory resampled dataset.\n\n\n"
      },
      {
        "text": "--- /home/diannenewton170/formation-sat-2/docs/tehran_triangle_walkthrough.md ---\n\n# Tehran Triangle Simulation Walkthrough\n\n## Introduction\nThis memorandum collates the procedural knowledge required to reproduce the Tehran triangular-formation demonstration using the repository toolchain. The intent is to guide analysts from environment preparation through to Systems Tool Kit (STK 11.2) validation, ensuring the verified ninety-second access window can be regenerated without ambiguity.[Ref1]\n\n## Prerequisites\n1. **Establish the Python environment.** Execute `make setup` from the repository root to create the managed virtual environment and install the pinned dependency set declared in `requirements.txt`.\n2. **Review configuration artefacts.** Inspect `config/scenarios/tehran_triangle.json` to confirm the mission epoch, geometric tolerances, and plane assignments align with the intended analysis case.[Ref1]\n3. **Confirm exporter compatibility.** Familiarise yourself with `tools/stk_export.py` to understand the file formats and metadata required by STK 11.2 ingestion workflows.[Ref2]\n\n## Execution Procedure\n1. **Inspect the archived run.** Review the curated `artefacts/triangle_run/` snapshot—mirroring the drag-inclusive rerun `run_20251018_1424Z`—so that `triangle_summary.json`, `maintenance_summary.csv`, `command_windows.csv`, `injection_recovery.csv`, `drag_dispersion.csv`, `injection_recovery_cdf.svg`, and the accompanying `run_metadata.json` ledger refresh the baseline before initiating a fresh propagation.[Ref1]\n2. **Launch the formation runner.** Invoke `python -m sim.scripts.run_triangle --output-dir artefacts/triangle` to propagate the formation, compute geometry metrics, and serialise the results into a new `artefacts/triangle/` directory that mirrors the archived structure.\n3. **Observe command output.** Verify that the terminal log reports a formation window of at least ninety seconds alongside the UTC start and end timestamps for the Tehran overflight.\n4. **Inspect generated artefacts.** Examine the regenerated `artefacts/triangle/triangle_summary.json` (or the committed `artefacts/triangle_run/triangle_summary.json`) together with the curated CSV analytics to confirm the recorded triangle metrics, centroid behaviour, maintenance \\(\\Delta v\\), command latency, injection recovery statistics, and drag dispersion margins remain within the documented tolerances.[Ref1]\n\n## Output Interpretation\n1. **Validate temporal coverage.** Within either the archived or regenerated summary JSON, ensure the `formation_window.duration_s` field exceeds the ninety-second threshold mandated by the mission requirement set.[Ref1]\n2. **Check geometric fidelity.** Review the `triangle.aspect_ratio_max` and `triangle.mean_side_lengths_m` entries to confirm the equilateral geometry is preserved across the access interval.[Ref1]\n3. **Review ground-track alignment.** Inspect the `samples` entries within `triangle_summary.json` whose timestamps fall between the `metrics.formation_window.start` and `metrics.formation_window.end` markers. Confirm that the maximum of those windowed `max_ground_distance_km` values remains below the \\(350\\,\\text{km}\\) tolerance, demonstrating that all three spacecraft remain within the observation corridor above Tehran. The global propagation also exposes a \\(641.89\\,\\text{km}\\) maximum outside the validated window, so capturing the filtered figure in compliance reports prevents ambiguity.[Ref1]\n\n## STK Verification\n1. **Prepare the STK workspace.** Open STK 11.2 and create a new scenario matching the start and stop epochs reported in the JSON summary.\n2. **Import ephemerides and ground tracks.** Load the `.e` and `_groundtrack.gt` files located under `artefacts/triangle_run/stk_export/` (or from the regenerated `artefacts/triangle/stk_export/` directory) for each spacecraft, ensuring the assets are referenced to the TEME frame as documented in the exporter guide.[Ref2]\n3. **Load facilities and contact intervals.** Add the generated `.fac` and `.int` files to visualise the Tehran ground facility and the simultaneous contact window. The interval should match the ninety-second duration established by the Python simulation.[Ref1]\n4. **Cross-check metrics.** Use STK's built-in measurement tools to confirm that the triangle side lengths and centroid altitude match the analytical results to within numerical tolerance.[Ref1]\n\n## RAAN Provenance and Traceability\nThe \\(18.880656^{\\circ}\\) right-ascension-of-the-ascending-node value that surfaces in the triangle artefacts belongs to the validated ninety-second window generated on 21 March 2026 between 09:31:12Z and 09:32:48Z, as archived in both `artefacts/triangle_run/triangle_summary.json` and the reproduced `run_20251018_1207Z` catalogue.[Ref1][Ref3] These files retain the equilateral-formation solution that satisfied the Tehran access and geometry constraints during the dedicated triangle campaign, so the RAAN figure is deliberately preserved for historical replay and Systems Tool Kit ingestion.\n\nBy contrast, the daily pass scenario file `config/scenarios/tehran_daily_pass.json` and its locked alignment artefact `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/scenario_summary.json` document the \\(350.788504^{\\circ}\\) RAAN derived from the repeat-ground-track optimisation conducted later in the programme.[Ref4][Ref5] Keeping both data sets in the repository clarifies that the triangle demonstration remained successful while the daily operations baseline migrated to the refined nodal alignment; analysts selecting inputs for renewed studies should therefore choose the RAAN that matches the mission thread they are reproducing rather than assuming the \\(18.880656^{\\circ}\\) entry represents a failure case.\n\n## References\n- [Ref1] `docs/triangle_formation_results.md`.\n- [Ref2] `docs/stk_export.md`.\n- [Ref3] `artefacts/run_20251018_1207Z/triangle_summary.json` – Triangle run catalogue confirming the 96 s formation window and \\(18.880656^{\\circ}\\) RAAN.\n- [Ref4] `config/scenarios/tehran_daily_pass.json` – Daily pass configuration capturing the \\(350.788504^{\\circ}\\) RAAN used for routine operations.\n- [Ref5] `artefacts/run_20251020_1900Z_tehran_daily_pass_locked/scenario_summary.json` – Locked alignment ledger documenting the optimised RAAN lineage and solver sweep.\n\n\n"
      },
      {
        "text": "--- /home/diannenewton170/formation-sat-2/docs/triangle_formation_results.md ---\n\n# Tehran Triangular Formation Simulation Results\n\n## Introduction\nThis memorandum records the first fully fledged simulation of the three-satellite triangular formation mandated by the mission brief. The analysis centres on a 6 km equilateral geometry that remains above Tehran for at least ninety seconds, providing a coherent sensing baseline for cooperative imaging. The study ensures consistency with the wider Systems Tool Kit (STK 11.2) export workflow by relying on the in-repository `tools/stk_export.py` interface. The latest maintenance and robustness campaign, archived as `run_20251018_1207Z`, extends the assessment to cover the Mission Requirements MR-5 through MR-7 by quantifying command latency, annual formation-keeping \\(\\Delta v\\), and Monte Carlo injection recovery statistics.[Ref2]\n\n## Methodology\n1. **Establish orbital baseline.** The reference spacecraft adopts a near-circular \\(6898.137\\,\\text{km}\\) semi-major axis, \\(97.7^\\circ\\) inclination orbit. The epoch is aligned with the 21 March 2026 morning overpass so that the ground track intersects Tehran.\n2. **Define formation geometry.** Relative offsets of \\((0, -\\tfrac{1}{2}L, -\\tfrac{\\sqrt{3}}{6}L)\\), \\((0, \\tfrac{1}{2}L, -\\tfrac{\\sqrt{3}}{6}L)\\), and \\((0, 0, \\tfrac{\\sqrt{3}}{3}L)\\) metres (with \\(L = 50{,}000\\,\\text{m}\\)) generate a centroid-centred equilateral triangle embedded entirely within the local horizontal plane, ensuring that all vehicles maintain comparable altitude whilst satisfying the observability guidance of D'Amico et al. [Ref1].\n3. **Propagate inertial states.** The `constellation.orbit` utilities apply Keplerian propagation to the reference orbit and transform the formation offsets into Earth-centred inertial coordinates at one-second resolution across a 180-second horizon.\n4. **Assess ground geometry.** Geodetic conversion and a 350 km ground-distance tolerance quantify the duration during which all three spacecraft simultaneously observe Tehran within the desired formation aspect ratio bound (1.02).\n5. **Export for STK validation.** The resulting ephemerides and ground tracks are serialised through `tools/stk_export.py`, guaranteeing ingestion compatibility with STK 11.2 scenario files.\n\n## Results\nThe simulation yields the metrics summarised in Table 1. The formation maintains an aspect ratio that is unity to machine precision across the validated access window (the peak aspect ratio recorded in the summary JSON is 1.00000000000018). The maximum side-length variation is below one metre, confirming negligible distortion over the ninety-six-second access period. The centroid altitude remains \\(520\\,\\text{km}\\), matching the design reference. Automated regression `tests/unit/test_triangle_formation.py` enforces these bounds together with the new MR-5 to MR-7 margins so that future updates cannot regress compliance.[Ref3]\n\nThe committed `triangle_summary.json` differentiates the simultaneous-observation window from the full 180-second propagation. Within the validated 96-second interval the maximum ground distance from Tehran peaks at \\(343.62\\,\\text{km}\\), whereas the complete propagation reaches \\(641.89\\,\\text{km}\\). Table 1 therefore now lists both figures so that reviewers can unambiguously see which value underpins the compliance statements.[Ref2]\n\nTable 1 – Key formation metrics\n\n| Metric | Value |\n| --- | --- |\n| Formation window duration | \\(96\\,\\text{s}\\) |\n| Window start (UTC) | 2026-03-21T09:31:12Z |\n| Window end (UTC) | 2026-03-21T09:32:48Z |\n| Mean triangle area | \\(1.56\\times 10^7\\,\\text{m}^2\\) |\n| Mean side length | \\(6.00\\pm0.00\\,\\text{km}\\) |\n| Maximum aspect ratio | 1.00000000000018 (unity within numerical precision) |\n| Maximum ground distance to Tehran (validated 96 s window) | \\(343.62\\,\\text{km}\\) |\n| Maximum ground distance to Tehran (full propagation) | \\(641.89\\,\\text{km}\\) |\n\nThe debug export accompanying each simulation run now includes `triangle_geometry.csv`, capturing per-second area, aspect ratio, and side-length diagnostics, and the derived `triangle_geometry_timeseries.svg` plot. The automated figure overlays the ±5 per cent side-length requirement and the 1.02 aspect ratio ceiling mandated by the verification plan, demonstrating that every sample across the ninety-six-second window remains inside the authorised tolerance bands.[Ref2][Ref4]\n\nTo evidence simultaneous compliance with the \\(350\\,\\text{km}\\) ground-distance envelope and the \\(2{,}200\\,\\text{km}\\) command contact range, the artefact set now emits `ground_ranges.csv` together with the `ground_command_ranges.svg` diagnostic. The plot overlays both tolerance levels in kilometres and annotates any excursions, enabling reviewers to confirm that the Tehran target remains inside the ground track allowance while uplink opportunities continue to fall within the command antenna geometry.[Ref2]\n\n## Orbital Element Reconstruction\nThe midpoint of the validated access window was sampled to recover classical orbital elements for each spacecraft using the new `cartesian_to_classical` routine. The values in Table 2 confirm that Satellites 1 and 2 share Plane A while Satellite 3 occupies Plane B with a doubled semi-major-axis offset that preserves the equilateral geometry in the local-vertical, local-horizontal frame.[Ref1]\n\nThe debug plotting workflow now archives a focused diagnostic at `artefacts/debug/20250930T171815Z/plots/orbital_elements_formation_window.svg`, filtering the orbital-element time series to the \\(2026\\text{-}03\\text{-}21T09{:}31{:}12Z\\) to \\(2026\\text{-}03\\text{-}21T09{:}32{:}48Z\\) interval declared in `triangle_summary.json`. The chart mirrors the Systems Tool Kit (STK 11.2) compatible ephemerides exported through `tools/stk_export.py`, so reviewers can interrogate the same trajectory segment in both the SVG record and STK without encountering format limitations.\n\nTable 2 – Classical orbital elements at 2026-03-21T09:32:00Z\n\n| Satellite | Assigned plane | Semi-major axis (km) | Eccentricity | Inclination (deg) | RAAN (deg) | Argument of perigee (deg) | Mean anomaly (deg) |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| SAT-1 | Plane A | 6891.215 | 7.53\\(\\times 10^{-4}\\) | 97.70 | 18.881 | 216.040 | 180.0 |\n| SAT-2 | Plane A | 6891.215 | 7.53\\(\\times 10^{-4}\\) | 97.70 | 18.881 | 216.089 | 180.0 |\n| SAT-3 | Plane B | 6912.017 | 1.51\\(\\times 10^{-3}\\) | 97.70 | 18.881 | 36.065 | 0.0 |\n\n## STK Validation\nRunning `python -m sim.scripts.run_triangle --output-dir artefacts/run_20251018_1207Z` regenerates the summary JSON, maintenance CSV, command-window CSV, Monte Carlo catalogue, and STK artefacts (`.e`, `.sat`, `.gt`, `.fac`, `.int`). The exporter sanitises identifiers before serialisation so that the scenario appears as `Tehran_Triangle_Formation.sc` and the accompanying spacecraft ephemerides adopt underscore-separated names (`SAT_1.e`, `SAT_2.e`, `SAT_3.e`). Each ephemeris is now paired with a `.sat` wrapper referencing the corresponding `.e` file, allowing STK 11.2 to bind trajectories automatically during import. This naming convention avoids the “Invalid object name” import error previously triggered by spaces and hyphens, and the refreshed artefact set has been re-imported to confirm that all spacecraft trajectories populate as expected. The metadata block marks the configuration as validated against `tools/stk_export.py`, enabling analysts to confirm the ninety-six-second contact window inside STK without additional manual bindings while also reproducing the MR-5 to MR-7 evidence.\n\n## Maintenance and Responsiveness Assessment\nTable 3 summarises the new maintenance and responsiveness metrics captured during `run_20251018_1207Z`. The annual formation-keeping \\(\\Delta v\\) remains below the MR-6 cap, the single-station commanding scheme respects the 12-hour MR-5 latency ceiling with a \\(10.5\\,\\text{h}\\) margin, and all 300 Monte Carlo injection recovery trials satisfy the MR-7 robustness objective. The underlying datasets—`maintenance_summary.csv`, `command_windows.csv`, `injection_recovery.csv`, and `injection_recovery_cdf.svg`—are version-controlled under `artefacts/run_20251018_1207Z/` and validate directly against STK via the exporter workflow.[Ref2]\n\nTable 3 – MR-5 to MR-7 maintenance and responsiveness metrics\n\n| Requirement | Metric | Result | Margin | Evidence |\n| --- | --- | --- | --- | --- |\n| MR-5 | Maximum command latency | \\(1.53\\,\\text{h}\\) | \\(10.47\\,\\text{h}\\) margin versus 12-hour limit | `triangle_summary.json` (run_20251018_1207Z)[Ref2] |\n| MR-6 | Maximum annual formation-keeping \\(\\Delta v\\) | \\(14.04\\,\\text{m/s}\\) | \\(0.96\\,\\text{m/s}\\) below 15 m/s budget | `maintenance_summary.csv` (run_20251018_1207Z)[Ref2] |\n| MR-7 | Monte Carlo recovery success rate | 100% over 300 trials; \\(p_{95}\\) \\(\\Delta v = 0.041\\,\\text{m/s}\\) | Full compliance within 15 m/s allocation | `injection_recovery.csv` (run_20251018_1207Z)[Ref2] |\n\n## Repeatability Diagnostics\n1. **Catalogue access windows.** The new `formation_windows.csv` artefact enumerates every contiguous interval satisfying the ground-distance and aspect-ratio tolerances across the fourteen-day verification horizon. Each record notes the start and end timestamps, duration, and the worst-case triangle distortion so analysts can verify that repeated opportunities occur with consistent geometry.[Ref2]\n2. **Quantify recurrence statistics.** The `formation_recurrence` block inside `triangle_summary.json` reports the mean and extreme window separations alongside an orbital-period reference, enabling reviewers to confirm that the simulated constellation revisits Tehran with the expected synodic cadence.[Ref2]\n3. **Monitor station-keeping demand.** An automated scheduler evaluates the maximum side-length deviation against a one-percent tolerance and records recommended corrections in `station_keeping_events.csv`. The fourteen-day campaign identified no violations, demonstrating that the equilateral geometry remains self-sustaining without invoking the manoeuvre budget, yet the logic will flag any future drift that exceeds the tolerance envelope.[Ref2]\n\n## References\n- [Ref1] D'Amico, S., et al., \"Relative Orbital Elements for Spacecraft Formation-Flying,\" *Journal of Guidance, Control, and Dynamics*, 2005.\n- [Ref2] Formation-Sat Systems Team, *run_20251018_1207Z Tehran Triangle Maintenance and Robustness Campaign*, FS-ANL-005 v1.0, 2025.\n- [Ref3] Formation-Sat Systems Team, *Triangle Formation Unit Tests*, FS-TST-004 v1.1, 2025.\n- [Ref4] Formation-Sat Systems Team, *Verification Plan*, FS-PLN-003 v1.2, 2025.\n\n\n"
      },
      {
        "text": "\n--- End of content ---"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "Based on my review of the documentation, the project **does** appear to implement the scenario you described.\n\nHere's a summary of my findings from the documents `tehran_triangle_walkthrough.md` and `triangle_formation_results.md`:\n\n*   **Triangular Formation:** The \"Tehran Triangle\" scenario is designed to create a 3-satellite equilateral triangular formation.\n*   **Location:** The formation is designed to occur over Tehran.\n*   **Duration:** The simulation achieves a formation window of **96 seconds**, which is greater than your requirement of 90 seconds.\n*   **Daily Recurrence:** The `triangle_formation_results.md` document mentions a \"fourteen-day verification horizon\" and the generation of `formation_windows.csv` to enumerate every contiguous interval. This suggests that the formation is repeatable, and likely daily.\n*   **Two Orbital Planes:** The `triangle_formation_results.md` document explicitly states that the satellites are in two separate planes. Table 2 in that document shows that \"Satellites 1 and 2 share Plane A while Satellite 3 occupies Plane B\".\n\nIn conclusion, the existing \"Tehran Triangle\" scenario in the project seems to meet all the requirements you've outlined.\n\nIf you would like, I can run the simulation to verify these findings and provide you with the results."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "آره ران کن و وریفای کن و نتایجی که گرفتی رو توی یک فایل .md بذار حتما؛ فقط مدت ران ای که داری میگیری 7 روزه یا 14 روزه باشه که بتونی rgt بودن مدار ها هم بررسی کنی"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Of course. I will run the 14-day simulation for the \"Tehran Triangle\" scenario to verify the formation and its repeatability. After the simulation is complete, I will analyze the results and create a markdown file summarizing my findings.\n\nFirst, I need to determine how to configure the simulation duration. I will start by inspecting the command-line options for the simulation script."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "command": "python -m sim.scripts.run_triangle --help",
            "description": "Checking the help for the triangle simulation script to see how to set the duration."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762791244853-f885d5ee522bb",
          "name": "run_shell_command",
          "response": {
            "output": "Command: python -m sim.scripts.run_triangle --help\nDirectory: (root)\nOutput: Traceback (most recent call last):                                                                                                                                         \n  File \"<frozen runpy>\", line 189, in _run_module_as_main                                                                                                                  \n  File \"<frozen runpy>\", line 112, in _get_module_details                                                                                                                  \n  File \"/home/diannenewton170/formation-sat-2/sim/scripts/__init__.py\", line 14, in <module>                                                                               \n    from .scenario_execution import run_scenario                                                                                                                           \n  File \"/home/diannenewton170/formation-sat-2/sim/scripts/scenario_execution.py\", line 8, in <module>                                                                      \n    from . import run_scenario as scenario_runner                                                                                                                          \n  File \"/home/diannenewton170/formation-sat-2/sim/scripts/run_scenario.py\", line 36, in <module>                                                                           \n    from tools.render_scenario_plots import generate_visualisations as generate_scenario_plots                                                                             \n  File \"/home/diannenewton170/formation-sat-2/tools/render_scenario_plots.py\", line 18, in <module>                                                                        \n    import matplotlib                                                                                                                                                      \nModuleNotFoundError: No module named 'matplotlib'\nError: (none)\nExit Code: 1\nSignal: 0\nBackground PIDs: (none)\nProcess Group PGID: 1820"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XoHrGAxiarC10LIv3xFitWB6wxOOet6/U/C4Y5xKKSAKcQHj8f9ebzfyhVPUdY7Xzerv8xKy8W+yjRe+5POZKVLyqBRedhBg+3kpw/gCgDh8OJBd+nxzMOOLDgzZKr13AU/YlyCJRE3w2ItZOnfITT+p6U7fIzfrp2QIHgG3kZkvEykbae+q5kHCJfE4qypb/lPuClUB4/H/Xu1BPQXiPVz/O1OIeFrMz9yVCpoQIF3KCHrleWX1q3Hh63WWOtL4QrlN1/FoKShQUU1neYH41YCiPHYjWvwJrpKL+3WIQJunmbMmCW7C3JmgCsMBAePx/16AxmTzDdD9ClG6yvZCDdQsTNX3hhKQH+Mv1aGEj3xRw+ELhXVdvDxeJ3A3bPaVdov59kITFQw6zv2UwyPjtqW2ooyHBv8TZuKExL8HLtzj/mcpd1GvdBBZE8LiCM3V49JaJbSdhBOqGGQMou4rCPTc9IRT1UtRP7p41ibkDdRt+IT1V9kacBWFwrwbpdJy84Z6CJmnOTFd1fcql1yGCfCZHdFuLWiTX1y9pvZYZlVVwhB8lNSiCOnGntBI4fqBCnkB4/H/XtzcbZ8QvQA/UjUnuRmF7im2Btw//TSaUzMUqb/RKEXT1ILbRga9eJEh274GI48kOILstE/Fc8COv8WU3D1zKCbqXBZKlJUi7CoAOTiqbmEPJ+UvYT3QJ134D0xU4IJcJ85D650O6K6YbdS3BTrm9CS1DrkD",
        "text": "It seems that a dependency, `matplotlib`, is missing. I'll install the project dependencies as recommended in the documentation by running `make setup`. After that, I will proceed with the simulation."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "command": "make setup",
            "description": "Installing project dependencies using 'make setup' to resolve the missing module error."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762791253014-766774c247bdd",
          "name": "run_shell_command",
          "response": {
            "output": "Command: make setup\nDirectory: (root)\nOutput: python3 -m venv .venv                                                                                                                                                      \n.venv/bin/python -m pip install --upgrade pip                                                                                                                              \nRequirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.3)                                                                                          \n.venv/bin/pip install -r requirements.txt                                                                                                                                  \nIgnoring poliastro: markers 'python_version < \"3.11\"' don't match your environment                                                                                         \nRequirement already satisfied: numpy>=1.24 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.3.4)                                             \nRequirement already satisfied: scipy>=1.11 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.16.3)                                            \nRequirement already satisfied: astropy>=5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (7.1.1)                                              \nRequirement already satisfied: numba>=0.58 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.62.1)                                            \nRequirement already satisfied: jplephem>=2.20 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.23)                                           \nRequirement already satisfied: matplotlib>=3.7 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (3.10.7)                                       \nRequirement already satisfied: pandas>=2.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.3.3)                                            \nRequirement already satisfied: plotly>=5.15 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (6.4.0)                                           \nRequirement already satisfied: cartopy>=0.22 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.25.0)                                         \nRequirement already satisfied: pytest>=8 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (9.0.0)                                              \nRequirement already satisfied: pytest-cov>=6 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (7.0.0)                                          \nRequirement already satisfied: setuptools>=70 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (80.9.0)                                        \nRequirement already satisfied: wheel in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.45.1)                                                 \nRequirement already satisfied: fastapi>=0.111 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (0.121.1)                                       \nRequirement already satisfied: uvicorn>=0.29 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (0.38.0)                                         \nRequirement already satisfied: pyerfa>=2.0.1.1 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (2.0.1.5)                           \nRequirement already satisfied: astropy-iers-data>=0.2025.9.29.0.35.48 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (0.2025.11.10\n.0.38.31)                                                                                                                                                                  \nRequirement already satisfied: PyYAML>=6.0.0 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (6.0.3)                               \nRequirement already satisfied: packaging>=22.0.0 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (25.0)                            \nRequirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.58->-r requirements.txt (line 8)) (0.45.1)                \nRequirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (1.3.3)                      \nRequirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (0.12.1)                         \nRequirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (4.60.1)                    \nRequirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (1.4.9)                     \nRequirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (12.0.0)                            \nRequirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (3.2.5)                          \nRequirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (2.9.0.post0)            \nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=2.2->-r requirements.txt (line 13)) (2025.2)                             \nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=2.2->-r requirements.txt (line 13)) (2025.2)                           \nRequirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.12/site-packages (from plotly>=5.15->-r requirements.txt (line 14)) (2.10.2)                        \nRequirement already satisfied: shapely>=2.0 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (2.1.2)                            \nRequirement already satisfied: pyshp>=2.3 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (3.0.2.post1)                        \nRequirement already satisfied: pyproj>=3.3.1 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (3.7.2)                           \nRequirement already satisfied: iniconfig>=1.0.1 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (2.3.0)                            \nRequirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (1.6.0)                              \nRequirement already satisfied: pygments>=2.7.2 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (2.19.2)                            \nRequirement already satisfied: coverage>=7.10.6 in ./.venv/lib/python3.12/site-packages (from coverage[toml]>=7.10.6->pytest-cov>=6->-r requirements.txt (line 19)) (7.11.3\n)                                                                                                                                                                          \nRequirement already satisfied: starlette<0.50.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (0.49.3)             \nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.t\nxt (line 24)) (2.12.4)                                                                                                                                                     \nRequirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (4.15.0)              \nRequirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (0.0.3)                   \nRequirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=\n0.111->-r requirements.txt (line 24)) (0.7.0)                                                                                                                              \nRequirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0\n.111->-r requirements.txt (line 24)) (2.41.5)                                                                                                                              \nRequirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi\n>=0.111->-r requirements.txt (line 24)) (0.4.2)                                                                                                                            \nRequirement already satisfied: anyio<5,>=3.6.2 in ./.venv/lib/python3.12/site-packages (from starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (line 24)) (4.1\n1.0)                                                                                                                                                                       \nRequirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (lin\ne 24)) (3.11)                                                                                                                                                              \nRequirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (\nline 24)) (1.3.1)                                                                                                                                                          \nRequirement already satisfied: click>=7.0 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.29->-r requirements.txt (line 25)) (8.3.0)                              \nRequirement already satisfied: h11>=0.8 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.29->-r requirements.txt (line 25)) (0.16.0)                               \nRequirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from pyproj>=3.3.1->cartopy>=0.22->-r requirements.txt (line 15)) (2025.10.5)              \nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->-r requirements.txt (line 12)) (1.17.0)       \nEnvironment initialised in .venv.\nError: (none)\nExit Code: 0\nSignal: 0\nBackground PIDs: (none)\nProcess Group PGID: 1827"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XkBnIA675lJo0/bqqQKv8Mmln4s/SGcGYMYNRwWA6SYKeQHj8f9elMRC061IJ7/HptNlJTzT1gIzmeAtceuuEznEr1M3nH7rQtLHVqDpg6BIpqxqZKyowe/GQmPMdPBs2qLj4b9Zyr6ML762//riL1H/RUA1leAT2+KRzZldSlIKHvbpZuG2UNYFv01+snFuUEukEiZYqSEnjNEKMAHj8f9egeKruDK9sG6fZDO2W99CB/wBBXqj5bkKhwjpKDJMwmUzzeLct/AusB+XGQ==",
        "text": "Now that the dependencies are installed, I will again check the help for the `run_triangle` script to find the duration setting."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "description": "Checking the help for the triangle simulation script again after installing dependencies.",
            "command": ".venv/bin/python -m sim.scripts.run_triangle --help"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762791264601-0d69ac9f30fce",
          "name": "run_shell_command",
          "response": {
            "output": "Command: .venv/bin/python -m sim.scripts.run_triangle --help\nDirectory: (root)\nOutput: usage: run_triangle.py [-h] [--config CONFIG] [--output-dir OUTPUT_DIR]                                                                                                    \n                                                                                                                                                                           \nCommand-line entry point for the Tehran triangular-formation simulation.                                                                                                   \n                                                                                                                                                                           \noptions:                                                                                                                                                                   \n  -h, --help            show this help message and exit                                                                                                                    \n  --config CONFIG       Path to the triangle formation configuration file.                                                                                                 \n  --output-dir OUTPUT_DIR                                                                                                                                                  \n                        Directory in which to write simulation artefacts.\nError: (none)\nExit Code: 0\nSignal: 0\nBackground PIDs: (none)\nProcess Group PGID: 1888"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CikB4/H/Xo9gI7I/rFaucqKXDzELmIL/uPa72wdy61yhPniLTSRxwzKuiQplAePx/14d0B1r2lOIBttceqj3p8Xx3876N3E5a827GaK9LgA6GG0+bmrFoRQ/sGI8Ao+Pjmwj7k2xTXb4vL3YmHv3gojFgR69BLYCXUr+qBjr/UanNg0GT4om6VQnrVCvhQj558wKYAHj8f9e06vC+YvAsghWrhnsC+yt/NuDm5c+yLKZt7IBcDNk3PvV0qjIHBKGPjMJj21Bn0EqErAfZHrCWWRjUrwpM98F129jdtOYunLnoZi+ki98JiokzCIyHpAceiYdLArFAQHj8f9en19EnLOwpuDDwPxvRYWzyogru/D+k59RFOrx8M29qOuJ3/QF/d+c7Hl37IdHsyEvMm2J9MBV3i7OFxBG6loSzdGRjGxFebJ91KEXL9dZaMXnFDak9/CFzuTUIZwbEWq7hOgJTkQpe3r2bNPx40wM5wQCQvDaUdNj60cehoa7/6GuaqpIJxbsHCJ892GbJ3LYV3lvv1n2dfk4BhQDQKd0dKku8pBP8nbDTB7AKG45X+/IJDyYyd/KWRc5JsQ9aJBCCp4BAePx/14TVRl/I5Ln/Yq6GNhMoIuWMt4kJI7k1hx9Gcp4+T/vfb938K6B8NY2LUbcWvEZk7C0GJRFK9De74aBU/ZA1uwZOr8jQjZWOz3sRad1HxiJZV1yf8cErcySPOjNHjkiKxOfG2Ihj91C716DL3UqNl8Mh6gSZCqUDSZTLWTdmlzofz/nHN3xkBJJR43pf5VkSlGC72MYl6ZHW58KWgHj8f9eCeQbd9nIM9R0ZJSlnKeH0aG2y2vU5Qx5LqRk6lX/hurYhlMqv+mOPgQYeJxj5muUgHBZYtEwyo/tQZMtW3w0tLCXcTYIfiDOMzkWvUqCGYiFNZHZwg==",
        "text": "The script does not have a duration argument, so I'll check the configuration file, `config/scenarios/tehran_triangle.json`, for a duration parameter."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/config/scenarios/tehran_triangle.json"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762791271212-d457f0733da7",
          "name": "read_file",
          "response": {
            "output": "{\n  \"metadata\": {\n    \"scenario_name\": \"Tehran Triangle Formation\",\n    \"description\": \"Three-satellite equilateral formation centred on Tehran with 6 km side length.\",\n    \"author\": \"Formation Dynamics Team\",\n    \"validated_against_stk_export\": true,\n    \"alignment_validation\": {\n      \"run_id\": \"run_20260321_0740Z\",\n      \"epoch_utc\": \"2026-03-21T07:40:10Z\"\n    }\n  },\n  \"reference_orbit\": {\n    \"epoch_utc\": \"2026-03-21T07:40:10Z\",\n    \"semi_major_axis_km\": 6939.242,\n    \"eccentricity\": 0.0,\n    \"inclination_deg\": 97.7,\n    \"raan_deg\": 350.9838169642857,\n    \"argument_of_perigee_deg\": 0.0,\n    \"mean_anomaly_deg\": 36.06454701821796\n  },\n  \"formation\": {\n    \"side_length_m\": 6000.0,\n    \"duration_s\": 180.0,\n    \"time_step_s\": 1.0,\n    \"ground_tolerance_km\": 350.0,\n    \"aspect_ratio_tolerance\": 1.02,\n    \"target\": {\n      \"latitude_deg\": 35.6892,\n      \"longitude_deg\": 51.3890\n    },\n    \"plane_allocations\": {\n      \"SAT-1\": \"Plane A\",\n      \"SAT-2\": \"Plane A\",\n      \"SAT-3\": \"Plane B\"\n    },\n    \"maintenance\": {\n      \"burn_duration_s\": 32.0,\n      \"interval_days\": 7.0,\n      \"delta_v_budget_mps\": 15.0\n    },\n    \"command\": {\n      \"contact_range_km\": 2200.0,\n      \"station\": {\n        \"latitude_deg\": 30.283,\n        \"longitude_deg\": 57.083\n      }\n    },\n    \"monte_carlo\": {\n      \"samples\": 300,\n      \"position_sigma_m\": 250.0,\n      \"velocity_sigma_mmps\": 5.0,\n      \"recovery_time_s\": 43200.0,\n      \"delta_v_budget_mps\": 15.0,\n      \"seed\": 314159\n    },\n    \"drag_dispersion\": {\n      \"samples\": 200,\n      \"density_sigma\": 0.25,\n      \"drag_coefficient_sigma\": 0.05,\n      \"reference_density_kg_m3\": 3.2e-12,\n      \"drag_coefficient\": 2.2,\n      \"reference_area_m2\": 1.1,\n      \"spacecraft_mass_kg\": 165.0,\n      \"time_horizon_orbits\": 12.0,\n      \"integration_step_s\": 120.0,\n      \"seed\": 20260321\n    },\n    \"perturbations\": {\n      \"position_noise_sigma_m\": 1.0,\n      \"seed\": 12345\n    }\n  }\n}\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiIB4/H/XhYHuTh1/1au5W8z5MqWw5FRDqlwlSsl6QZUXkKCCmcB4/H/XnnSj+JV16x48LdWarR6x/CFQzPsjebSGxAsHamEgieXpCCwmx2Nj7V48s6b/ZBEGf1AZfETTLI7QVjaemdFQE+JJlU5upD/HBG4LngiLp5RZhgvrHBRO6hFRYkepkysDObsClYB4/H/XvfwB2TIg5lper+uFT3S/uVGCBeB/6dWJVeXmwIeisCu5GtW00OEa+i3mt7iKOJ8u2KRx2DjDNn2kMVAaQUTp6YIKBlSTQzQSfUnfp7V9Vtk2gqbAQHj8f9eQKMbLd7OlYqxqpsq0e40KXGrQTdm6t+keJrUUySTBdfw5xLgVObFKOErpfuRhQ7gD6JgDuiMYotOkfBwKO9suvzqvksY/qCmXgO8dUFfZQjCraO9FwimwZkBlcowUTmy57SHeCG1d5O4wJ0ujBlRqj4UWLFWCikoJbDsb8EwS1KMRWf50ly7LGNCXwlfNeFV5J8k84HUCpUBAePx/14MvbssBPDMijcjZ3z1kQAqXa/7ynex/64n5t+d15aff/VAd2RROrIsmIZ0rcs/Z8FKZ+B0AM0Ik7or2dBAxqqm/z/L6I3EkFfxilf9r1l3kbeKhjJyQRrmVZzQFwD3RXOKxducCphhsp+w9JDJG5s/y6U5oEaClv0FUMQ4BNhB2FxltqvA5iiCefV2OyrL1M0KsAEB4/H/XvgKQeK0+IzmDVLufi3I/Sgg43hVNZQPvQMt5fRS4LzYn3a1x5uWxmMWTU2+aMOS9RwMz+TwbDYAbagLg0Zb6Z+xBFsp/uzYM34Ijvmz5euOXzIBlR9lAZsC6Ge8EuI9irCwUNJ4zdAE/yqeH6Wm2Yyntdd3YHzB9RJf2c6aVOFObNs7nLxq/mwFCysMd8gNgsHfGukOxCIySXblideKR3+fM0EmzNWG270YGQqQAgHj8f9eRPnH9Erk/axB+KNFfZmVLuIL8Zteps644g2e9y3DgqD20Keq+rKfk1eerV+7nECn1od8pRxufMRfx819RY1FO+jtUIFfStmiroZjhwZ/9wiUSlfCvizlBamnaVvJZ63cgnXGqlzEVzLzIIDWUQustZ3iiBAiRpSNnVqTPECtUaoHSd3CVyVirHlw97+v2raF1XnqAYTL56geIyFhSKNpI19P0Hv7fW+84LqcyjmBRptULkn35TUPGH/TiETOLpoaZUvKnri74cnDC2GgvGXGHNEKhEuVrD8DthrKqSQAql6lBF7GRafZBIBTOqeu4X/I7MpAKwa9RyjsM/lEAyrG0ryPb56Vwm6SuT8ICqICAePx/17Lj1O3dGugnP0P43OS36uvpP/gd6pLPwrMTPS7WB2wMjdb0viPqZZZaVTsqzj9NgrFWdlOVaGjJmqXU+JiMW3Hoyh7+fE8nMJN53AM24b7tQ/3QPaPftJTICCkfBwG7QGMp7as0NaWSFBDpIiBkHOmU8UwqX3wTlj07seo1YKTlQld7xgGuspUwmJmP7OghkBpPcaoNtoHVTZWZ7njpcC/ynmAdIlMmuKNgZZ0i9n7oLkXTbF8HxEbkIqloq/Tk4QOTKZa5yKIBgTti8+KfSE8X2JjngTEvEx51kOK62pDVIio2O9WbQQiwyS0Plu9wbjiU283XYbk27qxilWV0CsrqP3Ek8PDGlYTGe7VQlVFWy8327SU5X0xOiQdFkEKugEB4/H/XseYyjHQdgSrxyiqy2KN1BpFfriSI5HE7JqaTa3mkpnpi6HOaJE6+IiU/QI7KUJAz2Tip8a+gW9NkVNapgEAiq6JWmmf7Th7rb+mtVPWzBCYgjFhZ8hb6Ovv6NctOZ01rM3t1sAfWdpLFyD3PDBaH/6opxusKFcUxf9GhZXUJYPtXiB1uwbR/1HaSC7ETrn+ZKh/uLa3WBY3mjk4o1G2BAzfiTq2yM28mEzTQ0zU5/dJoUcuuJY=",
        "text": "I see a 180-second propagation duration in the config, but the user wants a 14-day run to check for RGT, and a previous document mentioned a \"fourteen-day verification horizon.\" I'll examine `run_triangle.py` to find the main simulation loop and see how the total duration is set."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/run_triangle.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762791277218-9319d34fdd818",
          "name": "read_file",
          "response": {
            "output": "\"\"\"Command-line entry point for the Tehran triangular-formation simulation.\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Iterable, Mapping, Optional\n\nfrom sim.formation import simulate_triangle_formation\nfrom sim.formation.triangle_artefacts import export_triangle_time_series\nfrom tools.generate_triangle_report import main as generate_triangle_report_main\nfrom tools.render_debug_plots import generate_visualisations as generate_debug_visualisations\n\nPROJECT_ROOT = Path(__file__).resolve().parents[2]\nDEFAULT_CONFIG = PROJECT_ROOT / \"config\" / \"scenarios\" / \"tehran_triangle.json\"\nDEFAULT_OUTPUT_ROOT = PROJECT_ROOT / \"artefacts\" / \"triangle\"\n\n\ndef parse_args(args: Optional[Iterable[str]] = None) -> argparse.Namespace:\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--config\",\n        type=Path,\n        default=DEFAULT_CONFIG,\n        help=\"Path to the triangle formation configuration file.\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=Path,\n        help=\"Directory in which to write simulation artefacts.\",\n    )\n    return parser.parse_args(args)\n\n\ndef main(args: Optional[Iterable[str]] = None) -> int:\n    namespace = parse_args(args)\n\n    output_dir = _resolve_output_directory(namespace.output_dir)\n    result = simulate_triangle_formation(namespace.config, output_directory=output_dir)\n\n    csv_bundle = export_triangle_time_series(result, output_dir)\n    debug_outputs = _safe_generate_debug_plots(output_dir)\n    config_path = _resolve_configuration_path(namespace.config)\n    _safe_generate_triangle_report(output_dir, config_path)\n\n    window = result.metrics.get(\"formation_window\", {})\n    duration = float(window.get(\"duration_s\", 0.0)) if isinstance(window, Mapping) else 0.0\n    start = window.get(\"start\") if isinstance(window, Mapping) else None\n    end = window.get(\"end\") if isinstance(window, Mapping) else None\n    print(\n        \"Formation window {:.1f} s centred over Tehran. Start: {} End: {}\".format(\n            duration, start or \"n/a\", end or \"n/a\"\n        )\n    )\n\n    print(f\"Artefacts written to {output_dir}\")\n    for label in (\n        \"positions_m\",\n        \"velocities_mps\",\n        \"latitudes_rad\",\n        \"longitudes_rad\",\n        \"altitudes_m\",\n        \"triangle_geometry\",\n        \"ground_ranges\",\n        \"orbital_elements\",\n    ):\n        path = csv_bundle.csv_paths.get(label)\n        if path is not None:\n            print(f\"  • {label} CSV: {path}\")\n\n    if csv_bundle.per_satellite_csvs:\n        print(\"  • per-satellite orbital elements:\")\n        for sat_id, path in sorted(csv_bundle.per_satellite_csvs.items()):\n            print(f\"      - {sat_id}: {path}\")\n\n    artefacts = result.artefacts if isinstance(result.artefacts, Mapping) else {}\n    summary_path = artefacts.get(\"summary_path\")\n    if summary_path:\n        print(f\"  • triangle_summary.json: {summary_path}\")\n    stk_dir = artefacts.get(\"stk_directory\")\n    if stk_dir:\n        print(f\"  • STK export directory: {stk_dir}\")\n    for key, label in (\n        (\"maintenance_csv\", \"maintenance summary\"),\n        (\"command_windows_csv\", \"command windows\"),\n        (\"injection_recovery_csv\", \"injection recovery samples\"),\n        (\"drag_dispersion_csv\", \"drag dispersion samples\"),\n    ):\n        path = artefacts.get(key)\n        if path:\n            print(f\"  • {label}: {path}\")\n\n    if debug_outputs:\n        print(\"  • debug plots:\")\n        for label, produced in sorted(debug_outputs.items()):\n            if isinstance(produced, Mapping):\n                svg_path = produced.get(\"svg\")\n                if svg_path:\n                    print(f\"      - {label} (svg): {svg_path}\")\n                for fmt, path in sorted(produced.items()):\n                    if fmt == \"svg\":\n                        continue\n                    print(f\"      - {label} ({fmt}): {path}\")\n            else:\n                print(f\"      - {label}: {produced}\")\n\n    return 0\n\n\ndef _resolve_output_directory(candidate: Optional[Path]) -> Path:\n    if candidate is None:\n        timestamp = datetime.now(timezone.utc).strftime(\"run_%Y%m%d_%H%MZ\")\n        output_dir = DEFAULT_OUTPUT_ROOT / timestamp\n    else:\n        output_dir = candidate\n        if not output_dir.is_absolute():\n            output_dir = (PROJECT_ROOT / output_dir).resolve()\n    output_dir.mkdir(parents=True, exist_ok=True)\n    return output_dir\n\n\ndef _resolve_configuration_path(config_path: Path) -> Path:\n    if config_path.is_absolute():\n        return config_path\n    candidate = (PROJECT_ROOT / config_path).resolve()\n    return candidate if candidate.exists() else config_path\n\n\ndef _safe_generate_debug_plots(output_dir: Path) -> Mapping[str, object]:\n    try:\n        return generate_debug_visualisations(output_dir)\n    except Exception as exc:  # pragma: no cover - defensive logging only\n        print(f\"Warning: failed to generate debug visualisations: {exc}\")\n        return {}\n\n\ndef _safe_generate_triangle_report(output_dir: Path, config_path: Path) -> None:\n    argv = [\"--run-dir\", str(output_dir)]\n    if config_path.exists():\n        argv.extend([\"--config\", str(config_path)])\n    try:\n        generate_triangle_report_main(argv)\n    except Exception as exc:  # pragma: no cover - defensive logging only\n        print(f\"Warning: failed to generate triangle report plots: {exc}\")\n\n\nif __name__ == \"__main__\":  # pragma: no cover - exercised via CLI tests.\n    raise SystemExit(main())\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjIB4/H/XtvLV0iAjTMqKW+DRRWpxvOMuP2yWWY1JIWkAlmZRn3z+LxeaezkX0wzr2B+dAprAePx/169f2qXt/mTuKlbOxISn1U3FBZiwr4fsuYhgCdUscKX5G5ZmhBlbNemFAyVGCupLjYQUYKElYc1Ix7LEtdAd0RWVM1y6joJtPZhGfSg/AtElnXSh2RepyEBpl26heXsvLnsMLflgcAKhAEB4/H/Xvp1UkqI1rAypKkM6mD4Bk8iptezx44xYiF6k/yV6lYuEL74Z+wti5v3+SBwnKa//6kx8cd9sBmAepqTi/1YsDI9O1BgKY21K4VlqI0L63Gv6Xir+emPTNxMtvoTFyhnkDqsF6FvO3q0i3wYsyKM1L+WqIGKxCJiWMXgky0IU4cKrQEB4/H/Xj31lL9FtPSIVecmfhfly7BPZD2jRJR2cRnKygCGmJyYuZQJBTZ0O+YmQ+mseo8wd8Fk8kl3exvyvtrgyS7UwErBrwMJ4CThYLJsqZL1MY+tBKBCu8S4l31i04V5KCxDabZyGeuqfTyI5uZujrzhb+vDl1EbWcckNjYWfHc2GE8DRGRFANKc1G5P7GGeXmR5efhxEG4QoVs2/h0/qOZTvOL1hvg8isl+fgoiAePx/14I2mNnBa1PIdg2WTV7cfmjz0d7tOPBuZmD3JGiAQ==",
        "text": "`run_triangle.py` calls `simulate_triangle_formation` in `sim.formation`. I'll check `sim/formation/__init__.py` to see how it handles simulation duration."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/formation/__init__.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762791280843-6985065d960dc",
          "name": "read_file",
          "response": {
            "output": "\"\"\"Formation-flying simulation helpers.\"\"\"\n\nfrom .triangle import simulate_triangle_formation\n\n__all__ = [\"simulate_triangle_formation\"]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XqOb/fiLtUK38A4m3WERk75PATln1ahFXbicRHlV+bIKaQHj8f9eN+xW96EkXvHjmph3AEnoQ6nbUwsZxmuFA/lFutfvxMtap7IfOqLaPVoEv94gqxFx3AsldK/uuv5rgsO3S6E+56AClObPc9Dpb+gvZF8JsjKaCVxXguc135i9mr70l0Cn5Zp3pQo6AePx/16o8d2mCo2+BGPt/O1F4fhb+7DCm0BiK5AmIanU/no0oHA5CLjbaKQjOCRQE2ucc6tRTMtVvw==",
        "text": "Okay, reading `sim/formation/triangle.py` for `simulate_triangle_formation`."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/formation/triangle.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762791283096-629e9f30046ee",
          "name": "read_file",
          "response": {
            "output": "\"\"\"Simulation of a three-satellite triangular formation over Tehran.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Iterable, Mapping, MutableMapping, Optional, Sequence\n\nimport numpy as np\nimport pandas as pd\n\nSECONDS_PER_DAY = 86_400.0\nSECONDS_PER_YEAR = SECONDS_PER_DAY * 365.25\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nfrom src.constellation.geometry import (\n    triangle_area,\n    triangle_aspect_ratio,\n    triangle_side_lengths,\n)\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n    propagate_kepler,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\nfrom tools.stk_export import (\n    FacilityDefinition,\n    GroundContactInterval,\n    GroundTrack,\n    GroundTrackPoint,\n    PropagatedStateHistory,\n    ScenarioMetadata,\n    SimulationResults,\n    StateSample,\n    sanitize_stk_identifier,\n    unique_stk_names,\n    export_simulation_to_stk,\n)\n\n\n@dataclass\nclass TriangleFormationResult:\n    \"\"\"Container collecting the outputs of the triangle simulation.\"\"\"\n\n    times: Sequence[datetime]\n    positions_m: Mapping[str, np.ndarray]\n    velocities_mps: Mapping[str, np.ndarray]\n    classical_elements: Mapping[str, Mapping[str, np.ndarray]]\n    latitudes_rad: Mapping[str, np.ndarray]\n    longitudes_rad: Mapping[str, np.ndarray]\n    altitudes_m: Mapping[str, np.ndarray]\n    triangle_area_m2: np.ndarray\n    triangle_aspect_ratio: np.ndarray\n    triangle_sides_m: np.ndarray\n    centroid_lat_rad: np.ndarray\n    centroid_lon_rad: np.ndarray\n    centroid_alt_m: np.ndarray\n    max_ground_distance_km: np.ndarray\n    min_command_distance_km: np.ndarray\n    metrics: Mapping[str, object]\n    artefacts: Mapping[str, Optional[str]]\n\n    def to_summary(self) -> MutableMapping[str, object]:\n        \"\"\"Convert the simulation result into a JSON-serialisable mapping.\"\"\"\n\n        samples: list[MutableMapping[str, object]] = []\n        satellite_ids = sorted(self.positions_m)\n        geometry: MutableMapping[str, object] = {\n            \"times\": [epoch.isoformat().replace(\"+00:00\", \"Z\") for epoch in self.times],\n            \"satellite_ids\": satellite_ids,\n            \"positions_m\": {\n                sat_id: self.positions_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"classical_element_fields\": list(CLASSICAL_ELEMENT_FIELDS),\n            \"classical_element_units\": {\n                \"semi_major_axis_km\": \"km\",\n                \"eccentricity\": \"\",\n                \"inclination_deg\": \"deg\",\n                \"raan_deg\": \"deg\",\n                \"argument_of_perigee_deg\": \"deg\",\n                \"mean_anomaly_deg\": \"deg\",\n            },\n            \"classical_elements\": {\n                sat_id: {\n                    element: self.classical_elements[sat_id][element].tolist()\n                    for element in self.classical_elements[sat_id]\n                }\n                for sat_id in satellite_ids\n            },\n            \"latitudes_rad\": {\n                sat_id: self.latitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"longitudes_rad\": {\n                sat_id: self.longitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"altitudes_m\": {\n                sat_id: self.altitudes_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n        }\n        geometry[\"max_ground_distance_km\"] = [\n            float(value) for value in self.max_ground_distance_km\n        ]\n        geometry[\"min_command_distance_km\"] = [\n            float(value) for value in self.min_command_distance_km\n        ]\n        for index, epoch in enumerate(self.times):\n            samples.append(\n                {\n                    \"time\": epoch.isoformat().replace(\"+00:00\", \"Z\"),\n                    \"triangle_area_m2\": float(self.triangle_area_m2[index]),\n                    \"triangle_aspect_ratio\": float(self.triangle_aspect_ratio[index]),\n                    \"triangle_side_lengths_m\": [\n                        float(val) for val in self.triangle_sides_m[index]\n                    ],\n                    \"centroid\": {\n                        \"latitude_deg\": math.degrees(self.centroid_lat_rad[index]),\n                        \"longitude_deg\": math.degrees(self.centroid_lon_rad[index]),\n                        \"altitude_m\": float(self.centroid_alt_m[index]),\n                    },\n                    \"max_ground_distance_km\": float(self.max_ground_distance_km[index]),\n                }\n            )\n\n        return {\n            \"metrics\": self.metrics,\n            \"samples\": samples,\n            \"artefacts\": dict(self.artefacts),\n            \"geometry\": geometry,\n        }\n\n\ndef simulate_triangle_formation(\n    config_source: Mapping[str, object] | Path | str,\n    output_directory: Optional[Path | str] = None,\n) -> TriangleFormationResult:\n    \"\"\"Simulate the triangular formation described by *config_source*.\"\"\"\n\n    configuration = _load_configuration(config_source)\n    formation = configuration[\"formation\"]\n    metadata = configuration.get(\"metadata\", {})\n\n    satellite_elements: dict[str, OrbitalElements] = {}\n    if \"satellites\" in configuration:\n        satellite_ids = [sat[\"id\"] for sat in configuration[\"satellites\"]]\n        plane_allocations = {\n            sat[\"id\"]: sat[\"plane\"] for sat in configuration[\"satellites\"]\n        }\n        epoch = _parse_time(\n            configuration[\"satellites\"][0][\"orbital_elements\"][\"epoch_utc\"]\n        )\n        semi_major_axis_m = (\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"][\"semi_major_axis_km\"]\n            )\n            * 1_000.0\n        )\n        inclination = math.radians(\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"].get(\n                    \"inclination_deg\", 0.0\n                )\n            )\n        )\n\n        for sat_config in configuration[\"satellites\"]:\n            elements = sat_config[\"orbital_elements\"]\n            satellite_elements[sat_config[\"id\"]] = OrbitalElements(\n                semi_major_axis=float(elements[\"semi_major_axis_km\"]) * 1_000.0,\n                eccentricity=float(elements.get(\"eccentricity\", 0.0)),\n                inclination=math.radians(float(elements.get(\"inclination_deg\", 0.0))),\n                raan=math.radians(float(elements.get(\"raan_deg\", 0.0))),\n                arg_perigee=math.radians(\n                    float(elements.get(\"argument_of_perigee_deg\", 0.0))\n                ),\n                mean_anomaly=math.radians(float(elements.get(\"mean_anomaly_deg\", 0.0))),\n            )\n    else:\n        reference = configuration[\"reference_orbit\"]\n        epoch = _parse_time(reference[\"epoch_utc\"])\n        semi_major_axis_m = float(reference[\"semi_major_axis_km\"]) * 1_000.0\n        eccentricity = float(reference.get(\"eccentricity\", 0.0))\n        inclination = math.radians(float(reference.get(\"inclination_deg\", 0.0)))\n        raan = math.radians(float(reference.get(\"raan_deg\", 0.0)))\n        arg_perigee = math.radians(\n            float(reference.get(\"argument_of_perigee_deg\", 0.0))\n        )\n        mean_anomaly = math.radians(float(reference.get(\"mean_anomaly_deg\", 0.0)))\n\n        reference_elements = OrbitalElements(\n            semi_major_axis=semi_major_axis_m,\n            eccentricity=eccentricity,\n            inclination=inclination,\n            raan=raan,\n            arg_perigee=arg_perigee,\n            mean_anomaly=mean_anomaly,\n        )\n        offsets_m = _formation_offsets(float(formation[\"side_length_m\"]))\n        satellite_ids = tuple(sorted(offsets_m))\n\n        plane_allocations = formation.get(\"plane_allocations\", {})\n        if not plane_allocations:\n            plane_allocations = {\n                satellite_ids[0]: \"Plane A\",\n                satellite_ids[1]: \"Plane A\",\n                satellite_ids[2]: \"Plane B\",\n            }\n\n        # Get initial state of the reference orbit\n        ref_pos_at_epoch, ref_vel_at_epoch = propagate_kepler(reference_elements, 0)\n\n        # Get initial LVLH frame\n        frame_at_epoch = _lvlh_frame(ref_pos_at_epoch, ref_vel_at_epoch)\n\n        # Calculate initial orbital elements for each satellite\n        for sat_id in satellite_ids:\n            offset_vec = frame_at_epoch @ offsets_m[sat_id]\n            # For a stable formation with zero radial offset, the initial relative velocity in the LVLH frame should be zero.\n            offset_vel = np.array([0.0, 0.0, 0.0])\n\n            sat_pos = ref_pos_at_epoch + offset_vec\n            sat_vel = ref_vel_at_epoch + offset_vel\n            satellite_elements[sat_id] = cartesian_to_classical(sat_pos, sat_vel)\n\n    duration_s = float(formation.get(\"duration_s\", 180.0))\n    time_step_s = float(formation.get(\"time_step_s\", 1.0))\n    half_duration = 0.5 * duration_s\n    sample_count = int(round(duration_s / time_step_s)) + 1\n\n    offsets = np.arange(sample_count, dtype=float) * time_step_s - half_duration\n    times = [epoch + timedelta(seconds=float(offset)) for offset in offsets]\n\n    positions: dict[str, np.ndarray] = {\n        sat_id: np.zeros((sample_count, 3), dtype=float) for sat_id in satellite_ids\n    }\n    latitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    longitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    altitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n\n    triangle_area_series = np.zeros(sample_count, dtype=float)\n    triangle_aspect_series = np.zeros(sample_count, dtype=float)\n    triangle_sides_series = np.zeros((sample_count, 3), dtype=float)\n    centroid_positions = np.zeros((sample_count, 3), dtype=float)\n    centroid_latitudes = np.zeros(sample_count, dtype=float)\n    centroid_longitudes = np.zeros(sample_count, dtype=float)\n    centroid_altitudes = np.zeros(sample_count, dtype=float)\n    max_ground_distance = np.zeros(sample_count, dtype=float)\n    min_command_distance = np.full(sample_count, np.inf, dtype=float)\n\n    target = formation.get(\"target\", {})\n    target_lat = math.radians(float(target.get(\"latitude_deg\", 0.0)))\n    target_lon = math.radians(float(target.get(\"longitude_deg\", 0.0)))\n\n    command = formation.get(\"command\", {})\n    command_station = command.get(\"station\", {})\n    default_lat_deg = math.degrees(target_lat)\n    default_lon_deg = math.degrees(target_lon)\n    command_lat = math.radians(float(command_station.get(\"latitude_deg\", default_lat_deg)))\n    command_lon = math.radians(float(command_station.get(\"longitude_deg\", default_lon_deg)))\n\n    # Propagate each satellite independently\n    velocities_temp: dict[str, list[np.ndarray]] = {sat_id: [] for sat_id in satellite_ids}\n\n    perturbations = formation.get(\"perturbations\", {})\n    position_noise_sigma_m = float(perturbations.get(\"position_noise_sigma_m\", 0.0))\n\n    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    for index, delta_t in enumerate(offsets):\n        inertial_positions = {}\n        for sat_id in satellite_ids:\n            pos, vel = propagate_kepler(satellite_elements[sat_id], delta_t)\n\n            # if position_noise_sigma_m > 0.0:\n            #     noise = rng.normal(0.0, position_noise_sigma_m, size=3)\n            #     pos += noise\n\n            positions[sat_id][index] = pos\n            velocities_temp[sat_id].append(vel)\n            inertial_positions[sat_id] = pos\n\n            ecef = inertial_to_ecef(pos, times[index])\n            lat, lon, alt = geodetic_coordinates(ecef)\n            latitudes[sat_id][index] = lat\n            longitudes[sat_id][index] = lon\n            altitudes[sat_id][index] = alt\n\n        # Now, compute triangle geometry based on the actual positions\n        # The order of vertices matters for side length calculations, so sort by sat_id\n        vertices = [inertial_positions[sat_id] for sat_id in satellite_ids]\n\n        triangle_area_series[index] = triangle_area(vertices)\n        triangle_aspect_series[index] = triangle_aspect_ratio(vertices)\n        triangle_sides_series[index] = triangle_side_lengths(vertices)\n\n        centroid = sum(vertices) / len(vertices)\n        centroid_positions[index] = centroid\n        centroid_ecef = inertial_to_ecef(centroid, times[index])\n        centroid_lat, centroid_lon, centroid_alt = geodetic_coordinates(centroid_ecef)\n        centroid_latitudes[index] = centroid_lat\n        centroid_longitudes[index] = centroid_lon\n        centroid_altitudes[index] = centroid_alt\n\n        max_distance = 0.0\n        min_command = np.inf\n        for sat_id in satellite_ids:\n            distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                target_lat,\n                target_lon,\n            )\n            max_distance = max(max_distance, distance)\n            command_distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                command_lat,\n                command_lon,\n            )\n            min_command = min(min_command, command_distance)\n        max_ground_distance[index] = max_distance / 1_000.0\n        min_command_distance[index] = min_command\n\n    velocities: dict[str, np.ndarray] = {\n        sat_id: np.array(velocities_temp[sat_id]) for sat_id in satellite_ids\n    }\n\n    classical_series = _compute_classical_elements_series(positions, velocities)\n\n    finite_mask = np.isfinite(min_command_distance)\n    min_command_distance_km = np.where(\n        finite_mask, min_command_distance / 1_000.0, np.inf\n    )\n\n    centre_index = int(np.argmin(np.abs(offsets)))\n    orbital_elements = {}\n    for sat_id in satellite_ids:\n        elements = cartesian_to_classical(\n            positions[sat_id][centre_index], velocities[sat_id][centre_index]\n        )\n        orbital_elements[sat_id] = {\n            \"semi_major_axis_km\": elements.semi_major_axis / 1_000.0,\n            \"eccentricity\": elements.eccentricity,\n            \"inclination_deg\": _normalise_degrees(elements.inclination),\n            \"raan_deg\": _normalise_degrees(elements.raan),\n            \"argument_of_perigee_deg\": _normalise_degrees(elements.arg_perigee),\n            \"mean_anomaly_deg\": _normalise_degrees(elements.mean_anomaly),\n            \"assigned_plane\": plane_allocations.get(sat_id),\n        }\n\n    triangle_stats = _summarise_triangle_metrics(\n        triangle_area_series,\n        triangle_aspect_series,\n        triangle_sides_series,\n    )\n    ground_stats = _summarise_ground_metrics(max_ground_distance, formation)\n    maintenance = _estimate_maintenance_delta_v(\n        positions,\n        times,\n        formation,\n        semi_major_axis_m,\n    )\n    command_latency = _analyse_command_latency(\n        min_command_distance_km,\n        times,\n        formation,\n        semi_major_axis_m,\n        command_lat,\n        command_lon,\n    )\n    injection_recovery, injection_samples = _run_injection_recovery_monte_carlo(\n        satellite_ids,\n        formation,\n    )\n    drag_dispersion, drag_samples = _run_atmospheric_drag_dispersion_monte_carlo(\n        formation,\n        semi_major_axis_m,\n        inclination,\n    )\n\n    window, window_series = _formation_window(\n        triangle_aspect_series,\n        max_ground_distance,\n        time_step_s,\n        formation,\n        times,\n    )\n\n    recurrence = _summarise_window_recurrence(\n        window_series,\n        semi_major_axis_m,\n    )\n\n    station_keeping = _assess_station_keeping(\n        times,\n        triangle_sides_series,\n        formation,\n        maintenance,\n        time_step_s,\n    )\n\n    metrics: MutableMapping[str, object] = {\n        \"triangle\": triangle_stats,\n        \"ground_track\": ground_stats,\n        \"formation_window\": window,\n        \"formation_windows\": window_series,\n        \"formation_recurrence\": recurrence,\n        \"orbital_elements\": {\n            \"per_satellite\": orbital_elements,\n            \"plane_assignments\": plane_allocations,\n            \"time_series\": {\n                \"fields\": CLASSICAL_ELEMENT_FIELDS,\n                \"units\": {\n                    \"semi_major_axis_km\": \"km\",\n                    \"eccentricity\": \"\",\n                    \"inclination_deg\": \"deg\",\n                    \"raan_deg\": \"deg\",\n                    \"argument_of_perigee_deg\": \"deg\",\n                    \"mean_anomaly_deg\": \"deg\",\n                },\n                \"artefact_key\": \"orbital_elements_csv\",\n                \"per_satellite_directory_key\": \"orbital_elements_directory\",\n            },\n        },\n        \"maintenance\": maintenance,\n        \"station_keeping\": station_keeping,\n        \"command_latency\": command_latency,\n        \"injection_recovery\": injection_recovery,\n        \"drag_dispersion\": drag_dispersion,\n    }\n\n    artefacts: MutableMapping[str, Optional[str]] = {\n        \"summary_path\": None,\n        \"stk_directory\": None,\n        \"maintenance_csv\": None,\n        \"command_windows_csv\": None,\n        \"injection_recovery_csv\": None,\n        \"injection_recovery_plot\": None,\n        \"drag_dispersion_csv\": None,\n        \"formation_windows_csv\": None,\n        \"station_keeping_csv\": None,\n        \"orbital_elements_csv\": None,\n        \"orbital_elements_directory\": None,\n    }\n\n    result = TriangleFormationResult(\n        times=times,\n        positions_m=positions,\n        velocities_mps=velocities,\n        classical_elements=classical_series,\n        latitudes_rad=latitudes,\n        longitudes_rad=longitudes,\n        altitudes_m=altitudes,\n        triangle_area_m2=triangle_area_series,\n        triangle_aspect_ratio=triangle_aspect_series,\n        triangle_sides_m=triangle_sides_series,\n        centroid_lat_rad=centroid_latitudes,\n        centroid_lon_rad=centroid_longitudes,\n        centroid_alt_m=centroid_altitudes,\n        max_ground_distance_km=max_ground_distance,\n        min_command_distance_km=min_command_distance_km,\n        metrics=metrics,\n        artefacts=artefacts,\n    )\n\n    if output_directory is not None:\n        output_path = Path(output_directory)\n        output_path.mkdir(parents=True, exist_ok=True)\n        summary_path = output_path / \"triangle_summary.json\"\n        artefacts[\"summary_path\"] = str(summary_path)\n\n        maintenance_rows = [\n            {\n                \"satellite_id\": sat_id,\n                \"mean_diff_accel_mps2\": data[\"mean_diff_accel_mps2\"],\n                \"peak_diff_accel_mps2\": data[\"peak_diff_accel_mps2\"],\n                \"delta_v_per_burn_mps\": data[\"delta_v_per_burn_mps\"],\n                \"annual_delta_v_mps\": data[\"annual_delta_v_mps\"],\n            }\n            for sat_id, data in maintenance[\"per_spacecraft\"].items()\n        ]\n        maintenance_df = pd.DataFrame(\n            maintenance_rows,\n            columns=[\n                \"satellite_id\",\n                \"mean_diff_accel_mps2\",\n                \"peak_diff_accel_mps2\",\n                \"delta_v_per_burn_mps\",\n                \"annual_delta_v_mps\",\n            ],\n        )\n        maintenance_path = output_path / \"maintenance_summary.csv\"\n        maintenance_df.to_csv(maintenance_path, index=False)\n        artefacts[\"maintenance_csv\"] = str(maintenance_path)\n\n        command_rows = [\n            {\"window_index\": index, **window}\n            for index, window in enumerate(command_latency[\"contact_windows\"])\n        ]\n        command_df = pd.DataFrame(\n            command_rows, columns=[\"window_index\", \"start\", \"end\", \"duration_s\"]\n        )\n        command_path = output_path / \"command_windows.csv\"\n        command_df.to_csv(command_path, index=False)\n        artefacts[\"command_windows_csv\"] = str(command_path)\n\n        injection_path = output_path / \"injection_recovery.csv\"\n        injection_samples.to_csv(\n            injection_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"satellite_id\",\n                \"position_error_m\",\n                \"velocity_error_mps\",\n                \"delta_v_mps\",\n                \"success\",\n            ],\n        )\n        artefacts[\"injection_recovery_csv\"] = str(injection_path)\n\n        drag_path = output_path / \"drag_dispersion.csv\"\n        drag_samples.to_csv(\n            drag_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"density_scale\",\n                \"drag_coefficient\",\n                \"ballistic_coefficient_m2_per_kg\",\n                \"semi_major_axis_delta_m\",\n                \"altitude_delta_m\",\n                \"along_track_shift_km\",\n                \"ground_distance_delta_km\",\n                \"command_distance_delta_km\",\n                \"within_tolerance\",\n            ],\n        )\n        artefacts[\"drag_dispersion_csv\"] = str(drag_path)\n\n        windows_path = output_path / \"formation_windows.csv\"\n        _write_formation_windows_csv(windows_path, window_series)\n        artefacts[\"formation_windows_csv\"] = str(windows_path)\n\n        station_path = output_path / \"station_keeping_events.csv\"\n        _write_station_keeping_csv(station_path, station_keeping)\n        artefacts[\"station_keeping_csv\"] = str(station_path)\n\n        plot_path = output_path / \"injection_recovery_cdf.svg\"\n        _write_injection_recovery_plot(injection_samples, plot_path)\n        if plot_path.exists():\n            artefacts[\"injection_recovery_plot\"] = str(plot_path)\n\n        orbital_path = output_path / \"orbital_elements.csv\"\n        _write_orbital_elements_csv(orbital_path, times, classical_series)\n        artefacts[\"orbital_elements_csv\"] = str(orbital_path)\n        metrics[\"orbital_elements\"][\"time_series\"][\"artefact\"] = str(orbital_path)\n        orbital_sat_dir = output_path / \"orbital_elements\"\n        orbital_sat_dir.mkdir(parents=True, exist_ok=True)\n        per_sat_paths = _write_orbital_elements_per_spacecraft(\n            orbital_sat_dir, times, classical_series\n        )\n        if per_sat_paths:\n            artefacts[\"orbital_elements_directory\"] = str(orbital_sat_dir)\n            metrics[\"orbital_elements\"][\"time_series\"][\"per_satellite_files\"] = {\n                sat_id: str(path)\n                for sat_id, path in sorted(per_sat_paths.items(), key=lambda item: item[0])\n            }\n\n        stk_dir = output_path / \"stk\"\n        _export_to_stk(\n            result,\n            stk_dir,\n            scenario_name=str(metadata.get(\"scenario_name\", \"Tehran Triangle Formation\")),\n        )\n        artefacts[\"stk_directory\"] = str(stk_dir)\n\n        summary_payload = result.to_summary()\n        summary_path.write_text(\n            json.dumps(summary_payload, indent=2), encoding=\"utf-8\"\n        )\n\n    return result\n\n\ndef _load_configuration(source: Mapping[str, object] | Path | str) -> Mapping[str, object]:\n    if isinstance(source, Mapping):\n        return source\n    path = Path(source)\n    payload = json.loads(path.read_text(encoding=\"utf-8\"))\n    if not isinstance(payload, Mapping):\n        raise TypeError(\"Triangle configuration must be a mapping.\")\n    if \"satellites\" not in payload and \"reference_orbit\" not in payload:\n        raise ValueError(\n            \"Configuration missing required section: satellites or reference_orbit\"\n        )\n    if \"formation\" not in payload:\n        raise ValueError(\"Configuration missing required section: formation\")\n    return payload\n\n\ndef _parse_time(value: object) -> datetime:\n    if isinstance(value, datetime):\n        return value\n    if isinstance(value, str):\n        text = value.strip()\n        if text.endswith(\"Z\"):\n            text = text[:-1] + \"+00:00\"\n        return datetime.fromisoformat(text)\n    raise TypeError(\"Configuration epoch must be a datetime or ISO 8601 string.\")\n\n\ndef _lvlh_frame(position: Sequence[float], velocity: Sequence[float]) -> np.ndarray:\n    r = np.asarray(position, dtype=float)\n    v = np.asarray(velocity, dtype=float)\n    r_hat = r / np.linalg.norm(r)\n    h = np.cross(r, v)\n    k_hat = h / np.linalg.norm(h)\n    j_hat = np.cross(k_hat, r_hat)\n    j_hat /= np.linalg.norm(j_hat)\n    return np.column_stack((r_hat, j_hat, k_hat))\n\n\ndef _formation_offsets(side_length_m: float) -> Mapping[str, np.ndarray]:\n    \"\"\"Return equilateral offsets constrained to the local horizontal plane.\"\"\"\n\n    sqrt_three = math.sqrt(3.0)\n    return {\n        \"SAT-1\": np.array(\n            [0.0, -0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-2\": np.array(\n            [0.0, 0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-3\": np.array([0.0, 0.0, sqrt_three / 3.0 * side_length_m]),\n    }\n\n\ndef _differentiate(samples: np.ndarray, step: float) -> np.ndarray:\n    derivatives = np.zeros_like(samples)\n    count = samples.shape[0]\n    for index in range(count):\n        if 0 < index < count - 1:\n            derivatives[index] = (samples[index + 1] - samples[index - 1]) / (2.0 * step)\n        elif index == 0:\n            derivatives[index] = (samples[index + 1] - samples[index]) / step\n        else:\n            derivatives[index] = (samples[index] - samples[index - 1]) / step\n    return derivatives\n\n\ndef _compute_classical_elements_series(\n    positions: Mapping[str, np.ndarray],\n    velocities: Mapping[str, np.ndarray],\n) -> Mapping[str, Mapping[str, np.ndarray]]:\n    \"\"\"Evaluate classical orbital elements for each spacecraft over time.\"\"\"\n\n    if not positions:\n        return {}\n\n    sample_count = next(iter(positions.values())).shape[0]\n    series: dict[str, dict[str, np.ndarray]] = {}\n    for sat_id, position_history in positions.items():\n        per_satellite = {\n            name: np.zeros(sample_count, dtype=float) for name in CLASSICAL_ELEMENT_FIELDS\n        }\n        series[sat_id] = per_satellite\n        velocity_history = velocities.get(sat_id)\n        if velocity_history is None:\n            continue\n        for index in range(sample_count):\n            elements = cartesian_to_classical(\n                position_history[index], velocity_history[index]\n            )\n            per_satellite[\"semi_major_axis_km\"][index] = elements.semi_major_axis / 1_000.0\n            per_satellite[\"eccentricity\"][index] = elements.eccentricity\n            per_satellite[\"inclination_deg\"][index] = _normalise_degrees(\n                elements.inclination\n            )\n            per_satellite[\"raan_deg\"][index] = _normalise_degrees(elements.raan)\n            per_satellite[\"argument_of_perigee_deg\"][index] = _normalise_degrees(\n                elements.arg_perigee\n            )\n            per_satellite[\"mean_anomaly_deg\"][index] = _normalise_degrees(\n                elements.mean_anomaly\n            )\n\n    return series\n\n\ndef _normalise_degrees(angle_rad: float) -> float:\n    \"\"\"Convert *angle_rad* to degrees on the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _write_orbital_elements_csv(\n    path: Path, times: Sequence[datetime], series: Mapping[str, Mapping[str, np.ndarray]]\n) -> None:\n    \"\"\"Write a consolidated orbital-element history to *path*.\"\"\"\n\n    records: list[dict[str, object]] = []\n    satellite_ids = sorted(series)\n    for index, epoch in enumerate(times):\n        timestamp = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        for sat_id in satellite_ids:\n            elements = series.get(sat_id)\n            if not elements:\n                continue\n            record = {\"time_utc\": timestamp, \"satellite_id\": sat_id}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        columns = [\"time_utc\", \"satellite_id\", *CLASSICAL_ELEMENT_FIELDS]\n        dataframe = pd.DataFrame(columns=columns)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_orbital_elements_per_spacecraft(\n    directory: Path,\n    times: Sequence[datetime],\n    series: Mapping[str, Mapping[str, np.ndarray]],\n) -> Mapping[str, Path]:\n    \"\"\"Persist individual per-spacecraft orbital-element CSVs.\"\"\"\n\n    paths: dict[str, Path] = {}\n    for sat_id, elements in series.items():\n        records: list[dict[str, object]] = []\n        for index, epoch in enumerate(times):\n            record = {\"time_utc\": epoch.isoformat().replace(\"+00:00\", \"Z\")}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n        dataframe = pd.DataFrame.from_records(records)\n        if dataframe.empty:\n            dataframe = pd.DataFrame(columns=[\"time_utc\", *CLASSICAL_ELEMENT_FIELDS])\n        filename = f\"{sat_id.lower().replace(' ', '_')}_orbital_elements.csv\"\n        path = directory / filename\n        dataframe.to_csv(path, index=False)\n        paths[sat_id] = path\n    return paths\n\n\ndef _summarise_triangle_metrics(areas: np.ndarray, aspects: np.ndarray, sides: np.ndarray) -> Mapping[str, float]:\n    return {\n        \"mean_area_m2\": float(np.mean(areas)),\n        \"min_area_m2\": float(np.min(areas)),\n        \"max_area_m2\": float(np.max(areas)),\n        \"mean_aspect_ratio\": float(np.mean(aspects)),\n        \"aspect_ratio_max\": float(np.max(aspects)),\n        \"mean_side_lengths_m\": [float(val) for val in np.mean(sides, axis=0)],\n    }\n\n\ndef _summarise_ground_metrics(distances_km: np.ndarray, formation: Mapping[str, object]) -> Mapping[str, float]:\n    return {\n        \"max_ground_distance_km\": float(np.max(distances_km)),\n        \"min_ground_distance_km\": float(np.min(distances_km)),\n        \"ground_distance_tolerance_km\": float(formation.get(\"ground_tolerance_km\", 350.0)),\n    }\n\n\ndef _formation_window(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> tuple[Mapping[str, object], Sequence[Mapping[str, object]]]:\n    \"\"\"Derive the principal formation window together with the full schedule.\"\"\"\n\n    windows = list(\n        _enumerate_formation_windows(\n            aspects,\n            distances_km,\n            step,\n            formation,\n            times,\n        )\n    )\n    if not windows:\n        primary: Mapping[str, object] = {\n            \"duration_s\": 0.0,\n            \"start\": None,\n            \"end\": None,\n            \"sample_count\": 0,\n        }\n        return primary, windows\n\n    primary = max(windows, key=lambda item: float(item.get(\"duration_s\", 0.0)))\n    return primary, windows\n\n\ndef _enumerate_formation_windows(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> Iterable[Mapping[str, object]]:\n    \"\"\"Yield contiguous access windows that satisfy the ground-track criteria.\"\"\"\n\n    tolerance = float(formation.get(\"ground_tolerance_km\", 350.0))\n    aspect_limit = float(formation.get(\"aspect_ratio_tolerance\", 1.02))\n    mask = (distances_km <= tolerance) & (aspects <= aspect_limit)\n\n    start_index: Optional[int] = None\n    for index, valid in enumerate(mask):\n        if valid and start_index is None:\n            start_index = index\n            continue\n        if not valid and start_index is not None:\n            yield _describe_window(\n                start_index,\n                index - 1,\n                times,\n                distances_km,\n                aspects,\n                step,\n            )\n            start_index = None\n\n    if start_index is not None:\n        yield _describe_window(\n            start_index,\n            len(mask) - 1,\n            times,\n            distances_km,\n            aspects,\n            step,\n        )\n\n\ndef _describe_window(\n    start: int,\n    end: int,\n    times: Sequence[datetime],\n    distances_km: np.ndarray,\n    aspects: np.ndarray,\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Summarise a valid access window spanning indices ``start`` to ``end``.\"\"\"\n\n    start_time = times[start]\n    end_time = times[end]\n    if end > start:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        earlier = start_time - half_step\n        later = end_time + half_step\n        # Preserve the original timezone information when clamping.\n        if earlier < times[0]:\n            earlier = times[0]\n        if later > times[-1]:\n            later = times[-1]\n        duration_seconds = float(step)\n        start_time = earlier\n        end_time = later\n\n    slice_obj = slice(start, end + 1)\n    window_distances = distances_km[slice_obj]\n    window_aspects = aspects[slice_obj]\n    centroid_index = start + (end - start) // 2\n    centroid_time = times[centroid_index]\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"sample_count\": int(end - start + 1),\n        \"max_ground_distance_km\": float(np.max(window_distances)),\n        \"min_ground_distance_km\": float(np.min(window_distances)),\n        \"max_aspect_ratio\": float(np.max(window_aspects)),\n        \"centroid_time\": centroid_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n\n\ndef _summarise_window_recurrence(\n    windows: Sequence[Mapping[str, object]],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    \"\"\"Quantify the repeatability of access windows over the simulation horizon.\"\"\"\n\n    if not windows:\n        return {\n            \"window_count\": 0,\n            \"mean_duration_s\": 0.0,\n            \"min_duration_s\": 0.0,\n            \"max_duration_s\": 0.0,\n            \"mean_interval_s\": 0.0,\n            \"min_interval_s\": 0.0,\n            \"max_interval_s\": 0.0,\n            \"std_interval_s\": 0.0,\n            \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n            \"repeatability_score\": 0.0,\n            \"interval_samples\": 0,\n        }\n\n    durations = np.array([float(window.get(\"duration_s\", 0.0)) for window in windows])\n    mean_duration = float(np.mean(durations)) if durations.size else 0.0\n    min_duration = float(np.min(durations)) if durations.size else 0.0\n    max_duration = float(np.max(durations)) if durations.size else 0.0\n\n    midpoints: list[datetime] = []\n    for window in windows:\n        start = window.get(\"start\")\n        end = window.get(\"end\")\n        if not start or not end:\n            continue\n        start_time = _parse_time(start)\n        end_time = _parse_time(end)\n        delta = end_time - start_time\n        midpoint = start_time + delta / 2 if delta.total_seconds() >= 0.0 else start_time\n        midpoints.append(midpoint)\n\n    intervals = []\n    for first, second in zip(midpoints, midpoints[1:]):\n        delta = (second - first).total_seconds()\n        if delta > 0.0:\n            intervals.append(delta)\n\n    if intervals:\n        interval_array = np.asarray(intervals, dtype=float)\n        mean_interval = float(np.mean(interval_array))\n        min_interval = float(np.min(interval_array))\n        max_interval = float(np.max(interval_array))\n        std_interval = float(np.std(interval_array))\n        if mean_interval > 0.0:\n            repeatability = max(0.0, 1.0 - std_interval / mean_interval)\n        else:\n            repeatability = 0.0\n    else:\n        mean_interval = min_interval = max_interval = std_interval = 0.0\n        repeatability = 1.0 if len(windows) == 1 else 0.0\n\n    return {\n        \"window_count\": len(windows),\n        \"mean_duration_s\": mean_duration,\n        \"min_duration_s\": min_duration,\n        \"max_duration_s\": max_duration,\n        \"mean_interval_s\": mean_interval,\n        \"min_interval_s\": min_interval,\n        \"max_interval_s\": max_interval,\n        \"std_interval_s\": std_interval,\n        \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n        \"repeatability_score\": repeatability,\n        \"interval_samples\": len(intervals),\n    }\n\n\ndef _orbital_period(semi_major_axis_m: float) -> float:\n    if semi_major_axis_m <= 0.0:\n        return 0.0\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    return float(2.0 * math.pi / mean_motion)\n\n\ndef _assess_station_keeping(\n    times: Sequence[datetime],\n    triangle_sides: np.ndarray,\n    formation: Mapping[str, object],\n    maintenance: Mapping[str, object],\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Detect when the triangular geometry exceeds the maintenance tolerance.\"\"\"\n\n    if not len(times):\n        return {\n            \"status\": \"insufficient_data\",\n            \"events\": [],\n            \"tolerance_m\": 0.0,\n            \"violation_fraction\": 0.0,\n            \"recommended_delta_v_mps\": 0.0,\n            \"max_deviation_m\": 0.0,\n        }\n\n    base_length = float(formation.get(\"side_length_m\", 0.0))\n    default_tolerance = base_length * 0.01 if base_length > 0.0 else 10.0\n    tolerance = float(\n        formation.get(\n            \"station_keeping_tolerance_m\",\n            formation.get(\"repeatability_tolerance_m\", default_tolerance),\n        )\n    )\n    if tolerance <= 0.0:\n        tolerance = default_tolerance\n\n    deviations = np.abs(triangle_sides - base_length)\n    max_deviation_series = np.max(deviations, axis=1)\n    violation_mask = max_deviation_series > tolerance\n\n    events: list[Mapping[str, object]] = []\n    start_index: Optional[int] = None\n    peak_violation = 0.0\n    for index, violated in enumerate(violation_mask):\n        if violated:\n            if start_index is None:\n                start_index = index\n                peak_violation = float(max_deviation_series[index])\n            else:\n                peak_violation = max(peak_violation, float(max_deviation_series[index]))\n            continue\n        if start_index is not None:\n            events.append(\n                _build_station_event(\n                    start_index,\n                    index - 1,\n                    times,\n                    max_deviation_series,\n                    peak_violation,\n                    step,\n                )\n            )\n            start_index = None\n            peak_violation = 0.0\n\n    if start_index is not None:\n        events.append(\n            _build_station_event(\n                start_index,\n                len(times) - 1,\n                times,\n                max_deviation_series,\n                peak_violation,\n                step,\n            )\n        )\n\n    recommended_delta_v = _recommended_delta_v(maintenance)\n    for event in events:\n        event.setdefault(\"recommended_delta_v_mps\", recommended_delta_v)\n\n    violation_fraction = (\n        float(np.count_nonzero(violation_mask)) / float(len(max_deviation_series))\n        if len(max_deviation_series)\n        else 0.0\n    )\n\n    max_deviation = float(np.max(max_deviation_series)) if len(max_deviation_series) else 0.0\n\n    status = \"nominal\" if not events else \"correction_required\"\n    return {\n        \"status\": status,\n        \"events\": events,\n        \"tolerance_m\": tolerance,\n        \"violation_fraction\": violation_fraction,\n        \"recommended_delta_v_mps\": recommended_delta_v,\n        \"max_deviation_m\": max_deviation,\n    }\n\n\ndef _build_station_event(\n    start_index: int,\n    end_index: int,\n    times: Sequence[datetime],\n    deviations: np.ndarray,\n    peak_violation: float,\n    step: float,\n) -> Mapping[str, object]:\n    start_time = times[start_index]\n    end_time = times[end_index]\n    if end_index > start_index:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        start_time = max(start_time - half_step, times[0])\n        end_time = min(end_time + half_step, times[-1])\n        duration_seconds = float(step)\n\n    slice_obj = slice(start_index, end_index + 1)\n    peak = max(peak_violation, float(np.max(deviations[slice_obj])))\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"violation_samples\": int(end_index - start_index + 1),\n        \"peak_deviation_m\": float(peak),\n    }\n\n\ndef _recommended_delta_v(maintenance: Mapping[str, object]) -> float:\n    per_spacecraft = maintenance.get(\"per_spacecraft\") if isinstance(maintenance, Mapping) else None\n    if isinstance(per_spacecraft, Mapping) and per_spacecraft:\n        deltas = [\n            float(entry.get(\"delta_v_per_burn_mps\", 0.0))\n            for entry in per_spacecraft.values()\n        ]\n        if deltas:\n            return float(np.mean(deltas))\n\n    assumptions = maintenance.get(\"assumptions\") if isinstance(maintenance, Mapping) else None\n    if isinstance(assumptions, Mapping):\n        return float(assumptions.get(\"delta_v_budget_mps\", 0.0))\n    return 0.0\n\n\ndef _write_formation_windows_csv(\n    path: Path,\n    windows: Sequence[Mapping[str, object]],\n) -> None:\n    \"\"\"Serialise the detected formation windows to ``path``.\"\"\"\n\n    records: list[MutableMapping[str, object]] = []\n    for index, window in enumerate(windows):\n        record: MutableMapping[str, object] = {\"window_index\": index}\n        record[\"start\"] = window.get(\"start\")\n        record[\"end\"] = window.get(\"end\")\n        record[\"duration_s\"] = float(window.get(\"duration_s\", 0.0))\n        record[\"sample_count\"] = int(window.get(\"sample_count\", 0))\n        record[\"max_ground_distance_km\"] = float(\n            window.get(\"max_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"min_ground_distance_km\"] = float(\n            window.get(\"min_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"max_aspect_ratio\"] = float(window.get(\"max_aspect_ratio\", float(\"nan\")))\n        record[\"centroid_time\"] = window.get(\"centroid_time\")\n        records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"window_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"sample_count\",\n                \"max_ground_distance_km\",\n                \"min_ground_distance_km\",\n                \"max_aspect_ratio\",\n                \"centroid_time\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_station_keeping_csv(\n    path: Path,\n    station_keeping: Mapping[str, object],\n) -> None:\n    \"\"\"Write the station-keeping assessment events to ``path``.\"\"\"\n\n    events = station_keeping.get(\"events\") if isinstance(station_keeping, Mapping) else []\n    records: list[MutableMapping[str, object]] = []\n    if isinstance(events, Sequence):\n        for index, event in enumerate(events):\n            record: MutableMapping[str, object] = {\"event_index\": index}\n            record[\"start\"] = event.get(\"start\")\n            record[\"end\"] = event.get(\"end\")\n            record[\"duration_s\"] = float(event.get(\"duration_s\", 0.0))\n            record[\"violation_samples\"] = int(event.get(\"violation_samples\", 0))\n            record[\"peak_deviation_m\"] = float(event.get(\"peak_deviation_m\", 0.0))\n            record[\"recommended_delta_v_mps\"] = float(\n                event.get(\"recommended_delta_v_mps\", station_keeping.get(\"recommended_delta_v_mps\", 0.0))\n            )\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"event_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"violation_samples\",\n                \"peak_deviation_m\",\n                \"recommended_delta_v_mps\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _estimate_maintenance_delta_v(\n    positions: Mapping[str, np.ndarray],\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    maintenance = formation.get(\"maintenance\", {})\n    burn_duration_s = float(maintenance.get(\"burn_duration_s\", 45.0))\n    interval_days = float(maintenance.get(\"interval_days\", 7.0))\n    delta_v_budget_mps = float(maintenance.get(\"delta_v_budget_mps\", 15.0))\n    interval_seconds = max(interval_days * SECONDS_PER_DAY, 1.0)\n    burns_per_year = SECONDS_PER_YEAR / interval_seconds\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    centroid = sum(positions[sat_id] for sat_id in positions) / float(len(positions))\n    centroid_norm = np.linalg.norm(centroid, axis=1)\n    centroid_accel = -MU_EARTH * centroid / centroid_norm[:, None] ** 3\n\n    per_spacecraft: MutableMapping[str, Mapping[str, float]] = {}\n    mean_accels = []\n    peak_accels = []\n    annual_totals = []\n\n    for sat_id, states in positions.items():\n        radii = np.linalg.norm(states, axis=1)\n        accelerations = -MU_EARTH * states / radii[:, None] ** 3\n        differential = np.linalg.norm(accelerations - centroid_accel, axis=1)\n        mean_diff = float(np.mean(differential))\n        peak_diff = float(np.max(differential))\n        dv_per_burn = mean_diff * burn_duration_s\n        annual_delta_v = dv_per_burn * burns_per_year\n\n        mean_accels.append(mean_diff)\n        peak_accels.append(peak_diff)\n        annual_totals.append(annual_delta_v)\n\n        per_spacecraft[sat_id] = {\n            \"mean_diff_accel_mps2\": mean_diff,\n            \"peak_diff_accel_mps2\": peak_diff,\n            \"delta_v_per_burn_mps\": dv_per_burn,\n            \"annual_delta_v_mps\": annual_delta_v,\n        }\n\n    annual_array = np.asarray(annual_totals, dtype=float)\n    mean_accel = float(np.mean(mean_accels)) if mean_accels else 0.0\n    peak_accel = float(np.max(peak_accels)) if peak_accels else 0.0\n\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n\n    return {\n        \"assumptions\": {\n            \"burn_duration_s\": burn_duration_s,\n            \"maintenance_interval_days\": interval_days,\n            \"burns_per_year\": float(burns_per_year),\n            \"analysis_window_s\": float(max((count - 1), 0) * step),\n            \"orbit_period_s\": float(orbit_period),\n            \"delta_v_budget_mps\": delta_v_budget_mps,\n        },\n        \"mean_differential_acceleration_mps2\": mean_accel,\n        \"peak_differential_acceleration_mps2\": peak_accel,\n        \"per_spacecraft\": per_spacecraft,\n        \"annual_delta_v_mps\": {\n            \"mean\": float(np.mean(annual_array)) if annual_totals else 0.0,\n            \"max\": float(np.max(annual_array)) if annual_totals else 0.0,\n            \"min\": float(np.min(annual_array)) if annual_totals else 0.0,\n        },\n    }\n\n\ndef _analyse_command_latency(\n    min_distances_km: np.ndarray,\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    command_lat: float,\n    command_lon: float,\n) -> Mapping[str, object]:\n    command_cfg = formation.get(\"command\", {})\n    range_km = float(\n        command_cfg.get(\"contact_range_km\", formation.get(\"ground_tolerance_km\", 350.0))\n    )\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    mask = np.asarray(min_distances_km) <= range_km\n\n    windows: list[Mapping[str, object]] = []\n    current_start: Optional[int] = None\n    for index, in_contact in enumerate(mask):\n        if in_contact and current_start is None:\n            current_start = index\n        elif not in_contact and current_start is not None:\n            end_index = index - 1\n            windows.append(\n                _format_contact_window(times, current_start, end_index, step)\n            )\n            current_start = None\n    if current_start is not None:\n        windows.append(\n            _format_contact_window(times, current_start, len(mask) - 1, step)\n        )\n\n    contact_duration_s = float(sum(window[\"duration_s\"] for window in windows))\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n    contact_probability = contact_duration_s / orbit_period if orbit_period else 0.0\n    gap_s = max(orbit_period - contact_duration_s, 0.0)\n    max_latency_hours = gap_s / 3600.0\n    mean_latency_hours = max_latency_hours / 2.0\n\n    return {\n        \"station\": {\n            \"latitude_deg\": math.degrees(command_lat),\n            \"longitude_deg\": math.degrees(command_lon),\n            \"contact_range_km\": range_km,\n        },\n        \"contact_probability\": contact_probability,\n        \"contact_duration_s\": contact_duration_s,\n        \"contact_windows\": windows,\n        \"passes_per_day\": SECONDS_PER_DAY / orbit_period if orbit_period else 0.0,\n        \"max_latency_hours\": max_latency_hours,\n        \"mean_latency_hours\": mean_latency_hours,\n        \"latency_margin_hours\": 12.0 - max_latency_hours,\n        \"assumptions\": {\n            \"uniform_request_distribution\": True,\n            \"single_station_operations\": True,\n        },\n    }\n\n\ndef _format_contact_window(\n    times: Sequence[datetime], start_index: int, end_index: int, step: float\n) -> Mapping[str, object]:\n    count = end_index - start_index + 1\n    duration = max(count - 1, 0) * step\n    return {\n        \"start\": times[start_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": times[end_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration),\n    }\n\n\ndef _run_injection_recovery_monte_carlo(\n    satellite_ids: Sequence[str], formation: Mapping[str, object]\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    config = formation.get(\"monte_carlo\", {})\n    sample_count = int(config.get(\"samples\", 200))\n    position_sigma_m = float(config.get(\"position_sigma_m\", 250.0))\n    velocity_sigma_mmps = float(config.get(\"velocity_sigma_mmps\", 5.0))\n    velocity_sigma_mps = velocity_sigma_mmps / 1_000.0\n    recovery_time_s = float(config.get(\"recovery_time_s\", 12.0 * 3600.0))\n    delta_v_budget = float(config.get(\"delta_v_budget_mps\", 15.0))\n    seed = config.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    records: list[Mapping[str, object]] = []\n    for sample_index in range(sample_count):\n        for sat_id in satellite_ids:\n            position_error = rng.normal(0.0, position_sigma_m, size=3)\n            velocity_error = rng.normal(0.0, velocity_sigma_mps, size=3)\n            position_mag = float(np.linalg.norm(position_error))\n            velocity_mag = float(np.linalg.norm(velocity_error))\n            delta_v_position = 2.0 * position_mag / recovery_time_s\n            delta_v_total = delta_v_position + velocity_mag\n            success = delta_v_total <= delta_v_budget\n            records.append(\n                {\n                    \"sample_id\": sample_index,\n                    \"satellite_id\": sat_id,\n                    \"position_error_m\": position_mag,\n                    \"velocity_error_mps\": velocity_mag,\n                    \"delta_v_mps\": delta_v_total,\n                    \"success\": bool(success),\n                }\n            )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"success_rate\": 1.0,\n            \"mean_delta_v_mps\": 0.0,\n            \"p95_delta_v_mps\": 0.0,\n            \"max_delta_v_mps\": 0.0,\n        }\n        per_spacecraft = {sat_id: aggregate for sat_id in satellite_ids}\n        success_rate = 1.0\n    else:\n        success_rate = float(dataframe[\"success\"].mean())\n        per_spacecraft = {}\n        for sat_id, group in dataframe.groupby(\"satellite_id\"):\n            per_spacecraft[str(sat_id)] = {\n                \"mean_delta_v_mps\": float(group[\"delta_v_mps\"].mean()),\n                \"p95_delta_v_mps\": float(group[\"delta_v_mps\"].quantile(0.95)),\n                \"max_delta_v_mps\": float(group[\"delta_v_mps\"].max()),\n                \"success_rate\": float(group[\"success\"].mean()),\n            }\n        aggregate = {\n            \"mean_delta_v_mps\": float(dataframe[\"delta_v_mps\"].mean()),\n            \"p95_delta_v_mps\": float(dataframe[\"delta_v_mps\"].quantile(0.95)),\n            \"max_delta_v_mps\": float(dataframe[\"delta_v_mps\"].max()),\n        }\n\n    metrics = {\n        \"success_rate\": success_rate,\n        \"sample_count\": sample_count,\n        \"per_spacecraft\": per_spacecraft,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"position_sigma_m\": position_sigma_m,\n            \"velocity_sigma_mmps\": velocity_sigma_mmps,\n            \"recovery_time_s\": recovery_time_s,\n            \"delta_v_budget_mps\": delta_v_budget,\n        },\n    }\n\n    return metrics, dataframe\n\n\ndef _run_atmospheric_drag_dispersion_monte_carlo(\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    inclination_rad: float,\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    settings = formation.get(\"drag_dispersion\", {})\n    sample_count = int(settings.get(\"samples\", 200))\n    density_sigma = float(settings.get(\"density_sigma\", 0.2))\n    drag_coefficient_sigma = float(settings.get(\"drag_coefficient_sigma\", 0.05))\n    reference_density = float(settings.get(\"reference_density_kg_m3\", 3.5e-12))\n    drag_coefficient = float(settings.get(\"drag_coefficient\", 2.2))\n    ballistic_area = float(settings.get(\"reference_area_m2\", 1.0))\n    spacecraft_mass = float(settings.get(\"spacecraft_mass_kg\", 150.0))\n    time_horizon_orbits = float(settings.get(\"time_horizon_orbits\", 15.0))\n    integration_step = float(settings.get(\"integration_step_s\", 60.0))\n    seed = settings.get(\"seed\")\n\n    if spacecraft_mass <= 0.0:\n        raise ValueError(\"Spacecraft mass must be positive for drag dispersions.\")\n\n    rng = np.random.default_rng(seed)\n    ballistic_coeff_base = ballistic_area / spacecraft_mass\n\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    orbital_period = 2.0 * math.pi / mean_motion\n    horizon = max(time_horizon_orbits, 0.0) * orbital_period\n    horizon = max(horizon, integration_step)\n\n    records: list[Mapping[str, object]] = []\n    tolerance_km = float(formation.get(\"ground_tolerance_km\", 350.0))\n    orbital_radius = semi_major_axis_m\n    ground_projection_scale = math.cos(inclination_rad)\n\n    time_samples = np.arange(0.0, horizon + 0.5 * integration_step, integration_step)\n\n    for sample_id in range(sample_count):\n        density_scale = max(0.0, 1.0 + rng.normal(0.0, density_sigma))\n        cd_scale = max(0.0, 1.0 + rng.normal(0.0, drag_coefficient_sigma))\n\n        rho = reference_density * density_scale\n        cd = drag_coefficient * cd_scale\n        ballistic_coefficient = ballistic_coeff_base\n\n        orbital_velocity = math.sqrt(MU_EARTH / semi_major_axis_m)\n        drag_acceleration = 0.5 * rho * cd * ballistic_coefficient * orbital_velocity**2\n        da_dt = -2.0 * semi_major_axis_m**2 * drag_acceleration / MU_EARTH\n\n        semi_major_axis_series = semi_major_axis_m + da_dt * time_samples\n        semi_major_axis_series = np.maximum(semi_major_axis_series, EARTH_EQUATORIAL_RADIUS_M + 150_000.0)\n\n        altitude_delta = semi_major_axis_series[-1] - semi_major_axis_m\n        new_mean_motion = math.sqrt(MU_EARTH / semi_major_axis_series[-1] ** 3)\n        delta_theta = (new_mean_motion - mean_motion) * horizon\n        along_track_shift = abs(delta_theta) * orbital_radius / 1_000.0\n\n        ground_distance_delta = along_track_shift * abs(ground_projection_scale)\n        command_distance_delta = along_track_shift\n\n        within_tolerance = ground_distance_delta <= tolerance_km\n\n        records.append(\n            {\n                \"sample_id\": sample_id,\n                \"density_scale\": float(density_scale),\n                \"drag_coefficient\": float(cd),\n                \"ballistic_coefficient_m2_per_kg\": float(ballistic_coefficient),\n                \"semi_major_axis_delta_m\": float(altitude_delta),\n                \"altitude_delta_m\": float(altitude_delta),\n                \"along_track_shift_km\": float(along_track_shift),\n                \"ground_distance_delta_km\": float(ground_distance_delta),\n                \"command_distance_delta_km\": float(command_distance_delta),\n                \"within_tolerance\": bool(within_tolerance),\n            }\n        )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": 0.0,\n            \"max_ground_distance_delta_km\": 0.0,\n            \"p95_along_track_shift_km\": 0.0,\n            \"max_along_track_shift_km\": 0.0,\n            \"p95_altitude_delta_m\": 0.0,\n        }\n        success_rate = 1.0\n    else:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].quantile(0.95)\n            ),\n            \"max_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].max()\n            ),\n            \"p95_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].quantile(0.95)\n            ),\n            \"max_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].max()\n            ),\n            \"p95_altitude_delta_m\": float(\n                dataframe[\"altitude_delta_m\"].quantile(0.95)\n            ),\n        }\n        success_rate = float(dataframe[\"within_tolerance\"].mean())\n\n    metrics = {\n        \"sample_count\": sample_count,\n        \"success_rate\": success_rate,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"density_sigma\": density_sigma,\n            \"drag_coefficient_sigma\": drag_coefficient_sigma,\n            \"reference_density_kg_m3\": reference_density,\n            \"drag_coefficient\": drag_coefficient,\n            \"reference_area_m2\": ballistic_area,\n            \"spacecraft_mass_kg\": spacecraft_mass,\n            \"time_horizon_orbits\": time_horizon_orbits,\n            \"integration_step_s\": integration_step,\n        },\n        \"tolerance_km\": tolerance_km,\n    }\n\n    return metrics, dataframe\n\n\ndef _write_injection_recovery_plot(samples: pd.DataFrame, output_path: Path) -> None:\n    if samples.empty:\n        return\n\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt  # noqa: WPS433 - imported for plotting\n\n    values = np.sort(samples[\"delta_v_mps\"].to_numpy(dtype=float))\n    cumulative = (np.arange(len(values), dtype=float) + 1.0) / float(len(values))\n\n    figure, axis = plt.subplots(figsize=(6.0, 4.0))\n    axis.plot(values, cumulative, color=\"#1f77b4\", linewidth=2.0)\n    axis.set_xlabel(\"Δv per spacecraft (m/s)\")\n    axis.set_ylabel(\"Cumulative probability\")\n    axis.set_title(\"Injection recovery Δv distribution\")\n    axis.grid(True, alpha=0.3)\n    figure.tight_layout()\n    figure.savefig(output_path, format=\"svg\")\n    plt.close(figure)\n\ndef _export_to_stk(result: TriangleFormationResult, output_dir: Path, scenario_name: str) -> None:\n    output_dir.mkdir(parents=True, exist_ok=True)\n    state_histories = []\n    ground_tracks = []\n    contacts = []\n\n    satellite_ids = sorted(result.positions_m)\n    safe_satellite_names = unique_stk_names(satellite_ids)\n    safe_facility_name = sanitize_stk_identifier(\"Tehran\", default=\"Facility\")\n    safe_scenario_name = sanitize_stk_identifier(scenario_name, default=\"Scenario\")\n\n    for sat_id in satellite_ids:\n        safe_id = safe_satellite_names[sat_id]\n        samples = [\n            StateSample(\n                epoch=epoch,\n                position_eci_km=result.positions_m[sat_id][index] / 1_000.0,\n                velocity_eci_kms=result.velocities_mps[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        state_histories.append(PropagatedStateHistory(satellite_id=safe_id, samples=samples))\n\n        track_points = [\n            GroundTrackPoint(\n                epoch=epoch,\n                latitude_deg=math.degrees(result.latitudes_rad[sat_id][index]),\n                longitude_deg=math.degrees(result.longitudes_rad[sat_id][index]),\n                altitude_km=result.altitudes_m[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        ground_tracks.append(GroundTrack(satellite_id=safe_id, points=track_points))\n\n    window = result.metrics[\"formation_window\"]\n    if window.get(\"start\") and window.get(\"end\"):\n        start = datetime.fromisoformat(window[\"start\"].replace(\"Z\", \"+00:00\"))\n        end = datetime.fromisoformat(window[\"end\"].replace(\"Z\", \"+00:00\"))\n        for sat_id in satellite_ids:\n            safe_id = safe_satellite_names[sat_id]\n            contacts.append(\n                GroundContactInterval(\n                    satellite_id=safe_id,\n                    facility_name=safe_facility_name,\n                    start=start,\n                    end=end,\n                )\n            )\n\n    facilities = [\n        FacilityDefinition(\n            name=safe_facility_name,\n            latitude_deg=35.6892,\n            longitude_deg=51.3890,\n            altitude_km=1.5,\n        )\n    ]\n\n    scenario_metadata = ScenarioMetadata(\n        scenario_name=safe_scenario_name,\n        start_epoch=result.times[0],\n        stop_epoch=result.times[-1],\n        central_body=\"Earth\",\n        coordinate_frame=\"TEME\",\n        ephemeris_step_seconds=(result.times[1] - result.times[0]).total_seconds()\n        if len(result.times) > 1\n        else None,\n    )\n\n    sim_results = SimulationResults(\n        state_histories=state_histories,\n        ground_tracks=ground_tracks,\n        ground_contacts=contacts,\n        facilities=facilities,\n    )\n\n    export_simulation_to_stk(sim_results, output_dir, scenario_metadata)\n\n\n__all__ = [\"simulate_triangle_formation\", \"TriangleFormationResult\"]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Okay, `simulate_triangle_formation` in `sim/formation/triangle.py` uses `duration_s` from `config/scenarios/tehran_triangle.json` to control simulation time. To run a 14-day simulation (1,209,600 seconds), I'll copy the config to `tehran_triangle_14_day.json`, update `duration_s`, and then execute the simulation with that new config, outputting to `artefacts/gemini_run_14_day`. I'll create the file now."
      },
      {
        "functionCall": {
          "name": "write_file",
          "args": {
            "content": "{\n  \"metadata\": {\n    \"scenario_name\": \"Tehran Triangle Formation (14-day)\",\n    \"description\": \"Three-satellite equilateral formation centred on Tehran with 6 km side length, propagated for 14 days.\",\n    \"author\": \"Gemini\",\n    \"validated_against_stk_export\": true,\n    \"alignment_validation\": {\n      \"run_id\": \"run_20260321_0740Z\",\n      \"epoch_utc\": \"2026-03-21T07:40:10Z\"\n    }\n  },\n  \"reference_orbit\": {\n    \"epoch_utc\": \"2026-03-21T07:40:10Z\",\n    \"semi_major_axis_km\": 6939.242,\n    \"eccentricity\": 0.0,\n    \"inclination_deg\": 97.7,\n    \"raan_deg\": 350.9838169642857,\n    \"argument_of_perigee_deg\": 0.0,\n    \"mean_anomaly_deg\": 36.06454701821796\n  },\n  \"formation\": {\n    \"side_length_m\": 6000.0,\n    \"duration_s\": 1209600.0,\n    \"time_step_s\": 1.0,\n    \"ground_tolerance_km\": 350.0,\n    \"aspect_ratio_tolerance\": 1.02,\n    \"target\": {\n      \"latitude_deg\": 35.6892,\n      \"longitude_deg\": 51.3890\n    },\n    \"plane_allocations\": {\n      \"SAT-1\": \"Plane A\",\n      \"SAT-2\": \"Plane A\",\n      \"SAT-3\": \"Plane B\"\n    },\n    \"maintenance\": {\n      \"burn_duration_s\": 32.0,\n      \"interval_days\": 7.0,\n      \"delta_v_budget_mps\": 15.0\n    },\n    \"command\": {\n      \"contact_range_km\": 2200.0,\n      \"station\": {\n        \"latitude_deg\": 30.283,\n        \"longitude_deg\": 57.083\n      }\n    },\n    \"monte_carlo\": {\n      \"samples\": 300,\n      \"position_sigma_m\": 250.0,\n      \"velocity_sigma_mmps\": 5.0,\n      \"recovery_time_s\": 43200.0,\n      \"delta_v_budget_mps\": 15.0,\n      \"seed\": 314159\n    },\n    \"drag_dispersion\": {\n      \"samples\": 200,\n      \"density_sigma\": 0.25,\n      \"drag_coefficient_sigma\": 0.05,\n      \"reference_density_kg_m3\": 3.2e-12,\n      \"drag_coefficient\": 2.2,\n      \"reference_area_m2\": 1.1,\n      \"spacecraft_mass_kg\": 165.0,\n      \"time_horizon_orbits\": 12.0,\n      \"integration_step_s\": 120.0,\n      \"seed\": 20260321\n    },\n    \"perturbations\": {\n      \"position_noise_sigma_m\": 1.0,\n      \"seed\": 12345\n    }\n  }\n}\n",
            "file_path": "/home/diannenewton170/formation-sat-2/config/scenarios/tehran_triangle_14_day.json"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "write_file-1762791294613-2bc43c305df4a",
          "name": "write_file",
          "response": {
            "output": "Successfully created and wrote to new file: /home/diannenewton170/formation-sat-2/config/scenarios/tehran_triangle_14_day.json."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XsU0d3r673WPE8+ynUTgKuWfMWg206OS0+cfPu099jMKdwHj8f9e58V6XZl9eKXRXvv5csu5+TPKYS70aBaYcvgoRhBq7xgS+VNxvxfu/X6cK/CC6o42x48EcfJDMmzdMe70NcUvPKYMaYsErMZ4wOZdDmWeKahGWzxK6VlxtloABVEwYscwjv+THxkIiH7yQU1tNAWmC5OMClAB4/H/Xvrf535Slt97Nq/EZUU7wyZrw1CRXOfn3/vGwfCllw2NBuO/YhGS3G+4IZVaj96HNgDRDoPlQQLTdrHg4sDzjjdQ82TWMG9oIreJDwqGAQHj8f9eeVnps5uaVlEKpn68zE32+4jV1Y+WcRSCsfXRG0hyFm2kfMAmVazJdi9Vf7wQtUMpt3VPdYo5PhmJbDPtPMiHAD8a/NvHR9ScyP+I1Mc+epD6iE91+OyBUHRuV3eQgwevfQXINFYuGSawldyNiXyByH6ycrHritAURIIFaVjjfRg8CokBAePx/14o39slUq/5NQ7KmbU9St6G853OFOEV0+4b8abNNDAN5pOp8nbgV1JAshw0RXtRqpQKdRv2ujsLs2nnEg6mzxyhKs5bCUevctFd7glNHtBuO9aib58aQcGSiwca4UmRgYPa2F7Dhs4dvJpv+M4qtV6rSWH+nut3Gy0NI12CimQX/1Gv7qcKqAEB4/H/Xmf2BJ0lpemX9eVapkVuEUymedBCJtmsK7MShc+txWd+hgBjBqOp8sVNEjI/ti4YivBXR1FKvcgHOWfEh4lX99dhc3cv/v68wya1sp6Dwzp3xA09fkiuzJsQpoidupOKdjttI4yGoii695o/ekqXclyBkZqaNyDhrGSKmBlPjA0BRrqAeynNl08GfQTrsDmiHfy16sZ3uxMal0mTnuu+/LZca80=",
        "text": "Configuration file created. Now, I'm running the simulation with the command: `.venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day`. Once it's done, I'll analyze the results and generate the markdown report."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "description": "Running the 14-day triangle formation simulation.",
            "command": ".venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762791328798-ed813ca4c4153",
          "name": "run_shell_command",
          "response": {
            "error": "[Operation Cancelled] Reason: User cancelled tool execution."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "continue"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "Ci8B4/H/XrujDvinnIqwC6gIj0yJGhloJ5yTFZ876haBYzLadnVwcC1hX4cpknn/AApwAePx/17VIM+NO+QtxTdC0UYYmrU0a6wfPUMFLzNLVNTZCpMKPqeEy1nD4B761RIIZRxCo4V8CdAKiN5sZ6E60fJGh+nRrD0/1PNmGzng/TjpGbuLwxkl3UDcg/nt7OtGiHwwEUZFZ0c0lEYrW6R1lgpBAePx/16KL+j4VrxhVM2HunWy993QCRwtZZ30WvHSoWo46PnSnaDYoBhx+jnJ8m4eGJp1w9NkUO4JAfDzS5RCE5I=",
        "text": "Okay, I'll restart the 14-day simulation."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "command": ".venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day",
            "description": "Running the 14-day triangle formation simulation."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762791931327-9a15af6a2eeae",
          "name": "run_shell_command",
          "response": {
            "error": "[Operation Cancelled] Reason: User cancelled tool execution."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "ران 14 روزه انجام نمیشه هر چقدر صبر کردم؛ یک کاری کن، درصد پیشرفت و وضعیت ایجاد کن برای همه ی انواع رانتوی پروژه"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XgEuqt0B5Nr9JZ9exY+RxSDmdgiVBaJgRHNs+W2QWZ0KbwHj8f9enyb+dLGBFWuNvFOFIvsU0xmqqlQrCpK+0/WlvkgvQh1duog6VJWVrhjf7ZPkikd6+FbIxX55eQTSJYVKyhwBPVphMiMkyOASMcTHegg5ANPZ5BU45ZWogjCxZzv6T263zOv56eZuqovg1gprAePx/17RY86p1btCmLBAT9gbsmi9cW315+nfq54XMogl+9dD9Zbsc8shAcCMC6lFaY0Ov9p27N5iUsWpdwjJ6RKZprY42VPAlALFfcvOFB9CgGbH7wlwiY4UaKNpk6Gtd/D6KMkwRYF3SgsKqwEB4/H/Xl4SJA88NNEV+4vd09OwUZbtQixf1ntY2Ca6S3XtxDRGSyIN6J2JOgQUzXIhMyhdby/+sUcsZSuiDovRyhLDv0nRGp0mgU4PUIMcpSixby+VdJg2XaeisigMDFyleD6xXw+UVbJyJaPjE+86jG8zf+kar7B4cnyaTwMZbapXqLMGr3TcNhTCDC6c+EMJJAddwHGBNmu/pFzWH53Isyrgu4swqMHZzvkKugEB4/H/XnRkR9jWe+8NbwQi2OGdxiLLp4dC3jhkkbLpqToNETALSKbkol/Sycsy2FM9Ortn2Uq1K9obyNlPINv9hx2lTb0XoP4QdrKc3JK3+ESBTZd6RKhJ0ioGy/HdKDRNGwT41IMq+2lKrq0dyhbh3JOk0yjbJf3n8NL7On3P18ebkYKAiWBdXjDABqUB3HkSWBwsAI+TQnwWL+x+/764WpQTssvYoSz7OXVS73/We8Bh3ZqBy8Pku1kK5gEB4/H/XmWHpK0oaLRod/Ux33gkHQgw9YqqbkXghrQ1hzDx/7YUsiJRBYykpiALMkR61/q+Bgci7As11vEVZvcNMvHCvl4IHvvSRu8CYs2EntTi44Ol5Cx28VjOXZrr38zld9cB+ypWd1uU7nCH6KCt72/pLRoxerwBmwDNSSrrLP5ft1gKJ09ikFkxO0jNFUlZysOlAWuoXK9QyHykotmZmIPdlShBqHtxUl8N1CMELWcg+m2uVrkYsy4dNnrmC77IbxScjea7ynQAu5N+LHFW988o49hiQwMW+0/djKPMue36k9ycgQq0AQHj8f9erziTbpAuZ8e1jSsP8D8600lkKJweiUcfgRF9VXcktCHzehUG4gAiWr5H4810Kg2ZwrcokVMve3MGcALUsYhXumk8ulxhIVumu7Yotl+d16zK9AWwAFiHPNGL/d0UUoWeXYTyzDLV8hXvvjG2q/UqBnl3Pi/thrAoYZKCtM9goMKlGB3CJHo6vLVQZi5agGBzQaF8vlhfZPlwsY4NoBiAiT4b1YYHhf7dUAAP3xQQ4Q==",
        "text": "The user wants a progress bar for all simulations since the 14-day run is too slow. I'll add `tqdm` to `sim/formation/triangle.py`'s main loop in `simulate_triangle_formation`. First, I'll check if `tqdm` is in `requirements.txt`; if not, I'll add it and install."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/requirements.txt"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762792318099-0bef5e54a3d26",
          "name": "read_file",
          "response": {
            "output": "# Core runtime library underpinning deterministic numerical handling\nnumpy>=1.24\n\n# Scientific analysis stack retained for continuity with STK validation workflows\nscipy>=1.11\nastropy>=5\npoliastro>=0.17; python_version < \"3.11\"\nnumba>=0.58\njplephem>=2.20\n\n# Plotting and briefing artefact generation\nmatplotlib>=3.7\npandas>=2.2\nplotly>=5.15\ncartopy>=0.22\n\n# Development and continuous-integration tooling\npytest>=8\npytest-cov>=6\nsetuptools>=70\nwheel\n\n# Web service interfaces for orchestration endpoints\nfastapi>=0.111\nuvicorn>=0.29\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "ClYB4/H/XuLkLbuEsTP/mK9N885CZdewI3yTuMKehFnfNyjXWGEOOy9XSDgHjZ4hv4Bn52LwwDc9DxOEcujMobpsFeV5IGM41lrU+hevhNkNDCEbekBtQgpXAePx/163HsWr/HamPoKIC3lVW3DSbVKUGHfpNtZ4NAO57fPZVxCRoGWEQEG++wDysoZU1pRYNn2PG9kRhikKY+QUMnbIZ+D0FSHV2/7NH+Mc2CqvCpSaCqQBAePx/153BDJ58ABjV/cvzzO7RJYlZStFXx0xqeaghUlB2XcVSlZ7DIOtB5kXOlLn8Mwo3JX2KPXqNFxG8vdXD/ocIddQi/X9NyDSui56JjgEbAb1R6VNYtUIHD9IfnzTM94wujtC0E+hOV3dSna+ZIEw2frGBbdwD+r2Qj3yKE7yi0GrsMwEL8NEvGmSWnF7zOJHtFbpAMm8EL2I9XIIQtDTVhoKbQHj8f9eCbrkiigWpxZyQT/0DXp3tpVQ8Oez3IB9H5xNNOuOiDxDtRLU11iWoA8Tg2xwvq4TNRFApbkc4882Lm+HClc7Ds5PdGw3/2S8L4ZOTNgeblw2hG9KcEKTHZsWr/dA0GB1lHI7Z38jMEYKNQHj8f9ekCi7+D0538o+lgfgBse5BOyhQwnE9+JV9bvXnt5c3RqF7YoCBfdgfH9ivNZnx/39",
        "text": "`tqdm` is missing. I'll add `tqdm>=4.62` to `requirements.txt`, run `make setup`, then modify `simulate_triangle_formation` in `sim/formation/triangle.py` to use it."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/requirements.txt",
            "old_string": "uvicorn>=0.29\n",
            "new_string": "uvicorn>=0.29\ntqdm>=4.62\n",
            "instruction": "Add the tqdm library to the list of dependencies in the requirements.txt file. This is needed to show a progress bar during long simulations."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762792322982-97f1d04e80e7e",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/requirements.txt (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/Xv7PAJsXyUMH5BvFtKgdAZFsRSN9/zn7eeiBdyhu89AKUQHj8f9e9+FAGy1ktgIe7F6YP5obICDqrjxD2T2+NadQ1r0NXUYmPNx6DHOjJT3Nu5fXz64/oL6TfbcvFscd6o3Z7ngcHtMn0GqUjjyROF9rYQpSAePx/16mrUpzHWfH+aW7a8fw/Ir9SXXJQW9J8gz8LI+tmPWzYBX3quQKkCqNMfVlrAEYfbPRB1D0Lj3LLGssQeHN1j7JFZW7Z3GshwuMww/KJA==",
        "text": "`tqdm` is in `requirements.txt`. I'll run `make setup` to install it."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "command": "make setup",
            "description": "Installing tqdm after adding it to requirements.txt."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762792327679-3a6e86b14496e",
          "name": "run_shell_command",
          "response": {
            "output": "Command: make setup\nDirectory: (root)\nOutput: python3 -m venv .venv                                                                                                                                                      \n.venv/bin/python -m pip install --upgrade pip                                                                                                                              \nRequirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.3)                                                                                          \n.venv/bin/pip install -r requirements.txt                                                                                                                                  \nIgnoring poliastro: markers 'python_version < \"3.11\"' don't match your environment                                                                                         \nRequirement already satisfied: numpy>=1.24 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.3.4)                                             \nRequirement already satisfied: scipy>=1.11 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.16.3)                                            \nRequirement already satisfied: astropy>=5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (7.1.1)                                              \nRequirement already satisfied: numba>=0.58 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.62.1)                                            \nRequirement already satisfied: jplephem>=2.20 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.23)                                           \nRequirement already satisfied: matplotlib>=3.7 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (3.10.7)                                       \nRequirement already satisfied: pandas>=2.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.3.3)                                            \nRequirement already satisfied: plotly>=5.15 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (6.4.0)                                           \nRequirement already satisfied: cartopy>=0.22 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.25.0)                                         \nRequirement already satisfied: pytest>=8 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (9.0.0)                                              \nRequirement already satisfied: pytest-cov>=6 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (7.0.0)                                          \nRequirement already satisfied: setuptools>=70 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (80.9.0)                                        \nRequirement already satisfied: wheel in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (0.45.1)                                                 \nRequirement already satisfied: fastapi>=0.111 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (0.121.1)                                       \nRequirement already satisfied: uvicorn>=0.29 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (0.38.0)                                         \nCollecting tqdm>=4.62 (from -r requirements.txt (line 26))                                                                                                                 \n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)                                                                                                                \nRequirement already satisfied: pyerfa>=2.0.1.1 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (2.0.1.5)                           \nRequirement already satisfied: astropy-iers-data>=0.2025.9.29.0.35.48 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (0.2025.11.10\n.0.38.31)                                                                                                                                                                  \nRequirement already satisfied: PyYAML>=6.0.0 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (6.0.3)                               \nRequirement already satisfied: packaging>=22.0.0 in ./.venv/lib/python3.12/site-packages (from astropy>=5->-r requirements.txt (line 6)) (25.0)                            \nRequirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in ./.venv/lib/python3.12/site-packages (from numba>=0.58->-r requirements.txt (line 8)) (0.45.1)                \nRequirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (1.3.3)                      \nRequirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (0.12.1)                         \nRequirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (4.60.1)                    \nRequirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (1.4.9)                     \nRequirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (12.0.0)                            \nRequirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (3.2.5)                          \nRequirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib>=3.7->-r requirements.txt (line 12)) (2.9.0.post0)            \nRequirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=2.2->-r requirements.txt (line 13)) (2025.2)                             \nRequirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=2.2->-r requirements.txt (line 13)) (2025.2)                           \nRequirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.12/site-packages (from plotly>=5.15->-r requirements.txt (line 14)) (2.10.2)                        \nRequirement already satisfied: shapely>=2.0 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (2.1.2)                            \nRequirement already satisfied: pyshp>=2.3 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (3.0.2.post1)                        \nRequirement already satisfied: pyproj>=3.3.1 in ./.venv/lib/python3.12/site-packages (from cartopy>=0.22->-r requirements.txt (line 15)) (3.7.2)                           \nRequirement already satisfied: iniconfig>=1.0.1 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (2.3.0)                            \nRequirement already satisfied: pluggy<2,>=1.5 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (1.6.0)                              \nRequirement already satisfied: pygments>=2.7.2 in ./.venv/lib/python3.12/site-packages (from pytest>=8->-r requirements.txt (line 18)) (2.19.2)                            \nRequirement already satisfied: coverage>=7.10.6 in ./.venv/lib/python3.12/site-packages (from coverage[toml]>=7.10.6->pytest-cov>=6->-r requirements.txt (line 19)) (7.11.3\n)                                                                                                                                                                          \nRequirement already satisfied: starlette<0.50.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (0.49.3)             \nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.t\nxt (line 24)) (2.12.4)                                                                                                                                                     \nRequirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (4.15.0)              \nRequirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.111->-r requirements.txt (line 24)) (0.0.3)                   \nRequirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=\n0.111->-r requirements.txt (line 24)) (0.7.0)                                                                                                                              \nRequirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0\n.111->-r requirements.txt (line 24)) (2.41.5)                                                                                                                              \nRequirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi\n>=0.111->-r requirements.txt (line 24)) (0.4.2)                                                                                                                            \nRequirement already satisfied: anyio<5,>=3.6.2 in ./.venv/lib/python3.12/site-packages (from starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (line 24)) (4.1\n1.0)                                                                                                                                                                       \nRequirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (lin\ne 24)) (3.11)                                                                                                                                                              \nRequirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111->-r requirements.txt (\nline 24)) (1.3.1)                                                                                                                                                          \nRequirement already satisfied: click>=7.0 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.29->-r requirements.txt (line 25)) (8.3.0)                              \nRequirement already satisfied: h11>=0.8 in ./.venv/lib/python3.12/site-packages (from uvicorn>=0.29->-r requirements.txt (line 25)) (0.16.0)                               \nRequirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from pyproj>=3.3.1->cartopy>=0.22->-r requirements.txt (line 15)) (2025.10.5)              \nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7->-r requirements.txt (line 12)) (1.17.0)       \nDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)                                                                                                                           \nInstalling collected packages: tqdm                                                                                                                                        \nSuccessfully installed tqdm-4.67.1                                                                                                                                         \nEnvironment initialised in .venv.\nError: (none)\nExit Code: 0\nSignal: 0\nBackground PIDs: (none)\nProcess Group PGID: 1942"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "Ci8B4/H/XnXNVKt5cD5iq20Od3CkwYsIzL4S70Zwl3tfttfl+IpPXPteKHFkXtTBbApZAePx/148YSizeRLX1L6bcIVzIODOw4avbQAKoUw6mUv6dNbwFwVcB7JCZ9TKI1/wAwNfiRpeWlN7G2pd1b4gCgPu75cgupiDEZL0Hciu8y3kwpK9yin8uYwKWAHj8f9eMBHQSSn3KgLMc25fM4D6VTxC6zdcG0pb0MIh5IAKTk440YYv0fnHIo/KAnFfoFQQdS4XKbXxE32NmoU1CIHCGeo1KdY56fieQVsC0WRjHBKpzu8KoAEB4/H/Xr1mpF0fRmvroLTqBjdvYmDquCPNZhHdGAXXraijTvZUN1fLMwKcQvznG0Ele9oqx0Qa+jszkV2cKti8yFE/q36wWyFdc6kmA7ueHXpLoDoELwx1G7CdPceaLJl6jsqiyEduqQNoRFbRhNZgzZWF5m61z7H+7IUdlAkI3UaXys4q+bSZqTOX2I+HzB9AIQyrDlAzkBpU7GXgQOJhCpgBAePx/14khAzt7k9mVpDMRBgmfAagIaECobKtNZtaKndnCLvi1IlTz8B2usorXHLQHKct7R4ciGzeVS2CVUvS22e1zIWT8brOFgGsuSlt52ePqxhB7XJWYPQzmnR0w0gAHcdKbaxf1LstknjIy25kz+S23E8qvHU1vNDfXoVAAOBtM36Mbf8/C/+bDpPI5EAZvlIWoLRc6DQKawHj8f9ei8yq8bRtskOe3M02VpreI9roqByQK279tWZWlMnoJ47ty23Q/U+ICWLCrVG8kin/VQEG0zRXlQAA8vWu2sLKZFUpNdLIswVTp8A8HUAdJQTB9KTya83+QHJDbZK3h6/ObgeHbfe6",
        "text": "Okay, `tqdm` is installed. I'll now modify `sim/formation/triangle.py` to add a progress bar. I'll import `tqdm` and wrap the main propagation loop, changing `for index, delta_t in enumerate(offsets):` to `for index, delta_t in enumerate(tqdm(offsets, desc=\"Propagating orbits\")):` using the `replace` tool."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/formation/triangle.py",
            "new_string": "    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    from tqdm import tqdm\n    for index, delta_t in enumerate(tqdm(offsets, desc=\"Propagating orbits\")):",
            "old_string": "    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    for index, delta_t in enumerate(offsets):",
            "instruction": "Add a tqdm progress bar to the main propagation loop in the simulate_triangle_formation function to show progress during long simulations."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762792340510-03904b99870cb",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/formation/triangle.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjgB4/H/XkuUqUG+zglkMUCp20YAyKqZRLj/UCFWAb4L8cjIiga+d0PVMyDHzUhb3L0SQaVrWiWvcgpZAePx/17HnVROeFQo/iMdZFEaWhWYg06Rc/aI8LTTtli0XonP8QUWzadq4Cwo/EtiIzDm3gRYAmNLtcORtWN2nz5llvA9I6/rWT/bvhUyFqamOBu95kVRlTwKhAEB4/H/XlzJcwJ4KjUWW7LrSyQcXRI6LhLfVJ3ESkSNoxnweLVrPOeE3maIxcB98YTbfsiHMe8aCruJQMX0XvPnXT3P5m3TKcjmmWfnbWgYWJva6WhxVnwGIGtN6WGLa8qeCLPN5IP7inEIh+3hqxEpJg5xPpYWKXScwBdAJh4TRukZzO8KhAEB4/H/XnTSQlHfByDPWJosJ3iGtYBYUz2OCEOfkvGf62EDV76pgErFk4xkCujn+pGQwseOBrNuBLFtD2jbPxalq2e5qXDhmuar8U8ADbblxX6edEqgxOZrAPs0NVVLR60AtSObW5a8jBy3+2DmxY7iVrkKSQA+ECN4JiZ+/TCeRWYAg+oKjgEB4/H/XooBHk98huEHj4Rr3+z44jfLdRGeRw8odJdusKKgDMxabQXwZArVlpi/7foU8TxXy040pVgpjjedCtP7Rt56IEfUAzF/EJrl1oNA8tv5Tl07n5fT81y4Bz7FeXFTV8doCBKDs5vmPKm/zM7WBFsf4aRVXmUES0aB+Z1CzRHKDDE5QHOUKA1xCy/P",
        "text": "Okay, progress bar for all sims. Triangle is done. Now, `run_scenario.py`: I'll read it and see what's next."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/run_scenario.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762792344397-47cdf8a7b2b2d",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"Command-line entry point for the lightweight scenario pipeline.\n\nThe runner orchestrates the placeholder formation-flying workflow used by the\ncontinuous integration (CI) smoke tests.  Although the numerical models remain\nrudimentary, the script reflects the order of operations expected from the\nfuture high-fidelity pipeline:\n\n1. Discover access nodes describing imaging and downlink opportunities.\n2. Translate nodes into mission phases with basic timing metadata.\n3. Propagate the constellation with a two-body approximation.\n4. Propagate with a high-fidelity \\(J_2\\)+drag force model capturing cross-track\n   behaviour for each vehicle.\n5. Derive headline metrics that downstream reporting utilities can consume.\n\nEach stage records a summary that is serialisable to JavaScript Object Notation\n(JSON) and therefore importable into Systems Tool Kit (STK 11.2) validation\nworkflows.  The CLI accepts either a named scenario (resolved relative to the\n``config/scenarios`` directory) or an explicit configuration file path.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport csv\nimport json\nimport logging\nimport math\nimport tempfile\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone, timedelta\nfrom pathlib import Path\nfrom statistics import fmean\nfrom typing import Iterable, Mapping, MutableMapping, Optional, Sequence\n\nfrom tools.render_scenario_plots import generate_visualisations as generate_scenario_plots\nfrom tools.stk_export import (\n    FacilityDefinition,\n    GroundContactInterval,\n    GroundTrack,\n    GroundTrackPoint,\n    PropagatedStateHistory,\n    ScenarioMetadata,\n    SimulationResults,\n    StateSample,\n    export_simulation_to_stk,\n)\n\nfrom . import configuration\nfrom . import perturbation_analysis\n\nLOGGER = logging.getLogger(__name__)\n\nMU_EARTH_KM3_S2 = 398_600.4418\nEARTH_RADIUS_KM = 6_378.1363\nEARTH_FLATTENING = 1.0 / 298.257_223_563\nEARTH_ECCENTRICITY_SQUARED = 2.0 * EARTH_FLATTENING - EARTH_FLATTENING**2\nEARTH_SECOND_ECCENTRICITY_SQUARED = (\n    EARTH_ECCENTRICITY_SQUARED / (1.0 - EARTH_ECCENTRICITY_SQUARED)\n)\n\n\n@dataclass\nclass Node:\n    \"\"\"Mission node capturing contact opportunities or operational windows.\"\"\"\n\n    identifier: int\n    label: str\n    kind: str\n    start: Optional[datetime]\n    end: Optional[datetime]\n    duration_s: float\n    attributes: MutableMapping[str, object]\n\n    def as_dict(self) -> MutableMapping[str, object]:\n        \"\"\"Return a JSON-serialisable representation.\"\"\"\n\n        return {\n            \"id\": self.identifier,\n            \"label\": self.label,\n            \"type\": self.kind,\n            \"start\": _format_time(self.start),\n            \"end\": _format_time(self.end),\n            \"duration_s\": float(self.duration_s),\n            \"attributes\": dict(self.attributes),\n        }\n\n\n@dataclass\nclass Phase:\n    \"\"\"Mission phase derived from the access node plan.\"\"\"\n\n    identifier: int\n    node_id: Optional[int]\n    objective: str\n    start: Optional[datetime]\n    end: Optional[datetime]\n    duration_s: float\n\n    def as_dict(self) -> MutableMapping[str, object]:\n        \"\"\"Return a JSON-serialisable representation.\"\"\"\n\n        return {\n            \"id\": self.identifier,\n            \"node_id\": self.node_id,\n            \"objective\": self.objective,\n            \"start\": _format_time(self.start),\n            \"end\": _format_time(self.end),\n            \"duration_s\": float(self.duration_s),\n        }\n\n\ndef run_scenario(\n    config_source: configuration.ConfigSource,\n    output_directory: Optional[Path | str] = None,\n    *,\n    metrics_specification: Optional[Sequence[str]] = None,\n) -> MutableMapping[str, object]:\n    \"\"\"Execute the sequential scenario pipeline.\"\"\"\n\n    scenario = _load_configuration(config_source)\n    LOGGER.debug(\"Loaded scenario configuration with keys: %s\", list(scenario))\n\n    stage_sequence: list[str] = []\n\n    raan_alignment = _resolve_raan_alignment(scenario)\n    if raan_alignment:\n        stage_sequence.append(\"raan_alignment\")\n        best = raan_alignment.get(\"best_evaluation\", {})\n        LOGGER.info(\n            \"Optimised RAAN from %.6f° to %.6f°; centroid |cross-track| at midpoint: %.3f km.\",\n            raan_alignment.get(\"initial_raan_deg\", 0.0),\n            raan_alignment.get(\"optimised_raan_deg\", 0.0),\n            float(best.get(\"centroid_abs_cross_track_km\", math.inf)),\n        )\n\n    nodes = _generate_nodes(scenario)\n    stage_sequence.append(\"access_nodes\")\n    LOGGER.info(\"Identified %d mission nodes.\", len(nodes))\n\n    phases = _generate_phases(scenario, nodes)\n    stage_sequence.append(\"mission_phases\")\n    LOGGER.info(\"Synthesised %d mission phases.\", len(phases))\n\n    two_body = _propagate_two_body(scenario, phases)\n    stage_sequence.append(\"two_body_propagation\")\n    LOGGER.info(\n        \"Two-body propagation yielded an orbital period of %.2f s.\",\n        two_body[\"orbital_period_s\"],\n    )\n\n    perturbed, propagation_artefacts = _apply_j2_drag(\n        two_body,\n        phases,\n        scenario,\n        output_directory=output_directory,\n        raan_alignment=raan_alignment,\n    )\n    stage_sequence.append(\"high_fidelity_j2_drag_propagation\")\n    LOGGER.info(\n        \"Perturbed propagation adjusts the orbital period by %.3f s.\",\n        perturbed[\"orbital_period_s\"] - two_body[\"orbital_period_s\"],\n    )\n\n    metrics = _summarise_metrics(nodes, phases, two_body, perturbed, metrics_specification)\n    stage_sequence.append(\"metric_extraction\")\n    LOGGER.info(\n        \"Metric extraction complete: %s.\",\n        {key: metrics[key] for key in (\"node_count\", \"phase_count\", \"total_contact_duration_s\")},\n    )\n\n    stk_export_summary: MutableMapping[str, object] = {}\n    if output_directory is not None:\n        export_dir = Path(output_directory)\n        stk_export_summary = _export_stk_products(\n            export_dir,\n            scenario,\n            nodes,\n            two_body,\n        )\n        if stk_export_summary:\n            stage_sequence.append(\"stk_export\")\n            LOGGER.info(\n                \"STK export created with artefacts: %s.\",\n                stk_export_summary.get(\"stk_export_files\", []),\n            )\n    else:\n        LOGGER.debug(\"Skipping STK export because no output directory was provided.\")\n\n    summary: MutableMapping[str, object] = {\n        \"configuration_summary\": _summarise_configuration(scenario),\n        \"nodes\": [node.as_dict() for node in nodes],\n        \"phases\": [phase.as_dict() for phase in phases],\n        \"propagation\": {\"two_body\": two_body, \"j2_drag\": perturbed},\n        \"metrics\": metrics,\n        \"stage_sequence\": stage_sequence,\n    }\n\n    if raan_alignment:\n        summary[\"raan_alignment\"] = raan_alignment\n\n    artefacts: MutableMapping[str, Optional[str] | object] = {\"summary_path\": None}\n    artefacts.update(propagation_artefacts)\n    artefacts.update(stk_export_summary)\n    if output_directory:\n        try:\n            scenario_plots = generate_scenario_plots(Path(output_directory))\n        except Exception as exc:  # pragma: no cover - defensive guard\n            LOGGER.warning(\n                \"Scenario plot generation failed for %s: %s\",\n                output_directory,\n                exc,\n            )\n        else:\n            if scenario_plots:\n                artefacts[\"scenario_plots\"] = {\n                    key: str(path) for key, path in scenario_plots.items()\n                }\n    if output_directory:\n        output_dir = Path(output_directory)\n        artefacts[\"summary_path\"] = str(output_dir / \"scenario_summary.json\")\n        summary[\"artefacts\"] = artefacts\n        output_path = _write_summary(output_dir, summary)\n        LOGGER.info(\"Scenario summary written to %s\", output_path)\n    else:\n        summary[\"artefacts\"] = artefacts\n\n    return summary\n\n\ndef parse_args(args: Optional[Iterable[str]] = None) -> argparse.Namespace:\n    \"\"\"Parse command-line arguments for the scenario runner.\"\"\"\n\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"scenario\",\n        nargs=\"?\",\n        help=(\n            \"Scenario identifier or configuration file path.\"\n            \" Defaults to the canonical Tehran daily pass scenario.\"\n        ),\n    )\n    parser.add_argument(\n        \"--config\",\n        type=Path,\n        help=\"Explicit configuration file path overriding the scenario identifier.\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=Path,\n        help=\"Directory in which to store the pipeline summary JSON file.\",\n    )\n    parser.add_argument(\n        \"--metrics\",\n        nargs=\"*\",\n        metavar=\"METRIC\",\n        help=\"Optional list of metric identifiers to prioritise in the summary.\",\n    )\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"],\n        help=\"Logging verbosity for diagnostic output.\",\n    )\n    return parser.parse_args(args)\n\n\ndef main(args: Optional[Iterable[str]] = None) -> int:\n    \"\"\"Entry point enabling ``python -m sim.scripts.run_scenario`` execution.\"\"\"\n\n    namespace = parse_args(args)\n    logging.basicConfig(level=getattr(logging, namespace.log_level))\n\n    config_source: configuration.ConfigSource\n    if namespace.config:\n        config_source = namespace.config\n    elif namespace.scenario:\n        config_source = namespace.scenario\n    else:\n        config_source = \"tehran_daily_pass\"\n\n    results = run_scenario(\n        config_source,\n        output_directory=namespace.output_dir,\n        metrics_specification=namespace.metrics,\n    )\n\n    stage_description = \", \".join(results[\"stage_sequence\"])\n    identifier = results[\"configuration_summary\"].get(\"identifier\")\n    display_name = results[\"configuration_summary\"].get(\"name\", identifier or \"scenario\")\n    print(f\"Executed scenario '{display_name}' with stages: {stage_description}.\")\n    summary_path = results[\"artefacts\"].get(\"summary_path\")\n    if summary_path:\n        print(f\"Summary written to {summary_path}.\")\n    return 0\n\n\ndef _load_configuration(source: configuration.ConfigSource) -> MutableMapping[str, object]:\n    \"\"\"Load or copy the scenario configuration from *source*.\"\"\"\n\n    if isinstance(source, Mapping):\n        return dict(source)\n    return configuration.load_scenario(source)\n\n\ndef _resolve_raan_alignment(\n    scenario: MutableMapping[str, object]\n) -> MutableMapping[str, object]:\n    \"\"\"Optimise the RAAN to minimise centroid cross-track at the window midpoint.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, MutableMapping):\n        return {}\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, MutableMapping):\n        return {}\n    if \"raan_deg\" not in classical:\n        return {}\n\n    solver_config = scenario.get(\"raan_alignment\")\n    if isinstance(solver_config, bool) and not solver_config:\n        return {}\n    if not isinstance(solver_config, Mapping):\n        solver_config = {}\n\n    window_start, window_end, window_midpoint = perturbation_analysis.scenario_access_window(\n        scenario\n    )\n\n    midpoint = _parse_time(solver_config.get(\"midpoint_utc\"))\n    if midpoint is None:\n        midpoint = window_midpoint or _raan_target_midpoint(scenario)\n    if midpoint is None:\n        return {}\n\n    initial_raan = float(_coerce_float(classical, \"raan_deg\", 0.0))\n\n    if window_start and window_end:\n        window_duration = (window_end - window_start).total_seconds()\n    else:\n        window_duration = float(solver_config.get(\"window_duration_s\", 90.0))\n    time_step = float(solver_config.get(\"time_step_s\", 10.0))\n    search_span = float(solver_config.get(\"search_span_deg\", 5.0))\n    samples = max(int(solver_config.get(\"samples_per_iteration\", 9)), 3)\n    iterations = max(int(solver_config.get(\"iterations\", 4)), 1)\n    coarse_samples = max(int(solver_config.get(\"coarse_samples\", 36)), 3)\n    margin_s = max(float(solver_config.get(\"propagation_margin_s\", 300.0)), 120.0)\n    coarse_range = solver_config.get(\"coarse_range_deg\", (0.0, 360.0))\n    if (\n        isinstance(coarse_range, Sequence)\n        and len(coarse_range) == 2\n        and all(isinstance(value, (int, float)) for value in coarse_range)\n    ):\n        coarse_lower = float(coarse_range[0])\n        coarse_upper = float(coarse_range[1])\n    else:\n        coarse_lower, coarse_upper = 0.0, 360.0\n\n    margin = timedelta(seconds=margin_s)\n    if window_start:\n        start_time = window_start - margin\n    else:\n        start_time = midpoint - margin\n    if window_end:\n        stop_time = window_end + margin\n    else:\n        stop_time = midpoint + margin\n\n    epoch_time = _parse_time(orbital.get(\"epoch_utc\")) or start_time\n\n    primary_limit, waiver_limit, plane_limit = perturbation_analysis.scenario_cross_track_limits(\n        scenario\n    )\n\n    baseline = _evaluate_raan_candidate(\n        scenario,\n        initial_raan,\n        start_time,\n        stop_time,\n        epoch_time,\n        time_step,\n        window_duration,\n        midpoint,\n        primary_limit,\n        waiver_limit,\n        plane_limit,\n    )\n\n    evaluations: list[MutableMapping[str, object]] = [baseline]\n    best_result: MutableMapping[str, object] | None = baseline\n\n    for index in range(coarse_samples):\n        if coarse_samples == 1:\n            candidate = coarse_lower\n        else:\n            fraction = index / (coarse_samples - 1)\n            candidate = coarse_lower + (coarse_upper - coarse_lower) * fraction\n        result = _evaluate_raan_candidate(\n            scenario,\n            candidate,\n            start_time,\n            stop_time,\n            epoch_time,\n            time_step,\n            window_duration,\n            midpoint,\n            primary_limit,\n            waiver_limit,\n            plane_limit,\n        )\n        evaluations.append(result)\n        if not best_result or (\n            result.get(\"centroid_abs_cross_track_km\", math.inf)\n            < best_result.get(\"centroid_abs_cross_track_km\", math.inf)\n        ):\n            best_result = result\n\n    if not best_result:\n        return {}\n\n    centre_raan = float(best_result.get(\"raan_deg\", initial_raan))\n    lower = centre_raan - search_span\n    upper = centre_raan + search_span\n\n    for _ in range(iterations):\n        iteration_results: list[MutableMapping[str, object]] = []\n        for index in range(samples):\n            if samples == 1:\n                candidate = lower\n            else:\n                fraction = index / (samples - 1)\n                candidate = lower + (upper - lower) * fraction\n            result = _evaluate_raan_candidate(\n                scenario,\n                candidate,\n                start_time,\n                stop_time,\n                epoch_time,\n                time_step,\n                window_duration,\n                midpoint,\n                primary_limit,\n                waiver_limit,\n                plane_limit,\n            )\n            evaluations.append(result)\n            iteration_results.append(result)\n            if not best_result or (\n                result.get(\"centroid_abs_cross_track_km\", math.inf)\n                < best_result.get(\"centroid_abs_cross_track_km\", math.inf)\n            ):\n                best_result = result\n\n        if not iteration_results:\n            break\n\n        iteration_best = min(\n            iteration_results,\n            key=lambda item: item.get(\"centroid_abs_cross_track_km\", math.inf),\n        )\n        centre = float(iteration_best.get(\"raan_deg\", initial_raan))\n        span = max((upper - lower) / max(samples - 1, 1), 1e-4)\n        lower = centre - span\n        upper = centre + span\n\n    baseline_value = baseline.get(\"centroid_abs_cross_track_km\", math.inf)\n    best_value = best_result.get(\"centroid_abs_cross_track_km\", math.inf) if best_result else math.inf\n    if (\n        isinstance(baseline_value, (int, float))\n        and isinstance(best_value, (int, float))\n        and float(best_value) < float(baseline_value)\n    ):\n        optimised_raan = float(best_result.get(\"raan_deg\", initial_raan))\n    else:\n        best_result = baseline\n        optimised_raan = float(initial_raan)\n\n    classical[\"raan_deg\"] = optimised_raan\n\n    summary: MutableMapping[str, object] = {\n        \"initial_raan_deg\": float(initial_raan),\n        \"optimised_raan_deg\": optimised_raan,\n        \"target_midpoint_utc\": _format_time(midpoint),\n        \"window_duration_s\": window_duration,\n        \"time_step_s\": time_step,\n        \"evaluations\": evaluations,\n        \"best_evaluation\": best_result,\n    }\n\n    worst_candidate = best_result.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n    if isinstance(worst_candidate, (int, float)):\n        worst_vehicle = float(worst_candidate)\n    else:\n        worst_vehicle = math.inf\n    summary[\"worst_vehicle_abs_cross_track_km\"] = worst_vehicle\n\n    centroid_candidate = best_result.get(\"centroid_abs_cross_track_km\", math.inf)\n    if isinstance(centroid_candidate, (int, float)):\n        centroid_abs = float(centroid_candidate)\n    else:\n        centroid_abs = math.inf\n    summary[\"centroid_abs_cross_track_km\"] = centroid_abs\n\n    return summary\n\n\ndef _raan_target_midpoint(scenario: Mapping[str, object]) -> Optional[datetime]:\n    \"\"\"Return the midpoint of the primary access window for RAAN alignment.\"\"\"\n\n    start, end, midpoint = perturbation_analysis.scenario_access_window(scenario)\n    if midpoint is not None:\n        return midpoint\n\n    orbital = scenario.get(\"orbital_elements\")\n    if isinstance(orbital, Mapping):\n        epoch = _parse_time(orbital.get(\"epoch_utc\"))\n        if epoch is not None:\n            return epoch\n\n    timing = scenario.get(\"timing\")\n    if isinstance(timing, Mapping):\n        windows = timing.get(\"daily_access_windows\")\n        if isinstance(windows, Sequence) and windows:\n            first = windows[0] if isinstance(windows[0], Mapping) else {}\n            start = _parse_time(first.get(\"start_utc\"))\n            end = _parse_time(first.get(\"end_utc\"))\n            if start and end:\n                midpoint = start + (end - start) / 2\n                return midpoint\n            if start:\n                return start + timedelta(seconds=45.0)\n\n    start = _parse_time(_extract_planning_horizon(scenario, \"start_utc\"))\n    stop = _parse_time(_extract_planning_horizon(scenario, \"stop_utc\"))\n    if start and stop:\n        return start + (stop - start) / 2\n    if start:\n        return start + timedelta(seconds=45.0)\n    return None\n\n\ndef _evaluate_raan_candidate(\n    scenario: Mapping[str, object],\n    raan_deg: float,\n    start_time: datetime,\n    stop_time: datetime,\n    epoch_time: datetime,\n    time_step_s: float,\n    window_duration_s: float,\n    evaluation_time: datetime,\n    primary_limit_km: float,\n    waiver_limit_km: float,\n    plane_limit_km: float | None,\n) -> MutableMapping[str, object]:\n    \"\"\"Evaluate centroid cross-track performance for a RAAN candidate.\"\"\"\n\n    scenario_copy = deepcopy(scenario)\n    orbital = scenario_copy.setdefault(\"orbital_elements\", {})\n    classical = orbital.setdefault(\"classical\", {})\n    normalised_raan = _normalise_raan_deg(raan_deg)\n    classical[\"raan_deg\"] = normalised_raan\n\n    settings = perturbation_analysis.PropagatorSettings(\n        start_time=start_time,\n        epoch_time=epoch_time,\n        stop_time=stop_time,\n        time_step_s=time_step_s,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=perturbation_analysis.SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit_km,\n        waiver_cross_track_limit_km=waiver_limit_km,\n        plane_intersection_limit_km=plane_limit_km,\n    )\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        perturbation_analysis.propagate_constellation(\n            scenario_copy,\n            output_directory=Path(tmp_dir),\n            settings=settings,\n            monte_carlo={\"enabled\": False, \"runs\": 0},\n        )\n        cross_track_path = Path(tmp_dir) / \"deterministic_cross_track.csv\"\n        samples = _load_cross_track_series(cross_track_path)\n\n    sample_time = None\n    sample_values: MutableMapping[str, float] = {}\n    best_diff = math.inf\n    for timestamp, values in samples:\n        if not values:\n            continue\n        diff = abs((timestamp - evaluation_time).total_seconds())\n        if diff < best_diff:\n            best_diff = diff\n            sample_time = timestamp\n            sample_values = values\n\n    if sample_time is None:\n        sample_time = evaluation_time\n        sample_values = {}\n\n    centroid = fmean(sample_values.values()) if sample_values else math.nan\n    abs_values = {identifier: abs(value) for identifier, value in sample_values.items()}\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    centroid_abs = abs(centroid) if math.isfinite(centroid) else math.inf\n    worst_abs = max(abs_values.values(), default=math.inf)\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in abs_values.values() if math.isfinite(value)\n    ]\n    waiver_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    window_half = timedelta(seconds=0.5 * window_duration_s)\n    window_start = evaluation_time - window_half\n    window_end = evaluation_time + window_half\n\n    centroid_value_out = float(centroid) if math.isfinite(centroid) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    result: MutableMapping[str, object] = {\n        \"raan_deg\": normalised_raan,\n        \"evaluation_time_utc\": _format_time(evaluation_time),\n        \"sample_time_utc\": _format_time(sample_time),\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"window_start_utc\": _format_time(window_start),\n        \"window_end_utc\": _format_time(window_end),\n        \"vehicle_cross_track_km\": {\n            k: (float(v) if math.isfinite(v) else None)\n            for k, v in sample_values.items()\n        },\n        \"vehicle_abs_cross_track_km\": {\n            k: (float(v) if math.isfinite(v) else None)\n            for k, v in abs_values.items()\n        },\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": primary_limit,\n        \"waiver_limit_km\": waiver_limit,\n        \"primary_pass_fraction\": 1.0 if primary_compliant else 0.0,\n        \"waiver_pass_fraction\": float(waiver_fraction),\n    }\n\n    return result\n\n\ndef _load_cross_track_series(\n    csv_path: Path,\n) -> list[tuple[datetime, MutableMapping[str, float]]]:\n    \"\"\"Return the cross-track series from *csv_path* as timestamped samples.\"\"\"\n\n    samples: list[tuple[datetime, MutableMapping[str, float]]] = []\n    if not csv_path.exists():\n        return samples\n\n    with csv_path.open(\"r\", encoding=\"utf-8\", newline=\"\") as handle:\n        reader = csv.DictReader(handle)\n        for row in reader:\n            timestamp = _parse_time(row.get(\"time_iso\"))\n            if timestamp is None:\n                continue\n            values: MutableMapping[str, float] = {}\n            for key, value in row.items():\n                if key == \"time_iso\":\n                    continue\n                try:\n                    values[key] = float(value)\n                except (TypeError, ValueError):\n                    continue\n            samples.append((timestamp, values))\n\n    return samples\n\n\ndef _normalise_raan_deg(value: float) -> float:\n    \"\"\"Wrap *value* into the [0, 360) interval.\"\"\"\n\n    normalised = math.fmod(value, 360.0)\n    if normalised < 0.0:\n        normalised += 360.0\n    if normalised == 360.0:\n        normalised = 0.0\n    return normalised\n\n\ndef _generate_nodes(scenario: Mapping[str, object]) -> list[Node]:\n    \"\"\"Create mission nodes from access windows or contact plans.\"\"\"\n\n    nodes: list[Node] = []\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    windows = timing.get(\"daily_access_windows\", [])\n\n    if isinstance(windows, Sequence) and windows:\n        for index, window in enumerate(windows, start=1):\n            if not isinstance(window, Mapping):\n                continue\n            start = _parse_time(window.get(\"start_utc\"))\n            end = _parse_time(window.get(\"end_utc\"))\n            duration = _duration_seconds(start, end)\n            attributes = {\n                key: value\n                for key, value in window.items()\n                if key not in {\"label\", \"start_utc\", \"end_utc\"}\n            }\n            nodes.append(\n                Node(\n                    identifier=index,\n                    label=str(window.get(\"label\", f\"window-{index}\")),\n                    kind=\"access_window\",\n                    start=start,\n                    end=end,\n                    duration_s=duration,\n                    attributes=dict(attributes),\n                )\n            )\n\n    if nodes:\n        return nodes\n\n    contact_plan = scenario.get(\"contact_plan\")\n    ground_sites: Iterable[Mapping[str, object]] = []\n    if isinstance(contact_plan, Mapping):\n        sites = contact_plan.get(\"ground_sites\", [])\n        if isinstance(sites, Sequence):\n            ground_sites = [site for site in sites if isinstance(site, Mapping)]\n\n    for index, site in enumerate(ground_sites, start=1):\n        attributes = {key: value for key, value in site.items() if key != \"name\"}\n        nodes.append(\n            Node(\n                identifier=index,\n                label=str(site.get(\"name\", f\"ground-site-{index}\")),\n                kind=\"ground_contact\",\n                start=None,\n                end=None,\n                duration_s=0.0,\n                attributes=dict(attributes),\n            )\n        )\n\n    if not nodes:\n        nodes.append(\n            Node(\n                identifier=1,\n                label=\"mission_start\",\n                kind=\"initial_condition\",\n                start=_parse_time(_extract_planning_horizon(scenario, \"start_utc\")),\n                end=None,\n                duration_s=0.0,\n                attributes={},\n            )\n        )\n\n    return nodes\n\n\ndef _generate_phases(scenario: Mapping[str, object], nodes: Sequence[Node]) -> list[Phase]:\n    \"\"\"Create mission phases aligned with the access nodes.\"\"\"\n\n    phases: list[Phase] = []\n    planning_start = _parse_time(_extract_planning_horizon(scenario, \"start_utc\"))\n    planning_end = _parse_time(_extract_planning_horizon(scenario, \"stop_utc\"))\n\n    if nodes:\n        for index, node in enumerate(nodes, start=1):\n            start = node.start or planning_start\n            end = node.end or planning_end or start\n            duration = _duration_seconds(start, end)\n            objective = (\n                f\"Support {node.label}\" if node.kind == \"ground_contact\" else node.label\n            )\n            phases.append(\n                Phase(\n                    identifier=index,\n                    node_id=node.identifier,\n                    objective=objective,\n                    start=start,\n                    end=end,\n                    duration_s=duration,\n                )\n            )\n\n    if not phases:\n        duration = _estimate_duration(scenario)\n        phases.append(\n            Phase(\n                identifier=1,\n                node_id=None,\n                objective=\"Mission timeline\",\n                start=planning_start,\n                end=planning_end,\n                duration_s=duration,\n            )\n        )\n\n    return phases\n\n\ndef _propagate_two_body(\n    scenario: Mapping[str, object],\n    phases: Sequence[Phase],\n) -> MutableMapping[str, float | Sequence[MutableMapping[str, float]]]:\n    \"\"\"Compute a coarse two-body propagation summary.\"\"\"\n\n    semi_major_axis_km = _infer_semi_major_axis_km(scenario)\n    period = 2.0 * math.pi * math.sqrt((semi_major_axis_km**3) / MU_EARTH_KM3_S2)\n    mean_motion_rev_per_day = 86_400.0 / period if period else 0.0\n\n    phase_revolutions = []\n    for phase in phases:\n        revolutions = phase.duration_s / period if period else 0.0\n        phase_revolutions.append(\n            {\n                \"phase_id\": float(phase.identifier),\n                \"revolutions\": float(revolutions),\n            }\n        )\n\n    return {\n        \"model\": \"two_body\",\n        \"semi_major_axis_km\": float(semi_major_axis_km),\n        \"orbital_period_s\": float(period),\n        \"mean_motion_rev_per_day\": float(mean_motion_rev_per_day),\n        \"phase_revolutions\": phase_revolutions,\n    }\n\n\ndef _apply_j2_drag(\n    two_body: Mapping[str, float | Sequence[Mapping[str, float]]],\n    phases: Sequence[Phase],\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Optional[Path | str] = None,\n    raan_alignment: Optional[Mapping[str, object]] = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation with \\(J_2\\) and drag perturbations.\"\"\"\n\n    del phases  # Phases are retained for compatibility with earlier signatures.\n\n    propagation_summary, artefacts = perturbation_analysis.propagate_constellation(\n        scenario,\n        output_directory=Path(output_directory) if output_directory else None,\n    )\n\n    _update_centroid_alignment_metrics(\n        artefacts,\n        propagation_summary,\n        raan_alignment,\n    )\n\n    orbital_period = float(propagation_summary.get(\"orbital_period_s\", 0.0))\n    nominal_period = float(two_body.get(\"orbital_period_s\", orbital_period))\n    propagation_summary.setdefault(\"two_body_period_s\", nominal_period)\n    propagation_summary.setdefault(\"orbital_period_s\", orbital_period or nominal_period)\n    propagation_summary.setdefault(\n        \"period_delta_s\",\n        float(propagation_summary.get(\"orbital_period_s\", nominal_period)) - nominal_period,\n    )\n\n    return propagation_summary, artefacts\n\n\ndef _update_centroid_alignment_metrics(\n    artefacts: Mapping[str, str],\n    propagation_summary: MutableMapping[str, object],\n    raan_alignment: Optional[Mapping[str, object]],\n) -> None:\n    \"\"\"Inject centroid alignment results into the propagation artefacts.\"\"\"\n\n    if not isinstance(raan_alignment, Mapping):\n        return\n    best = raan_alignment.get(\"best_evaluation\")\n    if not isinstance(best, Mapping):\n        return\n\n    centroid_metrics = {\n        \"min_cross_track_km\": float(best.get(\"centroid_cross_track_km\", math.inf)),\n        \"min_abs_cross_track_km\": float(best.get(\"centroid_abs_cross_track_km\", math.inf)),\n        \"time_of_min_abs_cross_track\": best.get(\"evaluation_time_utc\"),\n        \"vehicle_cross_track_km\": {\n            key: float(value) for key, value in (best.get(\"vehicle_cross_track_km\") or {}).items()\n        },\n        \"vehicle_abs_cross_track_km\": {\n            key: float(value) for key, value in (best.get(\"vehicle_abs_cross_track_km\") or {}).items()\n        },\n        \"worst_vehicle_abs_cross_track_km\": float(\n            best.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n        ),\n        \"primary_limit_km\": float(best.get(\"primary_limit_km\", 0.0)),\n        \"waiver_limit_km\": float(best.get(\"waiver_limit_km\", 0.0)),\n        \"primary_compliant\": bool(best.get(\"primary_compliant\", False)),\n        \"waiver_compliant\": bool(best.get(\"waiver_compliant\", False)),\n        \"primary_pass_fraction\": float(best.get(\"primary_pass_fraction\", 0.0)),\n        \"waiver_pass_fraction\": float(best.get(\"waiver_pass_fraction\", 0.0)),\n    }\n\n    propagation_summary[\"centroid_alignment\"] = centroid_metrics\n\n    summary_path = artefacts.get(\"deterministic_summary_json\")\n    if not summary_path:\n        return\n\n    path = Path(summary_path)\n    if not path.exists():\n        return\n\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n    except json.JSONDecodeError:\n        return\n\n    metrics = data.setdefault(\"metrics\", {})\n    metrics[\"centroid_cross_track\"] = centroid_metrics\n    path.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n\n\ndef _summarise_metrics(\n    nodes: Sequence[Node],\n    phases: Sequence[Phase],\n    two_body: Mapping[str, float | Sequence[Mapping[str, float]]],\n    perturbed: Mapping[str, float | Sequence[Mapping[str, float]]],\n    metrics_specification: Optional[Sequence[str]] = None,\n) -> MutableMapping[str, float]:\n    \"\"\"Return descriptive metrics for quick-look validation.\"\"\"\n\n    node_count = float(len(nodes))\n    phase_count = float(len(phases))\n    total_contact = float(sum(node.duration_s for node in nodes))\n\n    orbital_period_nominal = float(two_body.get(\"orbital_period_s\", 0.0))\n    orbital_period_perturbed = float(perturbed.get(\"orbital_period_s\", 0.0))\n    orbital_delta = orbital_period_perturbed - orbital_period_nominal\n\n    phase_revolutions = two_body.get(\"phase_revolutions\", [])\n    average_revolutions = 0.0\n    if isinstance(phase_revolutions, Sequence) and phase_revolutions:\n        average_revolutions = float(\n            sum(item.get(\"revolutions\", 0.0) for item in phase_revolutions)\n            / len(phase_revolutions)\n        )\n\n    metrics: MutableMapping[str, float] = {\n        \"node_count\": node_count,\n        \"phase_count\": phase_count,\n        \"total_contact_duration_s\": total_contact,\n        \"orbital_period_nominal_s\": orbital_period_nominal,\n        \"orbital_period_perturbed_s\": orbital_period_perturbed,\n        \"orbital_period_delta_s\": float(orbital_delta),\n        \"average_revolutions_per_phase\": average_revolutions,\n    }\n\n    cross_track = perturbed.get(\"cross_track\") if isinstance(perturbed, Mapping) else None\n    primary_compliant = False\n    waiver_compliant = False\n    primary_fraction = None\n    waiver_fraction = None\n    if isinstance(cross_track, Mapping):\n        evaluation = cross_track.get(\"evaluation\") if isinstance(cross_track.get(\"evaluation\"), Mapping) else {}\n        if evaluation:\n            metrics[\"cross_track_evaluation_time_utc\"] = evaluation.get(\"time_utc\")\n            metrics[\"centroid_cross_track_at_evaluation_km\"] = float(\n                evaluation.get(\"centroid_cross_track_km\", 0.0)\n            )\n            metrics[\"centroid_abs_cross_track_at_evaluation_km\"] = float(\n                evaluation.get(\"centroid_abs_cross_track_km\", 0.0)\n            )\n            metrics[\"worst_vehicle_cross_track_at_evaluation_km\"] = float(\n                evaluation.get(\"worst_vehicle_abs_cross_track_km\", 0.0)\n            )\n            metrics[\"deterministic_primary_limit_km\"] = float(\n                evaluation.get(\"primary_limit_km\", 0.0)\n            )\n            metrics[\"deterministic_waiver_limit_km\"] = float(\n                evaluation.get(\"waiver_limit_km\", 0.0)\n            )\n            primary_compliant = bool(evaluation.get(\"primary_compliant\", False))\n            waiver_compliant = bool(evaluation.get(\"waiver_compliant\", False))\n            if \"primary_pass_fraction\" in evaluation:\n                try:\n                    primary_fraction = float(evaluation.get(\"primary_pass_fraction\", 0.0))\n                except (TypeError, ValueError):\n                    primary_fraction = None\n            if \"waiver_pass_fraction\" in evaluation:\n                try:\n                    waiver_fraction = float(evaluation.get(\"waiver_pass_fraction\", 0.0))\n                except (TypeError, ValueError):\n                    waiver_fraction = None\n            vehicle_abs = evaluation.get(\"vehicle_abs\")\n            waiver_limit = evaluation.get(\"waiver_limit_km\")\n            if isinstance(vehicle_abs, Mapping) and vehicle_abs:\n                metrics[\"worst_vehicle_cross_track_km\"] = float(\n                    max(float(value) for value in vehicle_abs.values())\n                )\n                finite_values = [\n                    float(value)\n                    for value in vehicle_abs.values()\n                    if isinstance(value, (int, float)) and math.isfinite(float(value))\n                ]\n                if (\n                    waiver_fraction is None\n                    and finite_values\n                    and isinstance(waiver_limit, (int, float))\n                ):\n                    allowed = sum(\n                        1 for value in finite_values if value <= float(waiver_limit)\n                    )\n                    waiver_fraction = allowed / len(finite_values)\n        overall = float(cross_track.get(\"overall_max_abs_cross_track_km\", 0.0))\n        metrics[\"overall_max_cross_track_km\"] = overall\n        metrics[\"overall_min_cross_track_km\"] = float(\n            cross_track.get(\"overall_min_abs_cross_track_km\", 0.0)\n        )\n    metrics[\"deterministic_primary_compliant\"] = primary_compliant\n    if primary_fraction is None:\n        primary_fraction = 1.0 if primary_compliant else 0.0\n    metrics[\"deterministic_primary_pass_fraction\"] = float(primary_fraction)\n    metrics[\"deterministic_waiver_compliant\"] = waiver_compliant\n    if waiver_fraction is None:\n        waiver_fraction = 1.0 if waiver_compliant else 0.0\n    metrics[\"deterministic_waiver_pass_fraction\"] = float(waiver_fraction)\n\n    monte_carlo = perturbed.get(\"monte_carlo\") if isinstance(perturbed, Mapping) else None\n    if isinstance(monte_carlo, Mapping):\n        compliance = monte_carlo.get(\"compliance\")\n        if isinstance(compliance, Mapping):\n            metrics[\"monte_carlo_primary_compliance_probability\"] = float(\n                compliance.get(\"primary_fraction\", 0.0)\n            )\n            metrics[\"monte_carlo_waiver_compliance_probability\"] = float(\n                compliance.get(\"waiver_fraction\", 0.0)\n            )\n            metrics[\"monte_carlo_relative_compliance_probability\"] = float(\n                compliance.get(\"relative_fraction\", 0.0)\n            )\n            plane_fraction = compliance.get(\"plane_fraction\")\n            if isinstance(plane_fraction, (int, float)) and math.isfinite(\n                float(plane_fraction)\n            ):\n                metrics[\"monte_carlo_plane_compliance_probability\"] = float(\n                    plane_fraction\n                )\n        metrics[\"monte_carlo_fleet_compliance_probability\"] = float(\n            monte_carlo.get(\"fleet_compliance_probability\", 0.0)\n        )\n        metrics[\"monte_carlo_run_count\"] = float(monte_carlo.get(\"runs\", 0))\n\n    plane_intersection = (\n        perturbed.get(\"plane_intersection\") if isinstance(perturbed, Mapping) else None\n    )\n    if isinstance(plane_intersection, Mapping):\n        plane_limit = plane_intersection.get(\"limit_km\")\n        if isinstance(plane_limit, (int, float)) and math.isfinite(float(plane_limit)):\n            distance = float(plane_intersection.get(\"target_distance_km\", 0.0))\n            metrics[\"plane_intersection_distance_km\"] = distance\n            metrics[\"plane_intersection_limit_km\"] = float(plane_limit)\n            compliant_flag = plane_intersection.get(\"compliant\")\n            if compliant_flag is not None:\n                metrics[\"plane_intersection_compliant\"] = 1.0 if compliant_flag else 0.0\n\n    if metrics_specification:\n        # Retain ordering requested by the caller while keeping the full set in the map.\n        prioritised = {metric: metrics.get(metric, 0.0) for metric in metrics_specification}\n        metrics.update({f\"priority::{key}\": value for key, value in prioritised.items()})\n\n    return metrics\n\n\ndef _summarise_configuration(scenario: Mapping[str, object]) -> MutableMapping[str, object]:\n    \"\"\"Condense salient configuration fields for reporting.\"\"\"\n\n    summary: MutableMapping[str, object] = {}\n    metadata = scenario.get(\"metadata\")\n    if isinstance(metadata, Mapping):\n        summary[\"identifier\"] = metadata.get(\"identifier\") or metadata.get(\"scenario_name\")\n        summary[\"name\"] = metadata.get(\"scenario_name\") or metadata.get(\"identifier\")\n        if \"description\" in metadata:\n            summary[\"description\"] = metadata[\"description\"]\n        if \"author\" in metadata:\n            summary[\"author\"] = metadata[\"author\"]\n        if \"validated_against_stk_export\" in metadata:\n            summary[\"validated_against_stk_export\"] = bool(\n                metadata[\"validated_against_stk_export\"]\n            )\n\n    if \"mission_name\" in scenario and \"name\" not in summary:\n        summary[\"name\"] = scenario[\"mission_name\"]\n    if \"mission_name\" in scenario and \"identifier\" not in summary:\n        summary[\"identifier\"] = scenario[\"mission_name\"]\n\n    duration_hours = scenario.get(\"duration_hours\")\n    if isinstance(duration_hours, (int, float)):\n        summary[\"duration_hours\"] = float(duration_hours)\n\n    timing = scenario.get(\"timing\")\n    if isinstance(timing, Mapping):\n        planning_horizon = timing.get(\"planning_horizon\")\n        if isinstance(planning_horizon, Mapping):\n            summary[\"planning_horizon\"] = {\n                key: planning_horizon.get(key)\n                for key in (\"start_utc\", \"stop_utc\")\n                if planning_horizon.get(key) is not None\n            }\n\n    return summary\n\n\ndef _export_stk_products(\n    base_output_dir: Path,\n    scenario: Mapping[str, object],\n    nodes: Sequence[Node],\n    two_body: Mapping[str, float | Sequence[Mapping[str, float]]],\n) -> MutableMapping[str, object]:\n    \"\"\"Generate STK artefacts for the scenario and return metadata about the export.\"\"\"\n\n    export_dir = base_output_dir / \"stk_export\"\n    export_dir.mkdir(parents=True, exist_ok=True)\n\n    metadata = _build_scenario_metadata(scenario, two_body)\n    state_history, ground_track = _generate_state_history(scenario, metadata, two_body)\n    contacts = _collect_ground_contacts(nodes, state_history.satellite_id)\n    facilities = _collect_facilities(scenario, contacts)\n\n    sim_results = SimulationResults(\n        state_histories=[state_history],\n        ground_tracks=[ground_track],\n        ground_contacts=contacts,\n        facilities=facilities,\n    )\n\n    export_simulation_to_stk(sim_results, export_dir, metadata)\n\n    exported_files = sorted(\n        [path.name for path in export_dir.iterdir() if path.is_file()]\n    )\n\n    return {\n        \"stk_export_directory\": str(export_dir),\n        \"stk_export_files\": exported_files,\n        \"stk_scenario_start_epoch\": metadata.start_epoch.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n\n\ndef _build_scenario_metadata(\n    scenario: Mapping[str, object],\n    two_body: Mapping[str, float | Sequence[Mapping[str, float]]],\n) -> ScenarioMetadata:\n    \"\"\"Construct metadata describing the STK scenario context.\"\"\"\n\n    metadata = scenario.get(\"metadata\") if isinstance(scenario.get(\"metadata\"), Mapping) else {}\n    scenario_name = \"Scenario\"\n    if isinstance(metadata, Mapping):\n        scenario_name = str(\n            metadata.get(\"scenario_name\")\n            or metadata.get(\"identifier\")\n            or metadata.get(\"mission_name\")\n            or scenario_name\n        )\n\n    orbital_elements = scenario.get(\"orbital_elements\")\n    epoch = None\n    if isinstance(orbital_elements, Mapping):\n        epoch = _parse_time(orbital_elements.get(\"epoch_utc\"))\n\n    if epoch is None:\n        epoch = _parse_time(_extract_planning_horizon(scenario, \"start_utc\"))\n\n    if epoch is None:\n        epoch = datetime.now(timezone.utc)\n\n    stop = _parse_time(_extract_planning_horizon(scenario, \"stop_utc\"))\n    if stop is None:\n        duration = _estimate_duration(scenario)\n        if not duration:\n            duration = float(two_body.get(\"orbital_period_s\", 0.0))\n        stop = epoch + timedelta(seconds=duration or 5_400.0)\n\n    period = float(two_body.get(\"orbital_period_s\", 0.0))\n    if period <= 0.0:\n        period = 5_400.0\n    step = max(min(period / 60.0, 900.0), 60.0)\n\n    return ScenarioMetadata(\n        scenario_name=scenario_name,\n        start_epoch=epoch,\n        stop_epoch=stop,\n        central_body=\"Earth\",\n        coordinate_frame=\"TEME\",\n        ephemeris_step_seconds=step,\n        animation_step_seconds=step,\n    )\n\n\ndef _generate_state_history(\n    scenario: Mapping[str, object],\n    metadata: ScenarioMetadata,\n    two_body: Mapping[str, float | Sequence[Mapping[str, float]]],\n) -> tuple[PropagatedStateHistory, GroundTrack]:\n    \"\"\"Synthesise a representative propagation history for STK export.\"\"\"\n\n    classical = {}\n    orbital_elements = scenario.get(\"orbital_elements\")\n    if isinstance(orbital_elements, Mapping):\n        classical = orbital_elements.get(\"classical\") or {}\n\n    semi_major_axis = _coerce_float(classical, \"semi_major_axis_km\", 7_000.0)\n    eccentricity = _coerce_float(classical, \"eccentricity\", 0.0)\n    inclination = math.radians(_coerce_float(classical, \"inclination_deg\", 0.0))\n    raan = math.radians(_coerce_float(classical, \"raan_deg\", 0.0))\n    argument_of_perigee = math.radians(\n        _coerce_float(classical, \"argument_of_perigee_deg\", 0.0)\n    )\n    mean_anomaly = math.radians(_coerce_float(classical, \"mean_anomaly_deg\", 0.0))\n\n    satellite_id = _select_satellite_identifier(scenario)\n\n    if metadata.stop_epoch is None:\n        stop_epoch = metadata.start_epoch + timedelta(\n            seconds=float(two_body.get(\"orbital_period_s\", 5_400.0))\n        )\n    else:\n        stop_epoch = metadata.stop_epoch\n\n    step = metadata.ephemeris_step_seconds or 600.0\n    duration_seconds = max((stop_epoch - metadata.start_epoch).total_seconds(), step)\n    sample_count = int(math.ceil(duration_seconds / step)) + 1\n\n    state_samples: list[StateSample] = []\n    ground_track_points: list[GroundTrackPoint] = []\n\n    mean_motion = math.sqrt(MU_EARTH_KM3_S2 / (semi_major_axis**3))\n    if not math.isfinite(mean_motion) or mean_motion <= 0.0:\n        mean_motion = 0.0011635528346628863  # ~95-minute orbit fallback\n\n    for index in range(sample_count):\n        epoch = metadata.start_epoch + timedelta(seconds=index * step)\n        delta_t = (epoch - metadata.start_epoch).total_seconds()\n        mean_anomaly_current = mean_anomaly + mean_motion * delta_t\n        mean_anomaly_current = math.fmod(mean_anomaly_current, 2.0 * math.pi)\n        if mean_anomaly_current < 0.0:\n            mean_anomaly_current += 2.0 * math.pi\n\n        eccentric_anomaly = _solve_kepler(mean_anomaly_current, eccentricity)\n        true_anomaly = _eccentric_to_true_anomaly(eccentric_anomaly, eccentricity)\n\n        radius = semi_major_axis * (1.0 - eccentricity * math.cos(eccentric_anomaly))\n        semi_latus_rectum = semi_major_axis * (1.0 - eccentricity**2)\n        if semi_latus_rectum <= 0.0:\n            semi_latus_rectum = semi_major_axis\n        sqrt_mu_over_p = math.sqrt(MU_EARTH_KM3_S2 / semi_latus_rectum)\n\n        position_pqw = (\n            radius * math.cos(true_anomaly),\n            radius * math.sin(true_anomaly),\n            0.0,\n        )\n        velocity_pqw = (\n            -sqrt_mu_over_p * math.sin(true_anomaly),\n            sqrt_mu_over_p * (eccentricity + math.cos(true_anomaly)),\n            0.0,\n        )\n\n        position_eci = _pqw_to_eci(position_pqw, raan, argument_of_perigee, inclination)\n        velocity_eci = _pqw_to_eci(velocity_pqw, raan, argument_of_perigee, inclination)\n\n        state_samples.append(\n            StateSample(\n                epoch=epoch,\n                position_eci_km=position_eci,\n                velocity_eci_kms=velocity_eci,\n            )\n        )\n\n        latitude, longitude, altitude = _eci_to_geodetic(position_eci, metadata.start_epoch, epoch)\n        ground_track_points.append(\n            GroundTrackPoint(\n                epoch=epoch,\n                latitude_deg=latitude,\n                longitude_deg=longitude,\n                altitude_km=altitude,\n            )\n        )\n\n    return (\n        PropagatedStateHistory(satellite_id=satellite_id, samples=state_samples),\n        GroundTrack(satellite_id=satellite_id, points=ground_track_points),\n    )\n\n\ndef _coerce_float(data: Mapping[str, object], key: str, default: float) -> float:\n    value = data.get(key) if isinstance(data, Mapping) else None\n    try:\n        return float(value)\n    except (TypeError, ValueError):\n        return float(default)\n\n\ndef _solve_kepler(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Solve Kepler's equation for the eccentric anomaly.\"\"\"\n\n    if abs(eccentricity) < 1e-8:\n        return mean_anomaly\n\n    eccentric_anomaly = mean_anomaly\n    for _ in range(12):\n        delta = eccentric_anomaly - eccentricity * math.sin(eccentric_anomaly) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric_anomaly)\n        if derivative == 0.0:\n            break\n        correction = delta / derivative\n        eccentric_anomaly -= correction\n        if abs(correction) < 1e-12:\n            break\n    return eccentric_anomaly\n\n\ndef _eccentric_to_true_anomaly(eccentric_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert eccentric anomaly to true anomaly.\"\"\"\n\n    if abs(eccentricity - 1.0) < 1e-8:\n        return eccentric_anomaly\n\n    numerator = math.sqrt(1.0 + eccentricity) * math.sin(eccentric_anomaly / 2.0)\n    denominator = math.sqrt(1.0 - eccentricity) * math.cos(eccentric_anomaly / 2.0)\n    true_anomaly = 2.0 * math.atan2(numerator, denominator)\n    return true_anomaly\n\n\ndef _pqw_to_eci(\n    vector: Sequence[float],\n    raan: float,\n    argument_of_perigee: float,\n    inclination: float,\n) -> tuple[float, float, float]:\n    \"\"\"Rotate a vector from the orbital plane into the inertial frame.\"\"\"\n\n    cos_omega = math.cos(raan)\n    sin_omega = math.sin(raan)\n    cos_arg = math.cos(argument_of_perigee)\n    sin_arg = math.sin(argument_of_perigee)\n    cos_inc = math.cos(inclination)\n    sin_inc = math.sin(inclination)\n\n    x_p, y_p, z_p = vector\n\n    rotation_11 = cos_omega * cos_arg - sin_omega * sin_arg * cos_inc\n    rotation_12 = -cos_omega * sin_arg - sin_omega * cos_arg * cos_inc\n    rotation_21 = sin_omega * cos_arg + cos_omega * sin_arg * cos_inc\n    rotation_22 = -sin_omega * sin_arg + cos_omega * cos_arg * cos_inc\n    rotation_31 = sin_arg * sin_inc\n    rotation_32 = cos_arg * sin_inc\n\n    x = rotation_11 * x_p + rotation_12 * y_p\n    y = rotation_21 * x_p + rotation_22 * y_p\n    z = rotation_31 * x_p + rotation_32 * y_p\n\n    return (x, y, z)\n\n\ndef _julian_date(epoch: datetime) -> float:\n    \"\"\"Return the Julian date corresponding to *epoch* (UTC).\"\"\"\n\n    moment = epoch.astimezone(timezone.utc)\n    year = moment.year\n    month = moment.month\n    day = moment.day + (\n        moment.hour\n        + (moment.minute + (moment.second + moment.microsecond / 1_000_000.0) / 60.0) / 60.0\n    ) / 24.0\n\n    if month <= 2:\n        year -= 1\n        month += 12\n\n    a = math.floor(year / 100)\n    b = 2 - a + math.floor(a / 4)\n\n    jd = (\n        math.floor(365.25 * (year + 4716))\n        + math.floor(30.6001 * (month + 1))\n        + day\n        + b\n        - 1524.5\n    )\n    return jd\n\n\ndef _greenwich_sidereal_angle(epoch: datetime) -> float:\n    \"\"\"Compute the Greenwich mean sidereal angle for *epoch* in radians.\"\"\"\n\n    jd = _julian_date(epoch)\n    t = (jd - 2_451_545.0) / 36_525.0\n    gmst_deg = (\n        280.46061837\n        + 360.98564736629 * (jd - 2_451_545.0)\n        + 0.000387933 * t**2\n        - (t**3) / 38_710_000.0\n    )\n    return math.radians(gmst_deg % 360.0)\n\n\ndef _eci_to_geodetic(\n    position_eci: Sequence[float],\n    _reference_epoch: datetime,\n    epoch: datetime,\n) -> tuple[float, float, float]:\n    \"\"\"Convert ECI coordinates to geodetic latitude, longitude, and altitude.\"\"\"\n\n    angle = _greenwich_sidereal_angle(epoch)\n    cos_angle = math.cos(angle)\n    sin_angle = math.sin(angle)\n\n    x_eci, y_eci, z_eci = position_eci\n    x_ecef = cos_angle * x_eci + sin_angle * y_eci\n    y_ecef = -sin_angle * x_eci + cos_angle * y_eci\n    z_ecef = z_eci\n\n    longitude = math.degrees(math.atan2(y_ecef, x_ecef))\n    longitude = ((longitude + 180.0) % 360.0) - 180.0\n\n    p = math.hypot(x_ecef, y_ecef)\n    if p < 1.0e-9:\n        latitude = 90.0 if z_ecef >= 0.0 else -90.0\n        altitude = abs(z_ecef) - EARTH_RADIUS_KM * (1.0 - EARTH_FLATTENING)\n        return latitude, longitude, altitude\n\n    a = EARTH_RADIUS_KM\n    b = a * (1.0 - EARTH_FLATTENING)\n    theta = math.atan2(z_ecef * a, p * b)\n    sin_theta = math.sin(theta)\n    cos_theta = math.cos(theta)\n\n    latitude_rad = math.atan2(\n        z_ecef + EARTH_SECOND_ECCENTRICITY_SQUARED * b * sin_theta**3,\n        p - EARTH_ECCENTRICITY_SQUARED * a * cos_theta**3,\n    )\n    sin_lat = math.sin(latitude_rad)\n    cos_lat = math.cos(latitude_rad)\n    n = a / math.sqrt(1.0 - EARTH_ECCENTRICITY_SQUARED * sin_lat**2)\n\n    if abs(cos_lat) > 1.0e-12:\n        altitude = p / cos_lat - n\n    else:\n        altitude = z_ecef / sin_lat - n * (1.0 - EARTH_ECCENTRICITY_SQUARED)\n\n    latitude = math.degrees(latitude_rad)\n    return latitude, longitude, altitude\n\n\ndef _collect_ground_contacts(\n    nodes: Sequence[Node],\n    satellite_id: str,\n) -> list[GroundContactInterval]:\n    \"\"\"Create contact intervals based on mission nodes.\"\"\"\n\n    contacts: list[GroundContactInterval] = []\n    for node in nodes:\n        if node.start and node.end:\n            facility_name = _contact_facility_name(node)\n            contacts.append(\n                GroundContactInterval(\n                    satellite_id=satellite_id,\n                    facility_name=facility_name,\n                    start=node.start,\n                    end=node.end,\n                )\n            )\n    return contacts\n\n\ndef _contact_facility_name(node: Node) -> str:\n    attributes = node.attributes\n    if isinstance(attributes, Mapping):\n        if \"ground_station\" in attributes:\n            return str(attributes[\"ground_station\"])\n        if \"target\" in attributes:\n            return str(attributes[\"target\"])\n    return node.label\n\n\ndef _collect_facilities(\n    scenario: Mapping[str, object],\n    contacts: Sequence[GroundContactInterval],\n) -> list[FacilityDefinition]:\n    \"\"\"Build facility definitions referenced by the contact catalogue.\"\"\"\n\n    facilities: dict[str, FacilityDefinition] = {}\n    metadata = scenario.get(\"metadata\") if isinstance(scenario.get(\"metadata\"), Mapping) else {}\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    region_lat = None\n    region_lon = None\n    if isinstance(region, Mapping):\n        region_lat = _coerce_float(region, \"latitude_deg\", 0.0)\n        region_lon = _coerce_float(region, \"longitude_deg\", 0.0)\n\n    for interval in contacts:\n        name = interval.facility_name\n        if name in facilities:\n            continue\n        coords = _lookup_facility_coordinates(name, region_lat, region_lon)\n        facilities[name] = FacilityDefinition(\n            name=name,\n            latitude_deg=coords[0],\n            longitude_deg=coords[1],\n            altitude_km=coords[2],\n        )\n\n    if isinstance(region, Mapping):\n        descriptive_name = f\"{region.get('city', 'Region')} {region.get('country', '').strip()}\".strip()\n        coords = (\n            _coerce_float(region, \"latitude_deg\", 0.0),\n            _coerce_float(region, \"longitude_deg\", 0.0),\n            _coerce_float(region, \"altitude_km\", 1.5),\n        )\n        facilities.setdefault(\n            descriptive_name or \"AreaOfInterest\",\n            FacilityDefinition(\n                name=descriptive_name or \"AreaOfInterest\",\n                latitude_deg=coords[0],\n                longitude_deg=coords[1],\n                altitude_km=coords[2],\n            ),\n        )\n\n    return list(facilities.values())\n\n\ndef _lookup_facility_coordinates(\n    name: str,\n    region_lat: Optional[float],\n    region_lon: Optional[float],\n) -> tuple[float, float, float]:\n    \"\"\"Return coordinates for known facilities, defaulting to the scenario region.\"\"\"\n\n    catalogue: dict[str, tuple[float, float, float]] = {\n        \"svalbard\": (78.223, 15.646, 0.4),\n        \"inuvik\": (68.355, -133.721, 0.1),\n        \"tehran urban core\": (35.6892, 51.3890, 1.2),\n        \"tehran\": (35.6892, 51.3890, 1.2),\n    }\n\n    key = name.lower()\n    if key in catalogue:\n        return catalogue[key]\n\n    if region_lat is not None and region_lon is not None:\n        return (region_lat, region_lon, 0.0)\n\n    return (0.0, 0.0, 0.0)\n\n\ndef _select_satellite_identifier(scenario: Mapping[str, object]) -> str:\n    \"\"\"Derive a stable satellite identifier from the scenario metadata.\"\"\"\n\n    metadata = scenario.get(\"metadata\") if isinstance(scenario.get(\"metadata\"), Mapping) else {}\n    identifier = None\n    if isinstance(metadata, Mapping):\n        identifier = metadata.get(\"identifier\") or metadata.get(\"scenario_name\")\n\n    if identifier is None and \"mission_name\" in scenario:\n        identifier = scenario[\"mission_name\"]\n\n    if not identifier:\n        identifier = \"scenario_asset\"\n\n    safe_identifier = str(identifier).strip().replace(\" \", \"_\")\n    return safe_identifier\n\n\ndef _write_summary(output_dir: Path, summary: Mapping[str, object]) -> Path:\n    \"\"\"Persist the pipeline summary to *output_dir*.\"\"\"\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n    path = output_dir / \"scenario_summary.json\"\n    path.write_text(json.dumps(summary, indent=2, sort_keys=True), encoding=\"utf-8\")\n    return path\n\n\ndef _parse_time(value: object) -> Optional[datetime]:\n    \"\"\"Parse ISO 8601 timestamps into timezone-aware datetimes.\"\"\"\n\n    if value is None:\n        return None\n    if isinstance(value, datetime):\n        if value.tzinfo is None:\n            return value.replace(tzinfo=timezone.utc)\n        return value.astimezone(timezone.utc)\n    if isinstance(value, str):\n        text = value.replace(\"Z\", \"+00:00\") if value.endswith(\"Z\") else value\n        try:\n            parsed = datetime.fromisoformat(text)\n        except ValueError:\n            return None\n        if parsed.tzinfo is None:\n            parsed = parsed.replace(tzinfo=timezone.utc)\n        return parsed.astimezone(timezone.utc)\n    return None\n\n\ndef _format_time(value: Optional[datetime]) -> Optional[str]:\n    \"\"\"Convert datetimes back to ISO 8601 strings with ``Z`` designators.\"\"\"\n\n    if value is None:\n        return None\n    return value.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n\n\ndef _duration_seconds(start: Optional[datetime], end: Optional[datetime]) -> float:\n    \"\"\"Return the non-negative duration between *start* and *end*.\"\"\"\n\n    if start is None or end is None:\n        return 0.0\n    return max((end - start).total_seconds(), 0.0)\n\n\ndef _extract_planning_horizon(scenario: Mapping[str, object], key: str) -> Optional[str]:\n    \"\"\"Fetch a planning horizon field from the configuration if present.\"\"\"\n\n    timing = scenario.get(\"timing\")\n    if not isinstance(timing, Mapping):\n        return None\n    horizon = timing.get(\"planning_horizon\")\n    if not isinstance(horizon, Mapping):\n        return None\n    value = horizon.get(key)\n    return str(value) if value is not None else None\n\n\ndef _estimate_duration(scenario: Mapping[str, object]) -> float:\n    \"\"\"Derive a representative mission duration in seconds.\"\"\"\n\n    duration_hours = scenario.get(\"duration_hours\")\n    if isinstance(duration_hours, (int, float)):\n        return float(duration_hours) * 3_600.0\n\n    start = _parse_time(_extract_planning_horizon(scenario, \"start_utc\"))\n    end = _parse_time(_extract_planning_horizon(scenario, \"stop_utc\"))\n    return _duration_seconds(start, end)\n\n\ndef _infer_semi_major_axis_km(scenario: Mapping[str, object]) -> float:\n    \"\"\"Infer the semi-major axis from the configuration with sensible defaults.\"\"\"\n\n    orbital_elements = scenario.get(\"orbital_elements\")\n    if isinstance(orbital_elements, Mapping):\n        classical = orbital_elements.get(\"classical\")\n        if isinstance(classical, Mapping) and \"semi_major_axis_km\" in classical:\n            try:\n                return float(classical[\"semi_major_axis_km\"])\n            except (TypeError, ValueError):\n                pass\n\n    spacecraft = scenario.get(\"spacecraft\")\n    if isinstance(spacecraft, Sequence) and spacecraft:\n        axes: list[float] = []\n        for craft in spacecraft:\n            if not isinstance(craft, Mapping):\n                continue\n            state = craft.get(\"initial_state\")\n            if not isinstance(state, Mapping):\n                continue\n            semi_major_axis_m = state.get(\"semi_major_axis_m\")\n            if isinstance(semi_major_axis_m, (int, float)):\n                axes.append(float(semi_major_axis_m) / 1_000.0)\n        if axes:\n            return sum(axes) / len(axes)\n\n    return 7_000.0\n\n\nif __name__ == \"__main__\":  # pragma: no cover - exercised via CLI tests.\n    raise SystemExit(main())\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CicB4/H/Xrrbs06i6y502lDuZjOlFtI3DTZVSSerIxdugEHmxl1cdwIKXAHj8f9eRz0TX16a3eq0/NJGPKXg5YkEpHp4vhf1z+At5eNgAo+LFpJiq0SRi6JLYbCwN+msuNKbrnPn5Qf9zyrJHF7Fy/4azj27HqpBheOSSgzMMHsvHRZLilFJCn8B4/H/Xle+AS7XEKXN8bJNFB/BNl+d5pgubJ1OwD8M7HCwIDi9JKba/ueoYxo19GFelD2BaHNt9VGlEvUyf9dgll+hjtELxdI1AI8DTPAdUS3nLI8z/V5+9IHup3cAj9aQ3HES+8bBkAzWQyK7fIr+Lww5MPf86B2s1lm/UE/0CrABAePx/15vHPE7qDvRWlZKCoSyfc1mirpjN+BoL5VpGvYbgMrf70QyOUtZwdGuZiHNkW7wLKyggdeviJT/iQFmtiakEP7A93/LpPkpGRM/G70Q4T+UV48MGjZ4rlbrJY8H80Jm8bzGp/jZqkZH/bYFY6jzGeE251kx13BcRQxx+fCJ+w2zKlDKeuZdE7XADFWBahdibk+5yKXU8tAikyssLZWDpvyFrDYOIlSqmi8g4w0KSQHj8f9emwmLBbN3L6drDKBql3iEx3CIKDSt9hSfUAdpZmqNXroza0aARPuNH2f9QSbIhsyQmnLHPuzin6fYElkxSUldFaseM3M=",
        "text": "Okay, `run_scenario.py` uses `perturbation_analysis.propagate_constellation` for propagation. I'll find the main loop in `perturbation_analysis.py` and add a progress bar. Time to read the code."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762792361080-9444ffb0e2eb7",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"High-fidelity constellation propagation with \\(J_2\\) and drag perturbations.\n\nThe helper functions defined here extend the lightweight scenario pipeline by\nintroducing a deterministic force model and a Monte Carlo dispersion analysis\ntailored to the Tehran daily pass configuration.  The propagated states capture\ncross-track offsets for every spacecraft in the formation, enabling direct\nverification of the ±30 km primary alignment tolerance (and the ±70 km waiver\nceiling) that governs MR-2 and SRD-P-001.  Outputs are serialisable to JSON and\ncomma-separated value (CSV) formats so they remain compatible with Systems Tool\nKit (STK 11.2) validation workflows as required by the repository guidelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Mapping, MutableMapping, Sequence\n\nimport os\n\nimport numpy as np\nfrom statistics import fmean\n\nfrom src.constellation.frames import rotation_matrix_rtn_to_eci\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    EARTH_ROTATION_RATE,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\n\n\nJ2_COEFFICIENT = 1.08262668e-3\nSOLAR_FLUX_BASE = 150.0\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\n\nDEFAULT_FORMATION = (\n    {\n        \"identifier\": \"FSAT-LDR\",\n        \"role\": \"leader\",\n        \"radial_offset_km\": 0.0,\n        \"along_track_offset_km\": 0.0,\n        \"cross_track_offset_km\": 0.0,\n    },\n    {\n        \"identifier\": \"FSAT-DP1\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": -0.05,\n        \"along_track_offset_km\": 20.0,\n        \"cross_track_offset_km\": 0.2,\n    },\n    {\n        \"identifier\": \"FSAT-DP2\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": 0.05,\n        \"along_track_offset_km\": -20.0,\n        \"cross_track_offset_km\": -0.2,\n    },\n)\n\nPLANE_ASSIGNMENTS = {\n    \"FSAT-LDR\": \"Plane A\",\n    \"FSAT-DP1\": \"Plane A\",\n    \"FSAT-DP2\": \"Plane B\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nCLASSICAL_ELEMENT_UNITS = {\n    \"semi_major_axis_km\": \"km\",\n    \"eccentricity\": \"\",\n    \"inclination_deg\": \"deg\",\n    \"raan_deg\": \"deg\",\n    \"argument_of_perigee_deg\": \"deg\",\n    \"mean_anomaly_deg\": \"deg\",\n}\n\n\nDEFAULT_MONTE_CARLO = {\n    \"enabled\": True,\n    \"runs\": 1000,\n    \"seed\": 4242,\n    \"dispersions\": {\n        \"semi_major_axis_sigma_m\": 5.0,\n        \"inclination_sigma_deg\": 0.01,\n        \"drag_coefficient_sigma\": 0.05,\n    },\n}\n\n\n@dataclass\nclass PropagatorSettings:\n    \"\"\"Container describing the deterministic propagation configuration.\"\"\"\n\n    start_time: datetime\n    epoch_time: datetime\n    stop_time: datetime\n    time_step_s: float\n    drag_coefficient: float\n    ballistic_coefficient_m2_per_kg: float\n    solar_flux_index: float\n    evaluation_time: datetime | None = None\n    primary_cross_track_limit_km: float = 30.0\n    waiver_cross_track_limit_km: float = 70.0\n    plane_intersection_limit_km: float | None = None\n\n\n@dataclass\nclass SpacecraftState:\n    \"\"\"Cartesian state and drag properties for a spacecraft.\"\"\"\n\n    identifier: str\n    position_m: np.ndarray\n    velocity_mps: np.ndarray\n    drag_coefficient: float\n    area_m2: float\n    mass_kg: float\n\n\ndef propagate_constellation(\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Path | None = None,\n    formation: Sequence[Mapping[str, float | str]] = DEFAULT_FORMATION,\n    settings: PropagatorSettings | None = None,\n    monte_carlo: Mapping[str, object] | None = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation including \\(J_2\\) and atmospheric drag.\"\"\"\n\n    resolved_settings = settings or _default_settings(scenario)\n    monte_carlo_config = _normalise_monte_carlo(monte_carlo)\n\n    target_lat, target_lon = _target_coordinates(scenario)\n    leader_elements = _scenario_orbital_elements(scenario)\n\n    deterministic_series, deterministic_metrics = _propagate_deterministic(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n    )\n\n    monte_carlo_metrics = _run_monte_carlo(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n        monte_carlo_config,\n    )\n\n    artefact_paths: MutableMapping[str, str] = {}\n    if output_directory:\n        artefact_paths = _write_outputs(\n            Path(output_directory),\n            deterministic_series,\n            deterministic_metrics,\n            monte_carlo_metrics,\n            resolved_settings,\n            monte_carlo_config,\n        )\n\n    summary = _build_summary(\n        deterministic_metrics,\n        monte_carlo_metrics,\n        resolved_settings,\n        monte_carlo_config,\n    )\n\n    return summary, artefact_paths\n\n\ndef scenario_cross_track_limits(\n    scenario: Mapping[str, object]\n) -> tuple[float, float, float | None]:\n    \"\"\"Return the primary, waiver, and optional plane-intersection limits.\"\"\"\n\n    primary_limit = float(os.getenv(\"FSAT_PRIMARY_CROSS_TRACK_LIMIT_KM\", \"30.0\"))\n    waiver_limit = float(os.getenv(\"FSAT_WAIVER_CROSS_TRACK_LIMIT_KM\", \"70.0\"))\n    plane_limit_env = os.getenv(\"FSAT_PLANE_INTERSECTION_LIMIT_KM\")\n    plane_limit: float | None = (\n        float(plane_limit_env) if plane_limit_env is not None else None\n    )\n\n    limits = scenario.get(\"cross_track_limits\")\n    if isinstance(limits, Mapping):\n        if \"primary_km\" in limits:\n            try:\n                primary_limit = float(limits[\"primary_km\"])\n            except (TypeError, ValueError):\n                pass\n        if \"waiver_km\" in limits:\n            try:\n                waiver_limit = float(limits[\"waiver_km\"])\n            except (TypeError, ValueError):\n                pass\n        plane_candidate = limits.get(\"plane_intersection_limit_km\")\n        if plane_candidate is not None:\n            try:\n                plane_limit = float(plane_candidate)\n            except (TypeError, ValueError):\n                pass\n    else:\n        requirements = scenario.get(\"alignment_requirements\")\n        if isinstance(requirements, Mapping):\n            primary_limit = float(\n                requirements.get(\"primary_cross_track_limit_km\", primary_limit)\n            )\n            waiver_limit = float(\n                requirements.get(\"waiver_cross_track_limit_km\", waiver_limit)\n            )\n            plane_candidate = requirements.get(\"plane_intersection_limit_km\", plane_limit)\n            if plane_candidate is not None:\n                plane_limit = float(plane_candidate)\n\n    return primary_limit, waiver_limit, plane_limit\n\n\ndef scenario_access_window(\n    scenario: Mapping[str, object]\n) -> tuple[datetime | None, datetime | None, datetime | None]:\n    \"\"\"Return start, end, and midpoint timestamps for the primary access window.\"\"\"\n\n    window = scenario.get(\"access_window\")\n    start = _parse_time(window.get(\"start_utc\")) if isinstance(window, Mapping) else None\n    end = _parse_time(window.get(\"end_utc\")) if isinstance(window, Mapping) else None\n    midpoint = (\n        _parse_time(window.get(\"midpoint_utc\")) if isinstance(window, Mapping) else None\n    )\n\n    timing = (\n        scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    )\n    windows = timing.get(\"daily_access_windows\")\n    if start is None or end is None:\n        if isinstance(windows, Sequence) and windows:\n            primary = windows[0] if isinstance(windows[0], Mapping) else {}\n            if start is None:\n                start = _parse_time(primary.get(\"start_utc\"))\n            if end is None:\n                end = _parse_time(primary.get(\"end_utc\"))\n            if midpoint is None:\n                midpoint = _parse_time(primary.get(\"midpoint_utc\"))\n\n    if midpoint is None and start and end:\n        midpoint = start + (end - start) / 2\n    elif midpoint is None and start:\n        midpoint = start + timedelta(seconds=45.0)\n\n    return start, end, midpoint\n\n\ndef _build_summary(\n    deterministic_metrics: MutableMapping[str, object],\n    monte_carlo_metrics: MutableMapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Combine deterministic and Monte Carlo outputs into a summary map.\"\"\"\n\n    orbital_period = deterministic_metrics.get(\"orbital_period_s\", 0.0)\n    two_body_period = deterministic_metrics.get(\"two_body_period_s\", orbital_period)\n    period_delta = float(orbital_period) - float(two_body_period)\n\n    return {\n        \"model\": \"high_fidelity_j2_drag\",\n        \"orbital_period_s\": float(orbital_period),\n        \"period_delta_s\": float(period_delta),\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": float(settings.time_step_s),\n            \"drag_coefficient\": float(settings.drag_coefficient),\n            \"ballistic_coefficient_m2_per_kg\": float(settings.ballistic_coefficient_m2_per_kg),\n            \"solar_flux_index\": float(settings.solar_flux_index),\n            \"monte_carlo_runs\": int(monte_carlo_config.get(\"runs\", 0)),\n        },\n        \"cross_track\": deterministic_metrics.get(\"cross_track\", {}),\n        \"altitude_range_m\": deterministic_metrics.get(\"altitude_range_m\", {}),\n        \"plane_intersection\": deterministic_metrics.get(\"plane_intersection\", {}),\n        \"monte_carlo\": monte_carlo_metrics,\n    }\n\n\ndef _default_settings(scenario: Mapping[str, object]) -> PropagatorSettings:\n    \"\"\"Return baseline propagation settings derived from the scenario.\"\"\"\n\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    window_start, window_end, window_midpoint = scenario_access_window(scenario)\n    orbital_elements = scenario.get(\"orbital_elements\", {})\n    reference_start = window_start or window_midpoint\n    if reference_start is None:\n        reference_start = _parse_time(\n            orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n        )\n    if reference_start is None:\n        reference_start = datetime(2026, 3, 21, 0, 0, 0, tzinfo=timezone.utc)\n\n    epoch_time = _parse_time(\n        orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n    ) or reference_start\n\n    planning = timing.get(\"planning_horizon\", {}) if isinstance(timing.get(\"planning_horizon\"), Mapping) else {}\n    planning_stop = _parse_time(planning.get(\"stop_utc\"))\n\n    propagation = timing.get(\"propagation\") if isinstance(timing.get(\"propagation\"), Mapping) else {}\n    margin_s = 300.0\n    try:\n        margin_candidate = float(propagation.get(\"margin_s\", margin_s))\n        margin_s = max(margin_candidate, 120.0)\n    except (TypeError, ValueError):\n        margin_s = max(margin_s, 120.0)\n    margin = timedelta(seconds=margin_s)\n\n    start_override = _parse_time(propagation.get(\"start_utc\")) if propagation else None\n    end_override = _parse_time(propagation.get(\"end_utc\")) if propagation else None\n    time_step = 10.0\n    try:\n        time_step = float(propagation.get(\"time_step_s\", time_step))\n    except (TypeError, ValueError):\n        time_step = 10.0\n\n    analysis_start = start_override or (reference_start - margin)\n    if window_start and analysis_start > window_start - margin:\n        analysis_start = window_start - margin\n\n    candidate_stop = end_override\n    if candidate_stop is None:\n        if window_end:\n            candidate_stop = window_end + margin\n        elif window_midpoint:\n            candidate_stop = window_midpoint + margin\n        else:\n            candidate_stop = analysis_start + timedelta(minutes=10)\n\n    if planning_stop:\n        candidate_stop = min(candidate_stop, planning_stop)\n\n    if candidate_stop <= analysis_start:\n        candidate_stop = analysis_start + timedelta(minutes=10)\n\n    evaluation_time = window_midpoint or reference_start\n\n    primary_limit, waiver_limit, plane_limit = scenario_cross_track_limits(scenario)\n\n    return PropagatorSettings(\n        start_time=analysis_start,\n        epoch_time=epoch_time,\n        stop_time=candidate_stop,\n        time_step_s=time_step,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit,\n        waiver_cross_track_limit_km=waiver_limit,\n        plane_intersection_limit_km=plane_limit,\n    )\n\n\ndef _normalise_monte_carlo(monte_carlo: Mapping[str, object] | None) -> MutableMapping[str, object]:\n    \"\"\"Return a mutable Monte Carlo configuration with defaults applied.\"\"\"\n\n    if monte_carlo is None:\n        config = dict(DEFAULT_MONTE_CARLO)\n    else:\n        config = dict(DEFAULT_MONTE_CARLO)\n        config.update({key: monte_carlo.get(key, value) for key, value in DEFAULT_MONTE_CARLO.items()})\n        for key, value in monte_carlo.items():\n            if key == \"dispersions\" and isinstance(value, Mapping):\n                default_disp = dict(DEFAULT_MONTE_CARLO.get(\"dispersions\", {}))\n                default_disp.update({k: float(v) for k, v in value.items()})\n                config[\"dispersions\"] = default_disp\n            else:\n                config[key] = value\n\n    dispersions = config.get(\"dispersions\", {})\n    if not isinstance(dispersions, Mapping):\n        dispersions = DEFAULT_MONTE_CARLO[\"dispersions\"]\n    config[\"dispersions\"] = dict(dispersions)\n\n    try:\n        runs = int(config.get(\"runs\", DEFAULT_MONTE_CARLO[\"runs\"]))\n    except (TypeError, ValueError):\n        runs = DEFAULT_MONTE_CARLO[\"runs\"]\n    config[\"runs\"] = max(runs, 500)\n\n    return config\n\n\ndef _resolve_evaluation_time(settings: PropagatorSettings) -> datetime:\n    \"\"\"Return a clamped evaluation time within the propagation span.\"\"\"\n\n    evaluation = settings.evaluation_time or settings.start_time\n    if evaluation < settings.start_time:\n        evaluation = settings.start_time\n    if evaluation > settings.stop_time:\n        evaluation = settings.stop_time\n    return evaluation\n\n\ndef _scenario_orbital_elements(scenario: Mapping[str, object]) -> OrbitalElements:\n    \"\"\"Extract classical elements from the scenario mapping.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, Mapping):\n        fallback = _fallback_orbital_elements(scenario)\n        if fallback is None:\n            raise ValueError(\"Scenario is missing the 'orbital_elements' section.\")\n        return fallback\n\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, Mapping):\n        raise ValueError(\"Scenario orbital elements must include the 'classical' block.\")\n\n    semi_major_axis_m = float(classical.get(\"semi_major_axis_km\", 0.0)) * 1_000.0\n    eccentricity = float(classical.get(\"eccentricity\", 0.0))\n    inclination = math.radians(float(classical.get(\"inclination_deg\", 0.0)))\n    raan = math.radians(float(classical.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(classical.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(classical.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis_m,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _fallback_orbital_elements(\n    scenario: Mapping[str, object]\n) -> OrbitalElements | None:\n    spacecraft = scenario.get(\"spacecraft\")\n    if not isinstance(spacecraft, Sequence) or not spacecraft:\n        return None\n    leader = spacecraft[0]\n    if not isinstance(leader, Mapping):\n        return None\n    initial_state = leader.get(\"initial_state\")\n    if not isinstance(initial_state, Mapping):\n        return None\n\n    semi_major_axis = float(initial_state.get(\"semi_major_axis_m\", 0.0))\n    if semi_major_axis <= 0.0:\n        return None\n    eccentricity = float(initial_state.get(\"eccentricity\", 0.0))\n    inclination = float(initial_state.get(\"inclination_rad\", 0.0))\n    raan = math.radians(float(initial_state.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(initial_state.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(initial_state.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _target_coordinates(scenario: Mapping[str, object]) -> tuple[float, float]:\n    \"\"\"Return the target latitude and longitude in radians.\"\"\"\n\n    metadata = scenario.get(\"metadata\")\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    if isinstance(region, Mapping):\n        latitude = math.radians(float(region.get(\"latitude_deg\", 0.0)))\n        longitude = math.radians(float(region.get(\"longitude_deg\", 0.0)))\n        return latitude, longitude\n\n    return math.radians(35.6892), math.radians(51.3890)\n\n\ndef _initial_states(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    drag_coefficient: float,\n    *,\n    epoch_offset_s: float = 0.0,\n) -> list[SpacecraftState]:\n    \"\"\"Construct initial Cartesian states for the formation.\"\"\"\n\n    elements_at_start = leader_elements\n    if epoch_offset_s:\n        mean_motion = leader_elements.mean_motion()\n        propagated_mean_anomaly = _wrap_angle(\n            leader_elements.mean_anomaly + mean_motion * epoch_offset_s\n        )\n        elements_at_start = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis,\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination,\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=propagated_mean_anomaly,\n        )\n\n    position, velocity = _classical_to_cartesian(elements_at_start)\n    rotation = rotation_matrix_rtn_to_eci(position, velocity)\n    states: list[SpacecraftState] = []\n\n    for entry in formation:\n        identifier = str(entry.get(\"identifier\", \"spacecraft\"))\n        radial_offset = float(entry.get(\"radial_offset_km\", 0.0)) * 1_000.0\n        along_track_offset = float(entry.get(\"along_track_offset_km\", 0.0)) * 1_000.0\n        cross_track_offset = float(entry.get(\"cross_track_offset_km\", 0.0)) * 1_000.0\n\n        offset_rtn = np.array([radial_offset, along_track_offset, cross_track_offset], dtype=float)\n        position_shift = rotation @ offset_rtn\n\n        adjusted_position = position + position_shift\n        adjusted_velocity = velocity.copy()\n\n        states.append(\n            SpacecraftState(\n                identifier=identifier,\n                position_m=adjusted_position.astype(float),\n                velocity_mps=adjusted_velocity.astype(float),\n                drag_coefficient=float(entry.get(\"drag_coefficient\", drag_coefficient)),\n                area_m2=4.0,\n                mass_kg=120.0,\n            )\n        )\n\n    return states\n\n\ndef _classical_to_cartesian(elements: OrbitalElements) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return Cartesian state vectors for *elements* without SciPy dependency.\"\"\"\n\n    # This inline implementation mirrors :func:`constellation.orbit.classical_to_cartesian`\n    # but operates fully in metres to avoid repeated conversions.\n    a = elements.semi_major_axis\n    e = elements.eccentricity\n    i = elements.inclination\n    raan = elements.raan\n    argp = elements.arg_perigee\n    m_anomaly = elements.mean_anomaly\n\n    true_anomaly = _mean_to_true(m_anomaly, e)\n    semi_latus = a * (1.0 - e * e)\n    radius = semi_latus / (1.0 + e * math.cos(true_anomaly))\n\n    position_pf = np.array(\n        [radius * math.cos(true_anomaly), radius * math.sin(true_anomaly), 0.0],\n        dtype=float,\n    )\n    velocity_pf = np.array(\n        [\n            -math.sqrt(MU_EARTH / semi_latus) * math.sin(true_anomaly),\n            math.sqrt(MU_EARTH / semi_latus) * (e + math.cos(true_anomaly)),\n            0.0,\n        ],\n        dtype=float,\n    )\n\n    cos_raan = math.cos(raan)\n    sin_raan = math.sin(raan)\n    cos_argp = math.cos(argp)\n    sin_argp = math.sin(argp)\n    cos_i = math.cos(i)\n    sin_i = math.sin(i)\n\n    rotation = np.array(\n        [\n            [\n                cos_raan * cos_argp - sin_raan * sin_argp * cos_i,\n                -cos_raan * sin_argp - sin_raan * cos_argp * cos_i,\n                sin_raan * sin_i,\n            ],\n            [\n                sin_raan * cos_argp + cos_raan * sin_argp * cos_i,\n                -sin_raan * sin_argp + cos_raan * cos_argp * cos_i,\n                -cos_raan * sin_i,\n            ],\n            [sin_argp * sin_i, cos_argp * sin_i, cos_i],\n        ],\n        dtype=float,\n    )\n\n    position = rotation @ position_pf\n    velocity = rotation @ velocity_pf\n    return position, velocity\n\n\ndef _mean_to_true(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert mean anomaly to true anomaly using Newton iterations.\"\"\"\n\n    if eccentricity < 1.0e-12:\n        return _wrap_angle(mean_anomaly)\n\n    eccentric = mean_anomaly\n    for _ in range(30):\n        residual = eccentric - eccentricity * math.sin(eccentric) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric)\n        step = -residual / derivative\n        eccentric += step\n        if abs(step) < 1.0e-12:\n            break\n\n    sine_half = math.sqrt(1.0 + eccentricity) * math.sin(0.5 * eccentric)\n    cosine_half = math.sqrt(1.0 - eccentricity) * math.cos(0.5 * eccentric)\n    return _wrap_angle(2.0 * math.atan2(sine_half, cosine_half))\n\n\ndef _wrap_angle(angle: float) -> float:\n    \"\"\"Normalise *angle* to the ``[0, 2π)`` interval.\"\"\"\n\n    wrapped = math.fmod(angle, 2.0 * math.pi)\n    return wrapped + 2.0 * math.pi if wrapped < 0.0 else wrapped\n\n\ndef _propagate_deterministic(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n) -> tuple[\n    MutableMapping[str, list[tuple[datetime, dict[str, float]]]],\n    MutableMapping[str, object],\n]:\n    \"\"\"Propagate the deterministic trajectory and capture time histories.\"\"\"\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n    states = _initial_states(\n        leader_elements,\n        formation,\n        settings.drag_coefficient,\n        epoch_offset_s=epoch_offset,\n    )\n    epoch = settings.start_time\n    dt = settings.time_step_s\n\n    times: list[datetime] = []\n    cross_track_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    altitude_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    orbital_elements_series: dict[str, dict[str, list[float]]] = {\n        state.identifier: {field: [] for field in CLASSICAL_ELEMENT_FIELDS} for state in states\n    }\n    metrics = _initial_metric_structure(states, target_lat, target_lon)\n    plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n    relative_stats = {\"max_abs\": 0.0, \"min_abs\": math.inf, \"time_of_min\": None, \"series\": []}\n\n    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n        times.append(epoch)\n        for state in states:\n            elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n            per_satellite = orbital_elements_series[state.identifier]\n            per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n            per_satellite[\"eccentricity\"].append(elements.eccentricity)\n            per_satellite[\"inclination_deg\"].append(\n                _normalise_angle_degrees(elements.inclination)\n            )\n            per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n            per_satellite[\"argument_of_perigee_deg\"].append(\n                _normalise_angle_degrees(elements.arg_perigee)\n            )\n            per_satellite[\"mean_anomaly_deg\"].append(\n                _normalise_angle_degrees(elements.mean_anomaly)\n            )\n        _record_metrics(\n            states,\n            epoch,\n            target_lat,\n            target_lon,\n            metrics,\n            cross_track_series,\n            altitude_series,\n            relative_stats,\n        )\n\n        if epoch >= settings.stop_time:\n            break\n\n        states = _rk4_step(states, dt, settings)\n        epoch += timedelta(seconds=dt)\n\n    overall_min_abs = 0.0\n    for entry in metrics[\"vehicles\"]:\n        min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n        if math.isfinite(min_abs):\n            overall_min_abs = max(overall_min_abs, min_abs)\n    metrics[\"overall_min_abs_cross_track_km\"] = float(overall_min_abs)\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    evaluation_timestamp = evaluation_time\n    before_index = after_index = 0\n    fraction = 0.0\n    if times:\n        before_index = max(\n            (idx for idx, stamp in enumerate(times) if stamp <= evaluation_time),\n            default=0,\n        )\n        after_index = min(\n            (idx for idx, stamp in enumerate(times) if stamp >= evaluation_time),\n            default=len(times) - 1,\n        )\n        if after_index < before_index:\n            after_index = before_index\n        before_time = times[before_index]\n        after_time = times[after_index]\n        if after_time > before_time:\n            elapsed = (evaluation_time - before_time).total_seconds()\n            span = (after_time - before_time).total_seconds()\n            fraction = max(0.0, min(elapsed / span, 1.0))\n        else:\n            fraction = 0.0\n\n    evaluation_values: dict[str, float] = {}\n    evaluation_abs: dict[str, float] = {}\n    for identifier, series_values in cross_track_series.items():\n        value_before = float(\"nan\")\n        value_after = float(\"nan\")\n        if series_values:\n            clamped_before = min(before_index, len(series_values) - 1)\n            clamped_after = min(after_index, len(series_values) - 1)\n            value_before = float(series_values[clamped_before])\n            value_after = float(series_values[clamped_after])\n            if math.isfinite(value_before) and math.isfinite(value_after):\n                value = value_before + fraction * (value_after - value_before)\n            elif math.isfinite(value_before):\n                value = value_before\n            elif math.isfinite(value_after):\n                value = value_after\n            else:\n                value = float(\"nan\")\n        else:\n            value = float(\"nan\")\n        evaluation_values[identifier] = value\n        evaluation_abs[identifier] = abs(value) if math.isfinite(value) else math.inf\n\n    centroid_value = fmean(evaluation_values.values()) if evaluation_values else math.nan\n    centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n    worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in evaluation_abs.values() if math.isfinite(value)\n    ]\n    waiver_pass_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    primary_pass_fraction = 1.0 if primary_compliant else 0.0\n\n    for entry in metrics[\"vehicles\"]:\n        identifier = str(entry.get(\"identifier\", \"\"))\n        value = evaluation_values.get(identifier, float(\"nan\"))\n        abs_value = evaluation_abs.get(identifier, math.inf)\n        entry[\"cross_track_at_evaluation_km\"] = (\n            float(value) if math.isfinite(value) else None\n        )\n        entry[\"abs_cross_track_at_evaluation_km\"] = (\n            float(abs_value) if math.isfinite(abs_value) else None\n        )\n        entry[\"compliant\"] = bool(\n            math.isfinite(abs_value) and abs_value <= waiver_limit\n        )\n\n    evaluation_timestamp_iso = evaluation_timestamp.isoformat().replace(\"+00:00\", \"Z\")\n    centroid_value_out = float(centroid_value) if math.isfinite(centroid_value) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    metrics[\"evaluation\"] = {\n        \"time_utc\": evaluation_timestamp_iso,\n        \"vehicles\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_values.items()\n        },\n        \"vehicle_abs\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_abs.items()\n        },\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": float(primary_limit),\n        \"waiver_limit_km\": float(waiver_limit),\n        \"primary_pass_fraction\": float(primary_pass_fraction),\n        \"waiver_pass_fraction\": float(waiver_pass_fraction),\n    }\n    metrics[\"centroid_cross_track_km_at_evaluation\"] = centroid_value_out\n    metrics[\"centroid_abs_cross_track_km_at_evaluation\"] = centroid_abs_out\n    metrics[\"overall_abs_cross_track_km_at_evaluation\"] = worst_abs_out\n    metrics[\"primary_compliant\"] = bool(primary_compliant)\n    metrics[\"waiver_compliant\"] = bool(waiver_compliant)\n    metrics[\"primary_limit_km\"] = float(primary_limit)\n    metrics[\"waiver_limit_km\"] = float(waiver_limit)\n    metrics[\"primary_pass_fraction\"] = float(primary_pass_fraction)\n    metrics[\"waiver_pass_fraction\"] = float(waiver_pass_fraction)\n\n    relative_summary = {\n        \"max_abs_km\": float(relative_stats[\"max_abs\"]),\n        \"min_abs_km\": float(relative_stats[\"min_abs\"] if math.isfinite(relative_stats[\"min_abs\"]) else 0.0),\n        \"time_of_min\": relative_stats[\"time_of_min\"],\n        \"series\": list(relative_stats[\"series\"]),\n    }\n\n    altitude_min = min(min(series) for series in altitude_series.values()) if altitude_series else 0.0\n    altitude_max = max(max(series) for series in altitude_series.values()) if altitude_series else 0.0\n\n    deterministic_metrics: MutableMapping[str, object] = {\n        \"orbital_period_s\": float(orbital_period),\n        \"two_body_period_s\": float(orbital_period),\n        \"cross_track\": metrics,\n        \"altitude_range_m\": {\n            \"min\": float(altitude_min),\n            \"max\": float(altitude_max),\n        },\n    }\n    centroid_summary = metrics.get(\"centroid\", {})\n    if isinstance(centroid_summary, Mapping):\n        deterministic_metrics[\"centroid_cross_track\"] = {\n            \"min_cross_track_km\": float(centroid_summary.get(\"min_cross_track_km\", math.inf)),\n            \"min_abs_cross_track_km\": float(\n                centroid_summary.get(\"min_abs_cross_track_km\", math.inf)\n            ),\n            \"time_of_min_abs_cross_track\": centroid_summary.get(\n                \"time_of_min_abs_cross_track\"\n            ),\n            \"vehicle_cross_track_km\": centroid_summary.get(\n                \"vehicle_cross_track_km_at_min\", {}\n            ),\n            \"vehicle_abs_cross_track_km\": centroid_summary.get(\n                \"vehicle_abs_cross_track_km_at_min\", {}\n            ),\n            \"worst_vehicle_abs_cross_track_km\": float(\n                centroid_summary.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n            ),\n            \"evaluation_cross_track_km\": centroid_value_out,\n            \"evaluation_abs_cross_track_km\": centroid_abs_out,\n            \"worst_vehicle_abs_cross_track_km_at_evaluation\": worst_abs_out,\n            \"evaluation_time_utc\": evaluation_timestamp_iso,\n            \"primary_limit_km\": float(primary_limit),\n            \"waiver_limit_km\": float(waiver_limit),\n            \"primary_compliant\": bool(primary_compliant),\n            \"waiver_compliant\": bool(waiver_compliant),\n        }\n    plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n    plane_limit = settings.plane_intersection_limit_km\n    if plane_limit is not None:\n        plane_metrics[\"limit_km\"] = float(plane_limit)\n        if math.isfinite(plane_distance):\n            plane_metrics[\"compliant\"] = bool(plane_distance <= plane_limit)\n        else:\n            plane_metrics[\"compliant\"] = False\n    else:\n        plane_metrics[\"compliant\"] = None\n    deterministic_metrics[\"plane_intersection\"] = plane_metrics\n    deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    deterministic_metrics[\"cross_track_limits_km\"] = {\n        \"primary\": float(primary_limit),\n        \"waiver\": float(waiver_limit),\n    }\n    deterministic_metrics[\"cross_track_evaluation\"] = metrics[\"evaluation\"]\n    deterministic_metrics[\"orbital_elements\"] = {\n        \"time_series\": {\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n            \"artefact_key\": \"deterministic_orbital_elements_csv\",\n        }\n    }\n\n    series = {\n        \"samples\": [\n            (\n                timestamp,\n                {identifier: cross_track_series[identifier][index] for identifier in cross_track_series},\n            )\n            for index, timestamp in enumerate(times)\n        ],\n        \"altitudes\": {\n            identifier: altitude_series[identifier] for identifier in altitude_series\n        },\n        \"orbital_elements\": {\n            \"times\": list(times),\n            \"per_satellite\": {\n                identifier: {\n                    field: list(values)\n                    for field, values in per_satellite.items()\n                }\n                for identifier, per_satellite in orbital_elements_series.items()\n            },\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n        },\n    }\n\n    return series, deterministic_metrics\n\n\ndef _initial_metric_structure(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Construct the dictionary used to record cross-track statistics.\"\"\"\n\n    vehicle_metrics: list[MutableMapping[str, object]] = []\n    for state in states:\n        vehicle_metrics.append(\n            {\n                \"identifier\": state.identifier,\n                \"target_latitude_deg\": math.degrees(target_lat),\n                \"target_longitude_deg\": math.degrees(target_lon),\n                \"max_cross_track_km\": -math.inf,\n                \"min_cross_track_km\": math.inf,\n                \"max_abs_cross_track_km\": -math.inf,\n                \"min_abs_cross_track_km\": math.inf,\n                \"time_of_max_abs_cross_track\": None,\n                \"time_of_min_abs_cross_track\": None,\n                \"pass_count\": 0,\n                \"compliant\": False,\n            }\n        )\n\n    return {\n        \"vehicles\": vehicle_metrics,\n        \"overall_max_abs_cross_track_km\": -math.inf,\n        \"overall_min_abs_cross_track_km\": math.inf,\n        \"centroid\": {\n            \"min_cross_track_km\": math.inf,\n            \"min_abs_cross_track_km\": math.inf,\n            \"time_of_min_abs_cross_track\": None,\n            \"vehicle_cross_track_km_at_min\": {},\n            \"vehicle_abs_cross_track_km_at_min\": {},\n            \"worst_vehicle_abs_cross_track_km\": math.inf,\n        },\n    }\n\n\ndef _normalise_angle_degrees(angle_rad: float) -> float:\n    \"\"\"Convert radians to wrapped degrees in the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _plane_intersection_metrics(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Compute the plane intersection proximity to the target.\"\"\"\n\n    plane_normals: dict[str, list[np.ndarray]] = {}\n    for state in states:\n        plane = PLANE_ASSIGNMENTS.get(state.identifier, \"Plane A\")\n        h_vec = np.cross(state.position_m, state.velocity_mps)\n        norm = np.linalg.norm(h_vec)\n        if norm == 0.0:\n            continue\n        plane_normals.setdefault(plane, []).append(h_vec / norm)\n\n    averaged_normals: dict[str, np.ndarray] = {}\n    for plane, vectors in plane_normals.items():\n        if not vectors:\n            continue\n        stacked = np.vstack(vectors)\n        mean_vec = np.mean(stacked, axis=0)\n        norm = np.linalg.norm(mean_vec)\n        if norm == 0.0:\n            continue\n        averaged_normals[plane] = mean_vec / norm\n\n    plane_a = averaged_normals.get(\"Plane A\")\n    plane_b = averaged_normals.get(\"Plane B\")\n    if plane_a is None or plane_b is None:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    line_direction = np.cross(plane_a, plane_b)\n    norm_line = np.linalg.norm(line_direction)\n    if norm_line == 0.0:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    unit_direction = line_direction / norm_line\n    candidates = [unit_direction, -unit_direction]\n    candidate_metrics: list[MutableMapping[str, float]] = []\n    min_distance = math.inf\n\n    for candidate in candidates:\n        surface_point = candidate * EARTH_EQUATORIAL_RADIUS_M\n        lat, lon, _ = geodetic_coordinates(surface_point)\n        distance = haversine_distance(lat, lon, target_lat, target_lon)\n        min_distance = min(min_distance, distance)\n        candidate_metrics.append(\n            {\n                \"latitude_deg\": math.degrees(lat),\n                \"longitude_deg\": math.degrees(lon),\n                \"distance_to_target_km\": distance / 1_000.0,\n            }\n        )\n\n    return {\n        \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n        \"intersection_candidates\": candidate_metrics,\n        \"target_distance_km\": min_distance / 1_000.0,\n    }\n\n\ndef _record_metrics(\n    states: Sequence[SpacecraftState],\n    epoch: datetime,\n    target_lat: float,\n    target_lon: float,\n    metrics: MutableMapping[str, object],\n    cross_track_series: MutableMapping[str, list[float]],\n    altitude_series: MutableMapping[str, list[float]],\n    relative_stats: MutableMapping[str, object],\n) -> None:\n    \"\"\"Update cross-track statistics for the current epoch.\"\"\"\n\n    vehicle_entries = metrics[\"vehicles\"]\n    centroid_metrics = metrics.get(\"centroid\", {})\n    epoch_values: dict[str, float] = {}\n\n    for index, state in enumerate(states):\n        position_ecef = inertial_to_ecef(state.position_m, epoch)\n        latitude, longitude, altitude = geodetic_coordinates(position_ecef)\n        cross_track_distance_m = haversine_distance(latitude, longitude, target_lat, target_lon)\n        sign = 1.0 if latitude >= target_lat else -1.0\n        cross_track_km = sign * (cross_track_distance_m / 1_000.0)\n\n        cross_track_series[state.identifier].append(float(cross_track_km))\n        altitude_series[state.identifier].append(float(altitude))\n        epoch_values[state.identifier] = float(cross_track_km)\n\n        entry = vehicle_entries[index]\n        entry[\"max_cross_track_km\"] = max(entry[\"max_cross_track_km\"], cross_track_km)\n        entry[\"min_cross_track_km\"] = min(entry[\"min_cross_track_km\"], cross_track_km)\n\n        abs_value = abs(cross_track_km)\n        if abs_value > entry[\"max_abs_cross_track_km\"]:\n            entry[\"max_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_max_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        if abs_value < entry[\"min_abs_cross_track_km\"]:\n            entry[\"min_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_min_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n\n        previous_series = cross_track_series[state.identifier]\n        if len(previous_series) >= 2 and previous_series[-2] == 0.0:\n            entry[\"pass_count\"] += 1\n        elif len(previous_series) >= 2 and previous_series[-2] * cross_track_km < 0.0:\n            entry[\"pass_count\"] += 1\n\n        metrics[\"overall_max_abs_cross_track_km\"] = max(\n            metrics[\"overall_max_abs_cross_track_km\"], abs_value\n        )\n        metrics[\"overall_min_abs_cross_track_km\"] = min(\n            metrics[\"overall_min_abs_cross_track_km\"], abs_value\n        )\n\n    if epoch_values and isinstance(centroid_metrics, MutableMapping):\n        centroid_value = fmean(epoch_values.values())\n        abs_centroid = abs(centroid_value)\n        if abs_centroid < centroid_metrics.get(\"min_abs_cross_track_km\", math.inf):\n            centroid_metrics[\"min_cross_track_km\"] = float(centroid_value)\n            centroid_metrics[\"min_abs_cross_track_km\"] = float(abs_centroid)\n            centroid_metrics[\"time_of_min_abs_cross_track\"] = (\n                epoch.isoformat().replace(\"+00:00\", \"Z\")\n            )\n            centroid_metrics[\"vehicle_cross_track_km_at_min\"] = {\n                identifier: float(value) for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"vehicle_abs_cross_track_km_at_min\"] = {\n                identifier: float(abs(value))\n                for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"worst_vehicle_abs_cross_track_km\"] = float(\n                max(abs(value) for value in epoch_values.values())\n            )\n\n    leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n    plane_b_state = next(\n        (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n        None,\n    )\n    if leader_state is not None and plane_b_state is not None:\n        normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n        norm_normal = np.linalg.norm(normal_vec)\n        if norm_normal > 0.0:\n            normal_unit = normal_vec / norm_normal\n            relative_vec = plane_b_state.position_m - leader_state.position_m\n            cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n            abs_val = abs(cross_track_km)\n            if abs_val > relative_stats[\"max_abs\"]:\n                relative_stats[\"max_abs\"] = abs_val\n            if abs_val < relative_stats[\"min_abs\"]:\n                relative_stats[\"min_abs\"] = abs_val\n                relative_stats[\"time_of_min\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n            relative_stats[\"series\"].append(\n                (epoch.isoformat().replace(\"+00:00\", \"Z\"), cross_track_km)\n            )\n\n\ndef _rk4_step(states: Sequence[SpacecraftState], dt: float, settings: PropagatorSettings) -> list[SpacecraftState]:\n    \"\"\"Advance the spacecraft states by *dt* seconds using RK4 integration.\"\"\"\n\n    next_states: list[SpacecraftState] = []\n    for state in states:\n        x0 = state.position_m\n        v0 = state.velocity_mps\n\n        a1 = _acceleration(x0, v0, state, settings)\n        k1_pos = v0\n        k1_vel = a1\n\n        a2 = _acceleration(x0 + 0.5 * dt * k1_pos, v0 + 0.5 * dt * k1_vel, state, settings)\n        k2_pos = v0 + 0.5 * dt * k1_vel\n        k2_vel = a2\n\n        a3 = _acceleration(x0 + 0.5 * dt * k2_pos, v0 + 0.5 * dt * k2_vel, state, settings)\n        k3_pos = v0 + 0.5 * dt * k2_vel\n        k3_vel = a3\n\n        a4 = _acceleration(x0 + dt * k3_pos, v0 + dt * k3_vel, state, settings)\n        k4_pos = v0 + dt * k3_vel\n        k4_vel = a4\n\n        new_position = x0 + (dt / 6.0) * (k1_pos + 2.0 * k2_pos + 2.0 * k3_pos + k4_pos)\n        new_velocity = v0 + (dt / 6.0) * (k1_vel + 2.0 * k2_vel + 2.0 * k3_vel + k4_vel)\n\n        state.position_m = new_position\n        state.velocity_mps = new_velocity\n        next_states.append(state)\n\n    return next_states\n\n\ndef _acceleration(\n    position: np.ndarray,\n    velocity: np.ndarray,\n    state: SpacecraftState,\n    settings: PropagatorSettings,\n) -> np.ndarray:\n    r\"\"\"Return acceleration including central gravity, \\(J_2\\), and drag.\"\"\"\n\n    r_vec = position\n    v_vec = velocity\n    r_norm = np.linalg.norm(r_vec)\n\n    if r_norm == 0.0:\n        raise ValueError(\"State vector has zero magnitude; cannot propagate.\")\n\n    mu = MU_EARTH\n    central = -mu * r_vec / (r_norm**3)\n\n    z2 = r_vec[2] ** 2\n    r2 = r_norm**2\n    factor = 1.5 * J2_COEFFICIENT * mu * (EARTH_EQUATORIAL_RADIUS_M**2) / (r_norm**5)\n    coef_xy = factor * (5.0 * z2 / r2 - 1.0)\n    coef_z = factor * (5.0 * z2 / r2 - 3.0)\n    accel_j2 = np.array([coef_xy * r_vec[0], coef_xy * r_vec[1], coef_z * r_vec[2]], dtype=float)\n\n    v_rel = v_vec - np.cross(OMEGA_EARTH_VECTOR, r_vec)\n    speed_rel = np.linalg.norm(v_rel)\n    altitude = r_norm - EARTH_EQUATORIAL_RADIUS_M\n    density = _atmospheric_density(altitude, settings.solar_flux_index)\n    drag = np.zeros(3, dtype=float)\n\n    if speed_rel > 0.0 and density > 0.0:\n        ballistic = state.area_m2 / state.mass_kg\n        drag = (\n            -0.5\n            * density\n            * state.drag_coefficient\n            * ballistic\n            * speed_rel\n            * v_rel\n        )\n\n    return central + accel_j2 + drag\n\n\ndef _atmospheric_density(altitude_m: float, solar_index: float) -> float:\n    \"\"\"Return a simple exponential atmospheric density profile.\"\"\"\n\n    base_density = 3.614e-11  # kg/m^3 at 400 km reference\n    scale_height = 55_000.0 * math.sqrt(max(solar_index, 1.0) / SOLAR_FLUX_BASE)\n    density = base_density * math.exp(-(altitude_m - 400_000.0) / scale_height)\n    return max(density, 0.0)\n\n\ndef _run_monte_carlo(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n    monte_carlo: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Execute Monte Carlo dispersions and aggregate cross-track statistics.\"\"\"\n\n    if not monte_carlo.get(\"enabled\", True):\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n\n    runs = int(monte_carlo.get(\"runs\", 0))\n    if runs <= 0:\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n    runs = max(runs, 500)\n\n    dispersions = monte_carlo.get(\"dispersions\", {})\n    sigma_a = float(dispersions.get(\"semi_major_axis_sigma_m\", 0.0))\n    sigma_i = math.radians(float(dispersions.get(\"inclination_sigma_deg\", 0.0)))\n    sigma_cd = float(dispersions.get(\"drag_coefficient_sigma\", 0.0))\n\n    seed = int(monte_carlo.get(\"seed\", 42))\n    rng = np.random.default_rng(seed)\n\n    vehicle_ids = [entry.get(\"identifier\", \"spacecraft\") for entry in formation]\n    run_metrics_max: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_min: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_eval: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    intersection_distances: list[float] = []\n    relative_max_list: list[float] = []\n    relative_min_list: list[float] = []\n    centroid_abs_list: list[float] = []\n    worst_abs_list: list[float] = []\n\n    primary_success_count = 0\n    waiver_success_count = 0\n    relative_success_count = 0\n    plane_success_count = 0\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    plane_limit = settings.plane_intersection_limit_km\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n\n    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):\n        dispersed_elements = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis + rng.normal(0.0, sigma_a),\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination + rng.normal(0.0, sigma_i),\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=leader_elements.mean_anomaly,\n        )\n\n        drag_scale = 1.0 + rng.normal(0.0, sigma_cd)\n        states = _initial_states(\n            dispersed_elements,\n            formation,\n            settings.drag_coefficient * drag_scale,\n            epoch_offset_s=epoch_offset,\n        )\n        plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n        plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n\n        epoch = settings.start_time\n        dt = settings.time_step_s\n        vehicle_metrics = _initial_metric_structure(states, target_lat, target_lon)\n        previous_signs = {state.identifier: 0.0 for state in states}\n        relative_run_stats = {\"max\": 0.0, \"min\": math.inf}\n        evaluation_snapshot: dict[str, float] = {}\n        evaluation_best = math.inf\n\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            current_values: dict[str, float] = {}\n            for idx, state in enumerate(states):\n                position_ecef = inertial_to_ecef(state.position_m, epoch)\n                latitude, longitude, _ = geodetic_coordinates(position_ecef)\n                distance = haversine_distance(latitude, longitude, target_lat, target_lon)\n                sign = 1.0 if latitude >= target_lat else -1.0\n                cross_track_km = sign * (distance / 1_000.0)\n\n                vehicle_entry = vehicle_metrics[\"vehicles\"][idx]\n                vehicle_entry[\"max_cross_track_km\"] = max(\n                    vehicle_entry[\"max_cross_track_km\"], cross_track_km\n                )\n                vehicle_entry[\"min_cross_track_km\"] = min(\n                    vehicle_entry[\"min_cross_track_km\"], cross_track_km\n                )\n                abs_value = abs(cross_track_km)\n                if abs_value > vehicle_entry[\"max_abs_cross_track_km\"]:\n                    vehicle_entry[\"max_abs_cross_track_km\"] = abs_value\n                if abs_value < vehicle_entry[\"min_abs_cross_track_km\"]:\n                    vehicle_entry[\"min_abs_cross_track_km\"] = abs_value\n\n                current_values[str(state.identifier)] = float(cross_track_km)\n\n                identifier = str(state.identifier)\n                previous_sign = previous_signs[identifier]\n                if previous_sign != 0.0 and previous_sign * cross_track_km < 0.0:\n                    vehicle_entry[\"pass_count\"] += 1\n                previous_signs[identifier] = cross_track_km\n\n            diff = abs((epoch - evaluation_time).total_seconds())\n            if diff < evaluation_best:\n                evaluation_best = diff\n                evaluation_snapshot = dict(current_values)\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n\n            leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n            plane_b_state = next(\n                (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n                None,\n            )\n            if leader_state is not None and plane_b_state is not None:\n                normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n                norm_normal = np.linalg.norm(normal_vec)\n                if norm_normal > 0.0:\n                    normal_unit = normal_vec / norm_normal\n                    relative_vec = plane_b_state.position_m - leader_state.position_m\n                    cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n                    abs_val = abs(cross_track_km)\n                    if abs_val > relative_run_stats[\"max\"]:\n                        relative_run_stats[\"max\"] = abs_val\n                    if abs_val < relative_run_stats[\"min\"]:\n                        relative_run_stats[\"min\"] = abs_val\n\n        evaluation_abs = {\n            identifier: abs(value) if math.isfinite(value) else math.inf\n            for identifier, value in evaluation_snapshot.items()\n        }\n        centroid_value = (\n            fmean(evaluation_snapshot.values()) if evaluation_snapshot else math.nan\n        )\n        centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n        worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n        primary_compliant = (\n            math.isfinite(centroid_abs)\n            and centroid_abs <= primary_limit\n            and math.isfinite(worst_abs)\n            and worst_abs <= waiver_limit\n        )\n        waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n\n        if math.isfinite(centroid_abs):\n            centroid_abs_list.append(centroid_abs)\n        if math.isfinite(worst_abs):\n            worst_abs_list.append(worst_abs)\n\n        for entry in vehicle_metrics[\"vehicles\"]:\n            identifier = str(entry[\"identifier\"])\n            max_abs = float(entry[\"max_abs_cross_track_km\"])\n            min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n            value = evaluation_snapshot.get(identifier, float(\"nan\"))\n            abs_value = evaluation_abs.get(identifier, math.inf)\n            entry[\"cross_track_at_evaluation_km\"] = (\n                float(value) if math.isfinite(value) else None\n            )\n            entry[\"abs_cross_track_at_evaluation_km\"] = (\n                float(abs_value) if math.isfinite(abs_value) else None\n            )\n            entry[\"compliant\"] = bool(\n                math.isfinite(abs_value) and abs_value <= waiver_limit\n            )\n            run_metrics_max[identifier].append(max_abs)\n            if math.isfinite(min_abs):\n                run_metrics_min[identifier].append(min_abs)\n            if math.isfinite(abs_value):\n                run_metrics_eval[identifier].append(abs_value)\n\n        plane_success = True\n        if math.isfinite(plane_distance):\n            if plane_limit is not None:\n                intersection_distances.append(plane_distance)\n                plane_success = plane_distance <= plane_limit\n        elif plane_limit is not None:\n            plane_success = False\n\n        if plane_success:\n            plane_success_count += 1\n\n        if math.isfinite(relative_run_stats[\"max\"]):\n            relative_max_list.append(relative_run_stats[\"max\"])\n        if math.isfinite(relative_run_stats[\"min\"]):\n            relative_min_list.append(relative_run_stats[\"min\"])\n\n        if primary_compliant:\n            primary_success_count += 1\n        if waiver_compliant:\n            waiver_success_count += 1\n        if relative_run_stats[\"max\"] <= waiver_limit:\n            relative_success_count += 1\n\n    aggregated: MutableMapping[str, object] = {\n        \"runs\": runs,\n        \"seed\": seed,\n        \"max_abs_cross_track_km\": {},\n        \"min_abs_cross_track_km\": {},\n        \"evaluation_abs_cross_track_km\": {},\n        \"relative_cross_track_km\": {},\n        \"compliance\": {\n            \"primary_fraction\": float(primary_success_count / runs) if runs else 0.0,\n            \"waiver_fraction\": float(waiver_success_count / runs) if runs else 0.0,\n            \"relative_fraction\": float(relative_success_count / runs) if runs else 0.0,\n        },\n        \"fleet_compliance_probability\": float(primary_success_count / runs)\n        if runs\n        else 0.0,\n        \"absolute_cross_track_compliance_probability\": float(\n            waiver_success_count / runs\n        )\n        if runs\n        else 0.0,\n        \"evaluation_time_utc\": evaluation_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"cross_track_limits_km\": {\n            \"primary\": float(primary_limit),\n            \"waiver\": float(waiver_limit),\n        },\n    }\n\n    if plane_limit is not None:\n        aggregated[\"plane_intersection_distance_km\"] = {}\n\n    if plane_limit is not None:\n        aggregated[\"compliance\"][\"plane_fraction\"] = float(\n            plane_success_count / runs\n        ) if runs else 0.0\n\n    for identifier, values in run_metrics_max.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"max_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_min.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"min_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_eval.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"evaluation_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if plane_limit is not None and intersection_distances:\n        array = np.asarray(intersection_distances, dtype=float)\n        aggregated[\"plane_intersection_distance_km\"][\"fleet\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if relative_max_list:\n        array = np.asarray(relative_max_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_max\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n    if relative_min_list:\n        array = np.asarray(relative_min_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_min\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if centroid_abs_list:\n        array = np.asarray(centroid_abs_list, dtype=float)\n        centroid_stats = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n        aggregated[\"centroid_abs_cross_track_km\"] = centroid_stats\n        aggregated[\"centroid_abs_cross_track_km_mean\"] = centroid_stats[\"mean\"]\n        aggregated[\"centroid_abs_cross_track_km_p95\"] = centroid_stats[\"p95\"]\n    if worst_abs_list:\n        array = np.asarray(worst_abs_list, dtype=float)\n        aggregated[\"worst_vehicle_abs_cross_track_km\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    return aggregated\n\n\ndef _write_outputs(\n    output_directory: Path,\n    series: Mapping[str, object],\n    deterministic_metrics: Mapping[str, object],\n    monte_carlo_metrics: Mapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, str]:\n    \"\"\"Persist deterministic and Monte Carlo outputs to the run directory.\"\"\"\n\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    deterministic_path = output_directory / \"deterministic_cross_track.csv\"\n    relative_path = output_directory / \"relative_cross_track.csv\"\n    summary_path = output_directory / \"deterministic_summary.json\"\n    orbital_path = output_directory / \"deterministic_orbital_elements.csv\"\n    monte_carlo_path = output_directory / \"monte_carlo_cross_track.csv\"\n    monte_carlo_summary_path = output_directory / \"monte_carlo_summary.json\"\n    settings_path = output_directory / \"solver_settings.json\"\n\n    _write_cross_track_csv(deterministic_path, series)\n    orbital_series = series.get(\"orbital_elements\") if isinstance(series, Mapping) else {}\n    _write_orbital_elements_csv(orbital_path, orbital_series)\n\n    relative_summary = deterministic_metrics.get(\"relative_cross_track\")\n    if isinstance(relative_summary, Mapping):\n        _write_relative_cross_track_csv(relative_path, relative_summary)\n        relative_summary = dict(relative_summary)\n        relative_summary.pop(\"series\", None)\n        deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    else:\n        _write_relative_cross_track_csv(relative_path, {})\n\n    summary_payload = {\n        \"metrics\": deterministic_metrics,\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": settings.time_step_s,\n            \"drag_coefficient\": settings.drag_coefficient,\n            \"ballistic_coefficient_m2_per_kg\": settings.ballistic_coefficient_m2_per_kg,\n            \"solar_flux_index\": settings.solar_flux_index,\n        },\n    }\n    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding=\"utf-8\")\n\n    _write_monte_carlo_csv(monte_carlo_path, monte_carlo_metrics)\n    monte_carlo_summary_path.write_text(json.dumps(monte_carlo_metrics, indent=2), encoding=\"utf-8\")\n\n    settings_payload = {\n        \"propagator\": {\n            \"integrator\": \"fixed_step_rk4\",\n            \"time_step_s\": settings.time_step_s,\n            \"force_models\": {\n                \"central_body\": True,\n                \"j2\": True,\n                \"atmospheric_drag\": {\n                    \"model\": \"exponential\",\n                    \"solar_flux_index\": settings.solar_flux_index,\n                },\n            },\n        },\n        \"monte_carlo\": {\n            \"runs\": monte_carlo_config.get(\"runs\", 0),\n            \"seed\": monte_carlo_config.get(\"seed\", 0),\n            \"dispersions\": monte_carlo_config.get(\"dispersions\", {}),\n        },\n        \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n    settings_path.write_text(json.dumps(settings_payload, indent=2), encoding=\"utf-8\")\n\n    orbital_metrics = deterministic_metrics.get(\"orbital_elements\")\n    if isinstance(orbital_metrics, Mapping):\n        time_series_meta = orbital_metrics.get(\"time_series\")\n        if isinstance(time_series_meta, Mapping):\n            time_series_meta[\"artefact\"] = str(orbital_path)\n\n    return {\n        \"deterministic_cross_track_csv\": str(deterministic_path),\n        \"deterministic_orbital_elements_csv\": str(orbital_path),\n        \"deterministic_summary_json\": str(summary_path),\n        \"monte_carlo_cross_track_csv\": str(monte_carlo_path),\n        \"relative_cross_track_csv\": str(relative_path),\n        \"monte_carlo_summary_json\": str(monte_carlo_summary_path),\n        \"solver_settings_json\": str(settings_path),\n    }\n\n\ndef _write_cross_track_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Write deterministic cross-track histories to *path*.\"\"\"\n\n    samples = series.get(\"samples\", [])\n    if not samples:\n        path.write_text(\"time,\\n\", encoding=\"utf-8\")\n        return\n\n    vehicle_ids = sorted(next(iter(samples))[1])\n    header = [\"time_iso\"] + vehicle_ids\n    lines = [\",\".join(header)]\n\n    for timestamp, cross_track in samples:\n        row = [timestamp.isoformat().replace(\"+00:00\", \"Z\")]\n        for identifier in vehicle_ids:\n            row.append(f\"{cross_track.get(identifier, 0.0):.6f}\")\n        lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_orbital_elements_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Persist orbital-element histories to *path*.\"\"\"\n\n    header = \"time_iso,satellite_id,\" + \",\".join(CLASSICAL_ELEMENT_FIELDS)\n    if not isinstance(series, Mapping) or not series:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    times = list(series.get(\"times\", []))\n    per_satellite = series.get(\"per_satellite\")\n    if not isinstance(per_satellite, Mapping) or not times:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [header]\n    for index, timestamp in enumerate(times):\n        if isinstance(timestamp, datetime):\n            stamp = timestamp.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        else:\n            stamp = str(timestamp)\n        for identifier in sorted(per_satellite):\n            elements = per_satellite.get(identifier)\n            if not isinstance(elements, Mapping):\n                continue\n            row = [stamp, str(identifier)]\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                value = float(\"nan\")\n                if isinstance(values, Sequence) and index < len(values):\n                    try:\n                        value = float(values[index])  # type: ignore[arg-type]\n                    except (TypeError, ValueError):\n                        value = float(\"nan\")\n                row.append(f\"{value:.9g}\" if math.isfinite(value) else \"\")\n            lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_monte_carlo_csv(path: Path, metrics: Mapping[str, object]) -> None:\n    \"\"\"Write Monte Carlo aggregate metrics to *path*.\"\"\"\n\n    vehicle_stats = metrics.get(\"max_abs_cross_track_km\", {})\n    min_stats_all = metrics.get(\"min_abs_cross_track_km\", {})\n    plane_stats = metrics.get(\"plane_intersection_distance_km\", {})\n    relative_stats = metrics.get(\"relative_cross_track_km\", {})\n    if (\n        not vehicle_stats\n        and not min_stats_all\n        and not plane_stats\n        and not relative_stats\n    ):\n        path.write_text(\n            \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n            \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n            \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n            \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n            \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n            \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\\n\",\n            encoding=\"utf-8\",\n        )\n        return\n\n    header = (\n        \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n        \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n        \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n        \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n        \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n        \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\"\n    )\n    lines = [header]\n\n    identifiers = sorted(\n        set(\n            list(vehicle_stats.keys())\n            + list(min_stats_all.keys())\n            + list(plane_stats.keys())\n            + list(relative_stats.keys())\n        )\n    )\n\n    for identifier in identifiers:\n        stats = vehicle_stats.get(identifier, {})\n        min_stats = min_stats_all.get(identifier, {})\n        plane_entry = plane_stats.get(identifier, {})\n        relative_entry = relative_stats.get(identifier, {})\n        line = \",\".join(\n            [\n                str(identifier),\n                f\"{min_stats.get('mean', 0.0):.6f}\",\n                f\"{min_stats.get('std', 0.0):.6f}\",\n                f\"{min_stats.get('p95', 0.0):.6f}\",\n                f\"{min_stats.get('min', 0.0):.6f}\",\n                f\"{min_stats.get('max', 0.0):.6f}\",\n                f\"{stats.get('mean', 0.0):.6f}\",\n                f\"{stats.get('std', 0.0):.6f}\",\n                f\"{stats.get('p95', 0.0):.6f}\",\n                f\"{stats.get('min', 0.0):.6f}\",\n                f\"{stats.get('max', 0.0):.6f}\",\n                f\"{plane_entry.get('mean', 0.0):.6f}\",\n                f\"{plane_entry.get('std', 0.0):.6f}\",\n                f\"{plane_entry.get('p95', 0.0):.6f}\",\n                f\"{plane_entry.get('min', 0.0):.6f}\",\n                f\"{plane_entry.get('max', 0.0):.6f}\",\n                f\"{relative_entry.get('mean', 0.0):.6f}\",\n                f\"{relative_entry.get('std', 0.0):.6f}\",\n                f\"{relative_entry.get('p95', 0.0):.6f}\",\n                f\"{relative_entry.get('min', 0.0):.6f}\",\n                f\"{relative_entry.get('max', 0.0):.6f}\",\n            ]\n        )\n        lines.append(line)\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_relative_cross_track_csv(path: Path, summary: Mapping[str, object]) -> None:\n    \"\"\"Write relative cross-track series to *path*.\"\"\"\n\n    series = summary.get(\"series\") if isinstance(summary, Mapping) else None\n    if not series:\n        path.write_text(\"time_iso,relative_cross_track_km\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [\"time_iso,relative_cross_track_km\"]\n    for time_iso, value in series:\n        lines.append(f\"{time_iso},{float(value):.6f}\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _parse_time(value: object) -> datetime | None:\n    \"\"\"Parse an ISO-8601 timestamp into an aware :class:`datetime`.\"\"\"\n\n    if isinstance(value, str):\n        try:\n            if value.endswith(\"Z\"):\n                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n            parsed = datetime.fromisoformat(value)\n            if parsed.tzinfo is None:\n                return parsed.replace(tzinfo=timezone.utc)\n            return parsed.astimezone(timezone.utc)\n        except ValueError:\n            return None\n    return None\n\n\n__all__ = [\n    \"PropagatorSettings\",\n    \"propagate_constellation\",\n    \"scenario_cross_track_limits\",\n    \"scenario_access_window\",\n]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Okay, I'm adding progress bars to `sim/scripts/perturbation_analysis.py`. In `_propagate_deterministic`, I'll calculate `n_steps` and wrap the `for` loop with `tqdm(range(n_steps), desc=\"Deterministic propagation\")`. For `_run_monte_carlo`, I'll wrap the outer `for _ in range(runs)` loop with `tqdm` to show progress on the Monte Carlo runs. I'll start with `_propagate_deterministic`. I'll reread the file for context and then apply the change."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762792371015-84abc2edc6232",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"High-fidelity constellation propagation with \\(J_2\\) and drag perturbations.\n\nThe helper functions defined here extend the lightweight scenario pipeline by\nintroducing a deterministic force model and a Monte Carlo dispersion analysis\ntailored to the Tehran daily pass configuration.  The propagated states capture\ncross-track offsets for every spacecraft in the formation, enabling direct\nverification of the ±30 km primary alignment tolerance (and the ±70 km waiver\nceiling) that governs MR-2 and SRD-P-001.  Outputs are serialisable to JSON and\ncomma-separated value (CSV) formats so they remain compatible with Systems Tool\nKit (STK 11.2) validation workflows as required by the repository guidelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Mapping, MutableMapping, Sequence\n\nimport os\n\nimport numpy as np\nfrom statistics import fmean\n\nfrom src.constellation.frames import rotation_matrix_rtn_to_eci\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    EARTH_ROTATION_RATE,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\n\n\nJ2_COEFFICIENT = 1.08262668e-3\nSOLAR_FLUX_BASE = 150.0\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\n\nDEFAULT_FORMATION = (\n    {\n        \"identifier\": \"FSAT-LDR\",\n        \"role\": \"leader\",\n        \"radial_offset_km\": 0.0,\n        \"along_track_offset_km\": 0.0,\n        \"cross_track_offset_km\": 0.0,\n    },\n    {\n        \"identifier\": \"FSAT-DP1\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": -0.05,\n        \"along_track_offset_km\": 20.0,\n        \"cross_track_offset_km\": 0.2,\n    },\n    {\n        \"identifier\": \"FSAT-DP2\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": 0.05,\n        \"along_track_offset_km\": -20.0,\n        \"cross_track_offset_km\": -0.2,\n    },\n)\n\nPLANE_ASSIGNMENTS = {\n    \"FSAT-LDR\": \"Plane A\",\n    \"FSAT-DP1\": \"Plane A\",\n    \"FSAT-DP2\": \"Plane B\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nCLASSICAL_ELEMENT_UNITS = {\n    \"semi_major_axis_km\": \"km\",\n    \"eccentricity\": \"\",\n    \"inclination_deg\": \"deg\",\n    \"raan_deg\": \"deg\",\n    \"argument_of_perigee_deg\": \"deg\",\n    \"mean_anomaly_deg\": \"deg\",\n}\n\n\nDEFAULT_MONTE_CARLO = {\n    \"enabled\": True,\n    \"runs\": 1000,\n    \"seed\": 4242,\n    \"dispersions\": {\n        \"semi_major_axis_sigma_m\": 5.0,\n        \"inclination_sigma_deg\": 0.01,\n        \"drag_coefficient_sigma\": 0.05,\n    },\n}\n\n\n@dataclass\nclass PropagatorSettings:\n    \"\"\"Container describing the deterministic propagation configuration.\"\"\"\n\n    start_time: datetime\n    epoch_time: datetime\n    stop_time: datetime\n    time_step_s: float\n    drag_coefficient: float\n    ballistic_coefficient_m2_per_kg: float\n    solar_flux_index: float\n    evaluation_time: datetime | None = None\n    primary_cross_track_limit_km: float = 30.0\n    waiver_cross_track_limit_km: float = 70.0\n    plane_intersection_limit_km: float | None = None\n\n\n@dataclass\nclass SpacecraftState:\n    \"\"\"Cartesian state and drag properties for a spacecraft.\"\"\"\n\n    identifier: str\n    position_m: np.ndarray\n    velocity_mps: np.ndarray\n    drag_coefficient: float\n    area_m2: float\n    mass_kg: float\n\n\ndef propagate_constellation(\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Path | None = None,\n    formation: Sequence[Mapping[str, float | str]] = DEFAULT_FORMATION,\n    settings: PropagatorSettings | None = None,\n    monte_carlo: Mapping[str, object] | None = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation including \\(J_2\\) and atmospheric drag.\"\"\"\n\n    resolved_settings = settings or _default_settings(scenario)\n    monte_carlo_config = _normalise_monte_carlo(monte_carlo)\n\n    target_lat, target_lon = _target_coordinates(scenario)\n    leader_elements = _scenario_orbital_elements(scenario)\n\n    deterministic_series, deterministic_metrics = _propagate_deterministic(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n    )\n\n    monte_carlo_metrics = _run_monte_carlo(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n        monte_carlo_config,\n    )\n\n    artefact_paths: MutableMapping[str, str] = {}\n    if output_directory:\n        artefact_paths = _write_outputs(\n            Path(output_directory),\n            deterministic_series,\n            deterministic_metrics,\n            monte_carlo_metrics,\n            resolved_settings,\n            monte_carlo_config,\n        )\n\n    summary = _build_summary(\n        deterministic_metrics,\n        monte_carlo_metrics,\n        resolved_settings,\n        monte_carlo_config,\n    )\n\n    return summary, artefact_paths\n\n\ndef scenario_cross_track_limits(\n    scenario: Mapping[str, object]\n) -> tuple[float, float, float | None]:\n    \"\"\"Return the primary, waiver, and optional plane-intersection limits.\"\"\"\n\n    primary_limit = float(os.getenv(\"FSAT_PRIMARY_CROSS_TRACK_LIMIT_KM\", \"30.0\"))\n    waiver_limit = float(os.getenv(\"FSAT_WAIVER_CROSS_TRACK_LIMIT_KM\", \"70.0\"))\n    plane_limit_env = os.getenv(\"FSAT_PLANE_INTERSECTION_LIMIT_KM\")\n    plane_limit: float | None = (\n        float(plane_limit_env) if plane_limit_env is not None else None\n    )\n\n    limits = scenario.get(\"cross_track_limits\")\n    if isinstance(limits, Mapping):\n        if \"primary_km\" in limits:\n            try:\n                primary_limit = float(limits[\"primary_km\"])\n            except (TypeError, ValueError):\n                pass\n        if \"waiver_km\" in limits:\n            try:\n                waiver_limit = float(limits[\"waiver_km\"])\n            except (TypeError, ValueError):\n                pass\n        plane_candidate = limits.get(\"plane_intersection_limit_km\")\n        if plane_candidate is not None:\n            try:\n                plane_limit = float(plane_candidate)\n            except (TypeError, ValueError):\n                pass\n    else:\n        requirements = scenario.get(\"alignment_requirements\")\n        if isinstance(requirements, Mapping):\n            primary_limit = float(\n                requirements.get(\"primary_cross_track_limit_km\", primary_limit)\n            )\n            waiver_limit = float(\n                requirements.get(\"waiver_cross_track_limit_km\", waiver_limit)\n            )\n            plane_candidate = requirements.get(\"plane_intersection_limit_km\", plane_limit)\n            if plane_candidate is not None:\n                plane_limit = float(plane_candidate)\n\n    return primary_limit, waiver_limit, plane_limit\n\n\ndef scenario_access_window(\n    scenario: Mapping[str, object]\n) -> tuple[datetime | None, datetime | None, datetime | None]:\n    \"\"\"Return start, end, and midpoint timestamps for the primary access window.\"\"\"\n\n    window = scenario.get(\"access_window\")\n    start = _parse_time(window.get(\"start_utc\")) if isinstance(window, Mapping) else None\n    end = _parse_time(window.get(\"end_utc\")) if isinstance(window, Mapping) else None\n    midpoint = (\n        _parse_time(window.get(\"midpoint_utc\")) if isinstance(window, Mapping) else None\n    )\n\n    timing = (\n        scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    )\n    windows = timing.get(\"daily_access_windows\")\n    if start is None or end is None:\n        if isinstance(windows, Sequence) and windows:\n            primary = windows[0] if isinstance(windows[0], Mapping) else {}\n            if start is None:\n                start = _parse_time(primary.get(\"start_utc\"))\n            if end is None:\n                end = _parse_time(primary.get(\"end_utc\"))\n            if midpoint is None:\n                midpoint = _parse_time(primary.get(\"midpoint_utc\"))\n\n    if midpoint is None and start and end:\n        midpoint = start + (end - start) / 2\n    elif midpoint is None and start:\n        midpoint = start + timedelta(seconds=45.0)\n\n    return start, end, midpoint\n\n\ndef _build_summary(\n    deterministic_metrics: MutableMapping[str, object],\n    monte_carlo_metrics: MutableMapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Combine deterministic and Monte Carlo outputs into a summary map.\"\"\"\n\n    orbital_period = deterministic_metrics.get(\"orbital_period_s\", 0.0)\n    two_body_period = deterministic_metrics.get(\"two_body_period_s\", orbital_period)\n    period_delta = float(orbital_period) - float(two_body_period)\n\n    return {\n        \"model\": \"high_fidelity_j2_drag\",\n        \"orbital_period_s\": float(orbital_period),\n        \"period_delta_s\": float(period_delta),\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": float(settings.time_step_s),\n            \"drag_coefficient\": float(settings.drag_coefficient),\n            \"ballistic_coefficient_m2_per_kg\": float(settings.ballistic_coefficient_m2_per_kg),\n            \"solar_flux_index\": float(settings.solar_flux_index),\n            \"monte_carlo_runs\": int(monte_carlo_config.get(\"runs\", 0)),\n        },\n        \"cross_track\": deterministic_metrics.get(\"cross_track\", {}),\n        \"altitude_range_m\": deterministic_metrics.get(\"altitude_range_m\", {}),\n        \"plane_intersection\": deterministic_metrics.get(\"plane_intersection\", {}),\n        \"monte_carlo\": monte_carlo_metrics,\n    }\n\n\ndef _default_settings(scenario: Mapping[str, object]) -> PropagatorSettings:\n    \"\"\"Return baseline propagation settings derived from the scenario.\"\"\"\n\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    window_start, window_end, window_midpoint = scenario_access_window(scenario)\n    orbital_elements = scenario.get(\"orbital_elements\", {})\n    reference_start = window_start or window_midpoint\n    if reference_start is None:\n        reference_start = _parse_time(\n            orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n        )\n    if reference_start is None:\n        reference_start = datetime(2026, 3, 21, 0, 0, 0, tzinfo=timezone.utc)\n\n    epoch_time = _parse_time(\n        orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n    ) or reference_start\n\n    planning = timing.get(\"planning_horizon\", {}) if isinstance(timing.get(\"planning_horizon\"), Mapping) else {}\n    planning_stop = _parse_time(planning.get(\"stop_utc\"))\n\n    propagation = timing.get(\"propagation\") if isinstance(timing.get(\"propagation\"), Mapping) else {}\n    margin_s = 300.0\n    try:\n        margin_candidate = float(propagation.get(\"margin_s\", margin_s))\n        margin_s = max(margin_candidate, 120.0)\n    except (TypeError, ValueError):\n        margin_s = max(margin_s, 120.0)\n    margin = timedelta(seconds=margin_s)\n\n    start_override = _parse_time(propagation.get(\"start_utc\")) if propagation else None\n    end_override = _parse_time(propagation.get(\"end_utc\")) if propagation else None\n    time_step = 10.0\n    try:\n        time_step = float(propagation.get(\"time_step_s\", time_step))\n    except (TypeError, ValueError):\n        time_step = 10.0\n\n    analysis_start = start_override or (reference_start - margin)\n    if window_start and analysis_start > window_start - margin:\n        analysis_start = window_start - margin\n\n    candidate_stop = end_override\n    if candidate_stop is None:\n        if window_end:\n            candidate_stop = window_end + margin\n        elif window_midpoint:\n            candidate_stop = window_midpoint + margin\n        else:\n            candidate_stop = analysis_start + timedelta(minutes=10)\n\n    if planning_stop:\n        candidate_stop = min(candidate_stop, planning_stop)\n\n    if candidate_stop <= analysis_start:\n        candidate_stop = analysis_start + timedelta(minutes=10)\n\n    evaluation_time = window_midpoint or reference_start\n\n    primary_limit, waiver_limit, plane_limit = scenario_cross_track_limits(scenario)\n\n    return PropagatorSettings(\n        start_time=analysis_start,\n        epoch_time=epoch_time,\n        stop_time=candidate_stop,\n        time_step_s=time_step,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit,\n        waiver_cross_track_limit_km=waiver_limit,\n        plane_intersection_limit_km=plane_limit,\n    )\n\n\ndef _normalise_monte_carlo(monte_carlo: Mapping[str, object] | None) -> MutableMapping[str, object]:\n    \"\"\"Return a mutable Monte Carlo configuration with defaults applied.\"\"\"\n\n    if monte_carlo is None:\n        config = dict(DEFAULT_MONTE_CARLO)\n    else:\n        config = dict(DEFAULT_MONTE_CARLO)\n        config.update({key: monte_carlo.get(key, value) for key, value in DEFAULT_MONTE_CARLO.items()})\n        for key, value in monte_carlo.items():\n            if key == \"dispersions\" and isinstance(value, Mapping):\n                default_disp = dict(DEFAULT_MONTE_CARLO.get(\"dispersions\", {}))\n                default_disp.update({k: float(v) for k, v in value.items()})\n                config[\"dispersions\"] = default_disp\n            else:\n                config[key] = value\n\n    dispersions = config.get(\"dispersions\", {})\n    if not isinstance(dispersions, Mapping):\n        dispersions = DEFAULT_MONTE_CARLO[\"dispersions\"]\n    config[\"dispersions\"] = dict(dispersions)\n\n    try:\n        runs = int(config.get(\"runs\", DEFAULT_MONTE_CARLO[\"runs\"]))\n    except (TypeError, ValueError):\n        runs = DEFAULT_MONTE_CARLO[\"runs\"]\n    config[\"runs\"] = max(runs, 500)\n\n    return config\n\n\ndef _resolve_evaluation_time(settings: PropagatorSettings) -> datetime:\n    \"\"\"Return a clamped evaluation time within the propagation span.\"\"\"\n\n    evaluation = settings.evaluation_time or settings.start_time\n    if evaluation < settings.start_time:\n        evaluation = settings.start_time\n    if evaluation > settings.stop_time:\n        evaluation = settings.stop_time\n    return evaluation\n\n\ndef _scenario_orbital_elements(scenario: Mapping[str, object]) -> OrbitalElements:\n    \"\"\"Extract classical elements from the scenario mapping.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, Mapping):\n        fallback = _fallback_orbital_elements(scenario)\n        if fallback is None:\n            raise ValueError(\"Scenario is missing the 'orbital_elements' section.\")\n        return fallback\n\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, Mapping):\n        raise ValueError(\"Scenario orbital elements must include the 'classical' block.\")\n\n    semi_major_axis_m = float(classical.get(\"semi_major_axis_km\", 0.0)) * 1_000.0\n    eccentricity = float(classical.get(\"eccentricity\", 0.0))\n    inclination = math.radians(float(classical.get(\"inclination_deg\", 0.0)))\n    raan = math.radians(float(classical.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(classical.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(classical.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis_m,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _fallback_orbital_elements(\n    scenario: Mapping[str, object]\n) -> OrbitalElements | None:\n    spacecraft = scenario.get(\"spacecraft\")\n    if not isinstance(spacecraft, Sequence) or not spacecraft:\n        return None\n    leader = spacecraft[0]\n    if not isinstance(leader, Mapping):\n        return None\n    initial_state = leader.get(\"initial_state\")\n    if not isinstance(initial_state, Mapping):\n        return None\n\n    semi_major_axis = float(initial_state.get(\"semi_major_axis_m\", 0.0))\n    if semi_major_axis <= 0.0:\n        return None\n    eccentricity = float(initial_state.get(\"eccentricity\", 0.0))\n    inclination = float(initial_state.get(\"inclination_rad\", 0.0))\n    raan = math.radians(float(initial_state.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(initial_state.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(initial_state.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _target_coordinates(scenario: Mapping[str, object]) -> tuple[float, float]:\n    \"\"\"Return the target latitude and longitude in radians.\"\"\"\n\n    metadata = scenario.get(\"metadata\")\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    if isinstance(region, Mapping):\n        latitude = math.radians(float(region.get(\"latitude_deg\", 0.0)))\n        longitude = math.radians(float(region.get(\"longitude_deg\", 0.0)))\n        return latitude, longitude\n\n    return math.radians(35.6892), math.radians(51.3890)\n\n\ndef _initial_states(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    drag_coefficient: float,\n    *,\n    epoch_offset_s: float = 0.0,\n) -> list[SpacecraftState]:\n    \"\"\"Construct initial Cartesian states for the formation.\"\"\"\n\n    elements_at_start = leader_elements\n    if epoch_offset_s:\n        mean_motion = leader_elements.mean_motion()\n        propagated_mean_anomaly = _wrap_angle(\n            leader_elements.mean_anomaly + mean_motion * epoch_offset_s\n        )\n        elements_at_start = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis,\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination,\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=propagated_mean_anomaly,\n        )\n\n    position, velocity = _classical_to_cartesian(elements_at_start)\n    rotation = rotation_matrix_rtn_to_eci(position, velocity)\n    states: list[SpacecraftState] = []\n\n    for entry in formation:\n        identifier = str(entry.get(\"identifier\", \"spacecraft\"))\n        radial_offset = float(entry.get(\"radial_offset_km\", 0.0)) * 1_000.0\n        along_track_offset = float(entry.get(\"along_track_offset_km\", 0.0)) * 1_000.0\n        cross_track_offset = float(entry.get(\"cross_track_offset_km\", 0.0)) * 1_000.0\n\n        offset_rtn = np.array([radial_offset, along_track_offset, cross_track_offset], dtype=float)\n        position_shift = rotation @ offset_rtn\n\n        adjusted_position = position + position_shift\n        adjusted_velocity = velocity.copy()\n\n        states.append(\n            SpacecraftState(\n                identifier=identifier,\n                position_m=adjusted_position.astype(float),\n                velocity_mps=adjusted_velocity.astype(float),\n                drag_coefficient=float(entry.get(\"drag_coefficient\", drag_coefficient)),\n                area_m2=4.0,\n                mass_kg=120.0,\n            )\n        )\n\n    return states\n\n\ndef _classical_to_cartesian(elements: OrbitalElements) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return Cartesian state vectors for *elements* without SciPy dependency.\"\"\"\n\n    # This inline implementation mirrors :func:`constellation.orbit.classical_to_cartesian`\n    # but operates fully in metres to avoid repeated conversions.\n    a = elements.semi_major_axis\n    e = elements.eccentricity\n    i = elements.inclination\n    raan = elements.raan\n    argp = elements.arg_perigee\n    m_anomaly = elements.mean_anomaly\n\n    true_anomaly = _mean_to_true(m_anomaly, e)\n    semi_latus = a * (1.0 - e * e)\n    radius = semi_latus / (1.0 + e * math.cos(true_anomaly))\n\n    position_pf = np.array(\n        [radius * math.cos(true_anomaly), radius * math.sin(true_anomaly), 0.0],\n        dtype=float,\n    )\n    velocity_pf = np.array(\n        [\n            -math.sqrt(MU_EARTH / semi_latus) * math.sin(true_anomaly),\n            math.sqrt(MU_EARTH / semi_latus) * (e + math.cos(true_anomaly)),\n            0.0,\n        ],\n        dtype=float,\n    )\n\n    cos_raan = math.cos(raan)\n    sin_raan = math.sin(raan)\n    cos_argp = math.cos(argp)\n    sin_argp = math.sin(argp)\n    cos_i = math.cos(i)\n    sin_i = math.sin(i)\n\n    rotation = np.array(\n        [\n            [\n                cos_raan * cos_argp - sin_raan * sin_argp * cos_i,\n                -cos_raan * sin_argp - sin_raan * cos_argp * cos_i,\n                sin_raan * sin_i,\n            ],\n            [\n                sin_raan * cos_argp + cos_raan * sin_argp * cos_i,\n                -sin_raan * sin_argp + cos_raan * cos_argp * cos_i,\n                -cos_raan * sin_i,\n            ],\n            [sin_argp * sin_i, cos_argp * sin_i, cos_i],\n        ],\n        dtype=float,\n    )\n\n    position = rotation @ position_pf\n    velocity = rotation @ velocity_pf\n    return position, velocity\n\n\ndef _mean_to_true(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert mean anomaly to true anomaly using Newton iterations.\"\"\"\n\n    if eccentricity < 1.0e-12:\n        return _wrap_angle(mean_anomaly)\n\n    eccentric = mean_anomaly\n    for _ in range(30):\n        residual = eccentric - eccentricity * math.sin(eccentric) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric)\n        step = -residual / derivative\n        eccentric += step\n        if abs(step) < 1.0e-12:\n            break\n\n    sine_half = math.sqrt(1.0 + eccentricity) * math.sin(0.5 * eccentric)\n    cosine_half = math.sqrt(1.0 - eccentricity) * math.cos(0.5 * eccentric)\n    return _wrap_angle(2.0 * math.atan2(sine_half, cosine_half))\n\n\ndef _wrap_angle(angle: float) -> float:\n    \"\"\"Normalise *angle* to the ``[0, 2π)`` interval.\"\"\"\n\n    wrapped = math.fmod(angle, 2.0 * math.pi)\n    return wrapped + 2.0 * math.pi if wrapped < 0.0 else wrapped\n\n\ndef _propagate_deterministic(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n) -> tuple[\n    MutableMapping[str, list[tuple[datetime, dict[str, float]]]],\n    MutableMapping[str, object],\n]:\n    \"\"\"Propagate the deterministic trajectory and capture time histories.\"\"\"\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n    states = _initial_states(\n        leader_elements,\n        formation,\n        settings.drag_coefficient,\n        epoch_offset_s=epoch_offset,\n    )\n    epoch = settings.start_time\n    dt = settings.time_step_s\n\n    times: list[datetime] = []\n    cross_track_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    altitude_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    orbital_elements_series: dict[str, dict[str, list[float]]] = {\n        state.identifier: {field: [] for field in CLASSICAL_ELEMENT_FIELDS} for state in states\n    }\n    metrics = _initial_metric_structure(states, target_lat, target_lon)\n    plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n    relative_stats = {\"max_abs\": 0.0, \"min_abs\": math.inf, \"time_of_min\": None, \"series\": []}\n\n    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n        times.append(epoch)\n        for state in states:\n            elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n            per_satellite = orbital_elements_series[state.identifier]\n            per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n            per_satellite[\"eccentricity\"].append(elements.eccentricity)\n            per_satellite[\"inclination_deg\"].append(\n                _normalise_angle_degrees(elements.inclination)\n            )\n            per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n            per_satellite[\"argument_of_perigee_deg\"].append(\n                _normalise_angle_degrees(elements.arg_perigee)\n            )\n            per_satellite[\"mean_anomaly_deg\"].append(\n                _normalise_angle_degrees(elements.mean_anomaly)\n            )\n        _record_metrics(\n            states,\n            epoch,\n            target_lat,\n            target_lon,\n            metrics,\n            cross_track_series,\n            altitude_series,\n            relative_stats,\n        )\n\n        if epoch >= settings.stop_time:\n            break\n\n        states = _rk4_step(states, dt, settings)\n        epoch += timedelta(seconds=dt)\n\n    overall_min_abs = 0.0\n    for entry in metrics[\"vehicles\"]:\n        min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n        if math.isfinite(min_abs):\n            overall_min_abs = max(overall_min_abs, min_abs)\n    metrics[\"overall_min_abs_cross_track_km\"] = float(overall_min_abs)\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    evaluation_timestamp = evaluation_time\n    before_index = after_index = 0\n    fraction = 0.0\n    if times:\n        before_index = max(\n            (idx for idx, stamp in enumerate(times) if stamp <= evaluation_time),\n            default=0,\n        )\n        after_index = min(\n            (idx for idx, stamp in enumerate(times) if stamp >= evaluation_time),\n            default=len(times) - 1,\n        )\n        if after_index < before_index:\n            after_index = before_index\n        before_time = times[before_index]\n        after_time = times[after_index]\n        if after_time > before_time:\n            elapsed = (evaluation_time - before_time).total_seconds()\n            span = (after_time - before_time).total_seconds()\n            fraction = max(0.0, min(elapsed / span, 1.0))\n        else:\n            fraction = 0.0\n\n    evaluation_values: dict[str, float] = {}\n    evaluation_abs: dict[str, float] = {}\n    for identifier, series_values in cross_track_series.items():\n        value_before = float(\"nan\")\n        value_after = float(\"nan\")\n        if series_values:\n            clamped_before = min(before_index, len(series_values) - 1)\n            clamped_after = min(after_index, len(series_values) - 1)\n            value_before = float(series_values[clamped_before])\n            value_after = float(series_values[clamped_after])\n            if math.isfinite(value_before) and math.isfinite(value_after):\n                value = value_before + fraction * (value_after - value_before)\n            elif math.isfinite(value_before):\n                value = value_before\n            elif math.isfinite(value_after):\n                value = value_after\n            else:\n                value = float(\"nan\")\n        else:\n            value = float(\"nan\")\n        evaluation_values[identifier] = value\n        evaluation_abs[identifier] = abs(value) if math.isfinite(value) else math.inf\n\n    centroid_value = fmean(evaluation_values.values()) if evaluation_values else math.nan\n    centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n    worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in evaluation_abs.values() if math.isfinite(value)\n    ]\n    waiver_pass_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    primary_pass_fraction = 1.0 if primary_compliant else 0.0\n\n    for entry in metrics[\"vehicles\"]:\n        identifier = str(entry.get(\"identifier\", \"\"))\n        value = evaluation_values.get(identifier, float(\"nan\"))\n        abs_value = evaluation_abs.get(identifier, math.inf)\n        entry[\"cross_track_at_evaluation_km\"] = (\n            float(value) if math.isfinite(value) else None\n        )\n        entry[\"abs_cross_track_at_evaluation_km\"] = (\n            float(abs_value) if math.isfinite(abs_value) else None\n        )\n        entry[\"compliant\"] = bool(\n            math.isfinite(abs_value) and abs_value <= waiver_limit\n        )\n\n    evaluation_timestamp_iso = evaluation_timestamp.isoformat().replace(\"+00:00\", \"Z\")\n    centroid_value_out = float(centroid_value) if math.isfinite(centroid_value) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    metrics[\"evaluation\"] = {\n        \"time_utc\": evaluation_timestamp_iso,\n        \"vehicles\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_values.items()\n        },\n        \"vehicle_abs\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_abs.items()\n        },\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": float(primary_limit),\n        \"waiver_limit_km\": float(waiver_limit),\n        \"primary_pass_fraction\": float(primary_pass_fraction),\n        \"waiver_pass_fraction\": float(waiver_pass_fraction),\n    }\n    metrics[\"centroid_cross_track_km_at_evaluation\"] = centroid_value_out\n    metrics[\"centroid_abs_cross_track_km_at_evaluation\"] = centroid_abs_out\n    metrics[\"overall_abs_cross_track_km_at_evaluation\"] = worst_abs_out\n    metrics[\"primary_compliant\"] = bool(primary_compliant)\n    metrics[\"waiver_compliant\"] = bool(waiver_compliant)\n    metrics[\"primary_limit_km\"] = float(primary_limit)\n    metrics[\"waiver_limit_km\"] = float(waiver_limit)\n    metrics[\"primary_pass_fraction\"] = float(primary_pass_fraction)\n    metrics[\"waiver_pass_fraction\"] = float(waiver_pass_fraction)\n\n    relative_summary = {\n        \"max_abs_km\": float(relative_stats[\"max_abs\"]),\n        \"min_abs_km\": float(relative_stats[\"min_abs\"] if math.isfinite(relative_stats[\"min_abs\"]) else 0.0),\n        \"time_of_min\": relative_stats[\"time_of_min\"],\n        \"series\": list(relative_stats[\"series\"]),\n    }\n\n    altitude_min = min(min(series) for series in altitude_series.values()) if altitude_series else 0.0\n    altitude_max = max(max(series) for series in altitude_series.values()) if altitude_series else 0.0\n\n    deterministic_metrics: MutableMapping[str, object] = {\n        \"orbital_period_s\": float(orbital_period),\n        \"two_body_period_s\": float(orbital_period),\n        \"cross_track\": metrics,\n        \"altitude_range_m\": {\n            \"min\": float(altitude_min),\n            \"max\": float(altitude_max),\n        },\n    }\n    centroid_summary = metrics.get(\"centroid\", {})\n    if isinstance(centroid_summary, Mapping):\n        deterministic_metrics[\"centroid_cross_track\"] = {\n            \"min_cross_track_km\": float(centroid_summary.get(\"min_cross_track_km\", math.inf)),\n            \"min_abs_cross_track_km\": float(\n                centroid_summary.get(\"min_abs_cross_track_km\", math.inf)\n            ),\n            \"time_of_min_abs_cross_track\": centroid_summary.get(\n                \"time_of_min_abs_cross_track\"\n            ),\n            \"vehicle_cross_track_km\": centroid_summary.get(\n                \"vehicle_cross_track_km_at_min\", {}\n            ),\n            \"vehicle_abs_cross_track_km\": centroid_summary.get(\n                \"vehicle_abs_cross_track_km_at_min\", {}\n            ),\n            \"worst_vehicle_abs_cross_track_km\": float(\n                centroid_summary.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n            ),\n            \"evaluation_cross_track_km\": centroid_value_out,\n            \"evaluation_abs_cross_track_km\": centroid_abs_out,\n            \"worst_vehicle_abs_cross_track_km_at_evaluation\": worst_abs_out,\n            \"evaluation_time_utc\": evaluation_timestamp_iso,\n            \"primary_limit_km\": float(primary_limit),\n            \"waiver_limit_km\": float(waiver_limit),\n            \"primary_compliant\": bool(primary_compliant),\n            \"waiver_compliant\": bool(waiver_compliant),\n        }\n    plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n    plane_limit = settings.plane_intersection_limit_km\n    if plane_limit is not None:\n        plane_metrics[\"limit_km\"] = float(plane_limit)\n        if math.isfinite(plane_distance):\n            plane_metrics[\"compliant\"] = bool(plane_distance <= plane_limit)\n        else:\n            plane_metrics[\"compliant\"] = False\n    else:\n        plane_metrics[\"compliant\"] = None\n    deterministic_metrics[\"plane_intersection\"] = plane_metrics\n    deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    deterministic_metrics[\"cross_track_limits_km\"] = {\n        \"primary\": float(primary_limit),\n        \"waiver\": float(waiver_limit),\n    }\n    deterministic_metrics[\"cross_track_evaluation\"] = metrics[\"evaluation\"]\n    deterministic_metrics[\"orbital_elements\"] = {\n        \"time_series\": {\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n            \"artefact_key\": \"deterministic_orbital_elements_csv\",\n        }\n    }\n\n    series = {\n        \"samples\": [\n            (\n                timestamp,\n                {identifier: cross_track_series[identifier][index] for identifier in cross_track_series},\n            )\n            for index, timestamp in enumerate(times)\n        ],\n        \"altitudes\": {\n            identifier: altitude_series[identifier] for identifier in altitude_series\n        },\n        \"orbital_elements\": {\n            \"times\": list(times),\n            \"per_satellite\": {\n                identifier: {\n                    field: list(values)\n                    for field, values in per_satellite.items()\n                }\n                for identifier, per_satellite in orbital_elements_series.items()\n            },\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n        },\n    }\n\n    return series, deterministic_metrics\n\n\ndef _initial_metric_structure(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Construct the dictionary used to record cross-track statistics.\"\"\"\n\n    vehicle_metrics: list[MutableMapping[str, object]] = []\n    for state in states:\n        vehicle_metrics.append(\n            {\n                \"identifier\": state.identifier,\n                \"target_latitude_deg\": math.degrees(target_lat),\n                \"target_longitude_deg\": math.degrees(target_lon),\n                \"max_cross_track_km\": -math.inf,\n                \"min_cross_track_km\": math.inf,\n                \"max_abs_cross_track_km\": -math.inf,\n                \"min_abs_cross_track_km\": math.inf,\n                \"time_of_max_abs_cross_track\": None,\n                \"time_of_min_abs_cross_track\": None,\n                \"pass_count\": 0,\n                \"compliant\": False,\n            }\n        )\n\n    return {\n        \"vehicles\": vehicle_metrics,\n        \"overall_max_abs_cross_track_km\": -math.inf,\n        \"overall_min_abs_cross_track_km\": math.inf,\n        \"centroid\": {\n            \"min_cross_track_km\": math.inf,\n            \"min_abs_cross_track_km\": math.inf,\n            \"time_of_min_abs_cross_track\": None,\n            \"vehicle_cross_track_km_at_min\": {},\n            \"vehicle_abs_cross_track_km_at_min\": {},\n            \"worst_vehicle_abs_cross_track_km\": math.inf,\n        },\n    }\n\n\ndef _normalise_angle_degrees(angle_rad: float) -> float:\n    \"\"\"Convert radians to wrapped degrees in the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _plane_intersection_metrics(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Compute the plane intersection proximity to the target.\"\"\"\n\n    plane_normals: dict[str, list[np.ndarray]] = {}\n    for state in states:\n        plane = PLANE_ASSIGNMENTS.get(state.identifier, \"Plane A\")\n        h_vec = np.cross(state.position_m, state.velocity_mps)\n        norm = np.linalg.norm(h_vec)\n        if norm == 0.0:\n            continue\n        plane_normals.setdefault(plane, []).append(h_vec / norm)\n\n    averaged_normals: dict[str, np.ndarray] = {}\n    for plane, vectors in plane_normals.items():\n        if not vectors:\n            continue\n        stacked = np.vstack(vectors)\n        mean_vec = np.mean(stacked, axis=0)\n        norm = np.linalg.norm(mean_vec)\n        if norm == 0.0:\n            continue\n        averaged_normals[plane] = mean_vec / norm\n\n    plane_a = averaged_normals.get(\"Plane A\")\n    plane_b = averaged_normals.get(\"Plane B\")\n    if plane_a is None or plane_b is None:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    line_direction = np.cross(plane_a, plane_b)\n    norm_line = np.linalg.norm(line_direction)\n    if norm_line == 0.0:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    unit_direction = line_direction / norm_line\n    candidates = [unit_direction, -unit_direction]\n    candidate_metrics: list[MutableMapping[str, float]] = []\n    min_distance = math.inf\n\n    for candidate in candidates:\n        surface_point = candidate * EARTH_EQUATORIAL_RADIUS_M\n        lat, lon, _ = geodetic_coordinates(surface_point)\n        distance = haversine_distance(lat, lon, target_lat, target_lon)\n        min_distance = min(min_distance, distance)\n        candidate_metrics.append(\n            {\n                \"latitude_deg\": math.degrees(lat),\n                \"longitude_deg\": math.degrees(lon),\n                \"distance_to_target_km\": distance / 1_000.0,\n            }\n        )\n\n    return {\n        \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n        \"intersection_candidates\": candidate_metrics,\n        \"target_distance_km\": min_distance / 1_000.0,\n    }\n\n\ndef _record_metrics(\n    states: Sequence[SpacecraftState],\n    epoch: datetime,\n    target_lat: float,\n    target_lon: float,\n    metrics: MutableMapping[str, object],\n    cross_track_series: MutableMapping[str, list[float]],\n    altitude_series: MutableMapping[str, list[float]],\n    relative_stats: MutableMapping[str, object],\n) -> None:\n    \"\"\"Update cross-track statistics for the current epoch.\"\"\"\n\n    vehicle_entries = metrics[\"vehicles\"]\n    centroid_metrics = metrics.get(\"centroid\", {})\n    epoch_values: dict[str, float] = {}\n\n    for index, state in enumerate(states):\n        position_ecef = inertial_to_ecef(state.position_m, epoch)\n        latitude, longitude, altitude = geodetic_coordinates(position_ecef)\n        cross_track_distance_m = haversine_distance(latitude, longitude, target_lat, target_lon)\n        sign = 1.0 if latitude >= target_lat else -1.0\n        cross_track_km = sign * (cross_track_distance_m / 1_000.0)\n\n        cross_track_series[state.identifier].append(float(cross_track_km))\n        altitude_series[state.identifier].append(float(altitude))\n        epoch_values[state.identifier] = float(cross_track_km)\n\n        entry = vehicle_entries[index]\n        entry[\"max_cross_track_km\"] = max(entry[\"max_cross_track_km\"], cross_track_km)\n        entry[\"min_cross_track_km\"] = min(entry[\"min_cross_track_km\"], cross_track_km)\n\n        abs_value = abs(cross_track_km)\n        if abs_value > entry[\"max_abs_cross_track_km\"]:\n            entry[\"max_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_max_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        if abs_value < entry[\"min_abs_cross_track_km\"]:\n            entry[\"min_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_min_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n\n        previous_series = cross_track_series[state.identifier]\n        if len(previous_series) >= 2 and previous_series[-2] == 0.0:\n            entry[\"pass_count\"] += 1\n        elif len(previous_series) >= 2 and previous_series[-2] * cross_track_km < 0.0:\n            entry[\"pass_count\"] += 1\n\n        metrics[\"overall_max_abs_cross_track_km\"] = max(\n            metrics[\"overall_max_abs_cross_track_km\"], abs_value\n        )\n        metrics[\"overall_min_abs_cross_track_km\"] = min(\n            metrics[\"overall_min_abs_cross_track_km\"], abs_value\n        )\n\n    if epoch_values and isinstance(centroid_metrics, MutableMapping):\n        centroid_value = fmean(epoch_values.values())\n        abs_centroid = abs(centroid_value)\n        if abs_centroid < centroid_metrics.get(\"min_abs_cross_track_km\", math.inf):\n            centroid_metrics[\"min_cross_track_km\"] = float(centroid_value)\n            centroid_metrics[\"min_abs_cross_track_km\"] = float(abs_centroid)\n            centroid_metrics[\"time_of_min_abs_cross_track\"] = (\n                epoch.isoformat().replace(\"+00:00\", \"Z\")\n            )\n            centroid_metrics[\"vehicle_cross_track_km_at_min\"] = {\n                identifier: float(value) for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"vehicle_abs_cross_track_km_at_min\"] = {\n                identifier: float(abs(value))\n                for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"worst_vehicle_abs_cross_track_km\"] = float(\n                max(abs(value) for value in epoch_values.values())\n            )\n\n    leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n    plane_b_state = next(\n        (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n        None,\n    )\n    if leader_state is not None and plane_b_state is not None:\n        normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n        norm_normal = np.linalg.norm(normal_vec)\n        if norm_normal > 0.0:\n            normal_unit = normal_vec / norm_normal\n            relative_vec = plane_b_state.position_m - leader_state.position_m\n            cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n            abs_val = abs(cross_track_km)\n            if abs_val > relative_stats[\"max_abs\"]:\n                relative_stats[\"max_abs\"] = abs_val\n            if abs_val < relative_stats[\"min_abs\"]:\n                relative_stats[\"min_abs\"] = abs_val\n                relative_stats[\"time_of_min\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n            relative_stats[\"series\"].append(\n                (epoch.isoformat().replace(\"+00:00\", \"Z\"), cross_track_km)\n            )\n\n\ndef _rk4_step(states: Sequence[SpacecraftState], dt: float, settings: PropagatorSettings) -> list[SpacecraftState]:\n    \"\"\"Advance the spacecraft states by *dt* seconds using RK4 integration.\"\"\"\n\n    next_states: list[SpacecraftState] = []\n    for state in states:\n        x0 = state.position_m\n        v0 = state.velocity_mps\n\n        a1 = _acceleration(x0, v0, state, settings)\n        k1_pos = v0\n        k1_vel = a1\n\n        a2 = _acceleration(x0 + 0.5 * dt * k1_pos, v0 + 0.5 * dt * k1_vel, state, settings)\n        k2_pos = v0 + 0.5 * dt * k1_vel\n        k2_vel = a2\n\n        a3 = _acceleration(x0 + 0.5 * dt * k2_pos, v0 + 0.5 * dt * k2_vel, state, settings)\n        k3_pos = v0 + 0.5 * dt * k2_vel\n        k3_vel = a3\n\n        a4 = _acceleration(x0 + dt * k3_pos, v0 + dt * k3_vel, state, settings)\n        k4_pos = v0 + dt * k3_vel\n        k4_vel = a4\n\n        new_position = x0 + (dt / 6.0) * (k1_pos + 2.0 * k2_pos + 2.0 * k3_pos + k4_pos)\n        new_velocity = v0 + (dt / 6.0) * (k1_vel + 2.0 * k2_vel + 2.0 * k3_vel + k4_vel)\n\n        state.position_m = new_position\n        state.velocity_mps = new_velocity\n        next_states.append(state)\n\n    return next_states\n\n\ndef _acceleration(\n    position: np.ndarray,\n    velocity: np.ndarray,\n    state: SpacecraftState,\n    settings: PropagatorSettings,\n) -> np.ndarray:\n    r\"\"\"Return acceleration including central gravity, \\(J_2\\), and drag.\"\"\"\n\n    r_vec = position\n    v_vec = velocity\n    r_norm = np.linalg.norm(r_vec)\n\n    if r_norm == 0.0:\n        raise ValueError(\"State vector has zero magnitude; cannot propagate.\")\n\n    mu = MU_EARTH\n    central = -mu * r_vec / (r_norm**3)\n\n    z2 = r_vec[2] ** 2\n    r2 = r_norm**2\n    factor = 1.5 * J2_COEFFICIENT * mu * (EARTH_EQUATORIAL_RADIUS_M**2) / (r_norm**5)\n    coef_xy = factor * (5.0 * z2 / r2 - 1.0)\n    coef_z = factor * (5.0 * z2 / r2 - 3.0)\n    accel_j2 = np.array([coef_xy * r_vec[0], coef_xy * r_vec[1], coef_z * r_vec[2]], dtype=float)\n\n    v_rel = v_vec - np.cross(OMEGA_EARTH_VECTOR, r_vec)\n    speed_rel = np.linalg.norm(v_rel)\n    altitude = r_norm - EARTH_EQUATORIAL_RADIUS_M\n    density = _atmospheric_density(altitude, settings.solar_flux_index)\n    drag = np.zeros(3, dtype=float)\n\n    if speed_rel > 0.0 and density > 0.0:\n        ballistic = state.area_m2 / state.mass_kg\n        drag = (\n            -0.5\n            * density\n            * state.drag_coefficient\n            * ballistic\n            * speed_rel\n            * v_rel\n        )\n\n    return central + accel_j2 + drag\n\n\ndef _atmospheric_density(altitude_m: float, solar_index: float) -> float:\n    \"\"\"Return a simple exponential atmospheric density profile.\"\"\"\n\n    base_density = 3.614e-11  # kg/m^3 at 400 km reference\n    scale_height = 55_000.0 * math.sqrt(max(solar_index, 1.0) / SOLAR_FLUX_BASE)\n    density = base_density * math.exp(-(altitude_m - 400_000.0) / scale_height)\n    return max(density, 0.0)\n\n\ndef _run_monte_carlo(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n    monte_carlo: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Execute Monte Carlo dispersions and aggregate cross-track statistics.\"\"\"\n\n    if not monte_carlo.get(\"enabled\", True):\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n\n    runs = int(monte_carlo.get(\"runs\", 0))\n    if runs <= 0:\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n    runs = max(runs, 500)\n\n    dispersions = monte_carlo.get(\"dispersions\", {})\n    sigma_a = float(dispersions.get(\"semi_major_axis_sigma_m\", 0.0))\n    sigma_i = math.radians(float(dispersions.get(\"inclination_sigma_deg\", 0.0)))\n    sigma_cd = float(dispersions.get(\"drag_coefficient_sigma\", 0.0))\n\n    seed = int(monte_carlo.get(\"seed\", 42))\n    rng = np.random.default_rng(seed)\n\n    vehicle_ids = [entry.get(\"identifier\", \"spacecraft\") for entry in formation]\n    run_metrics_max: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_min: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_eval: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    intersection_distances: list[float] = []\n    relative_max_list: list[float] = []\n    relative_min_list: list[float] = []\n    centroid_abs_list: list[float] = []\n    worst_abs_list: list[float] = []\n\n    primary_success_count = 0\n    waiver_success_count = 0\n    relative_success_count = 0\n    plane_success_count = 0\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    plane_limit = settings.plane_intersection_limit_km\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n\n    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):\n        dispersed_elements = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis + rng.normal(0.0, sigma_a),\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination + rng.normal(0.0, sigma_i),\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=leader_elements.mean_anomaly,\n        )\n\n        drag_scale = 1.0 + rng.normal(0.0, sigma_cd)\n        states = _initial_states(\n            dispersed_elements,\n            formation,\n            settings.drag_coefficient * drag_scale,\n            epoch_offset_s=epoch_offset,\n        )\n        plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n        plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n\n        epoch = settings.start_time\n        dt = settings.time_step_s\n        vehicle_metrics = _initial_metric_structure(states, target_lat, target_lon)\n        previous_signs = {state.identifier: 0.0 for state in states}\n        relative_run_stats = {\"max\": 0.0, \"min\": math.inf}\n        evaluation_snapshot: dict[str, float] = {}\n        evaluation_best = math.inf\n\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            current_values: dict[str, float] = {}\n            for idx, state in enumerate(states):\n                position_ecef = inertial_to_ecef(state.position_m, epoch)\n                latitude, longitude, _ = geodetic_coordinates(position_ecef)\n                distance = haversine_distance(latitude, longitude, target_lat, target_lon)\n                sign = 1.0 if latitude >= target_lat else -1.0\n                cross_track_km = sign * (distance / 1_000.0)\n\n                vehicle_entry = vehicle_metrics[\"vehicles\"][idx]\n                vehicle_entry[\"max_cross_track_km\"] = max(\n                    vehicle_entry[\"max_cross_track_km\"], cross_track_km\n                )\n                vehicle_entry[\"min_cross_track_km\"] = min(\n                    vehicle_entry[\"min_cross_track_km\"], cross_track_km\n                )\n                abs_value = abs(cross_track_km)\n                if abs_value > vehicle_entry[\"max_abs_cross_track_km\"]:\n                    vehicle_entry[\"max_abs_cross_track_km\"] = abs_value\n                if abs_value < vehicle_entry[\"min_abs_cross_track_km\"]:\n                    vehicle_entry[\"min_abs_cross_track_km\"] = abs_value\n\n                current_values[str(state.identifier)] = float(cross_track_km)\n\n                identifier = str(state.identifier)\n                previous_sign = previous_signs[identifier]\n                if previous_sign != 0.0 and previous_sign * cross_track_km < 0.0:\n                    vehicle_entry[\"pass_count\"] += 1\n                previous_signs[identifier] = cross_track_km\n\n            diff = abs((epoch - evaluation_time).total_seconds())\n            if diff < evaluation_best:\n                evaluation_best = diff\n                evaluation_snapshot = dict(current_values)\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n\n            leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n            plane_b_state = next(\n                (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n                None,\n            )\n            if leader_state is not None and plane_b_state is not None:\n                normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n                norm_normal = np.linalg.norm(normal_vec)\n                if norm_normal > 0.0:\n                    normal_unit = normal_vec / norm_normal\n                    relative_vec = plane_b_state.position_m - leader_state.position_m\n                    cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n                    abs_val = abs(cross_track_km)\n                    if abs_val > relative_run_stats[\"max\"]:\n                        relative_run_stats[\"max\"] = abs_val\n                    if abs_val < relative_run_stats[\"min\"]:\n                        relative_run_stats[\"min\"] = abs_val\n\n        evaluation_abs = {\n            identifier: abs(value) if math.isfinite(value) else math.inf\n            for identifier, value in evaluation_snapshot.items()\n        }\n        centroid_value = (\n            fmean(evaluation_snapshot.values()) if evaluation_snapshot else math.nan\n        )\n        centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n        worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n        primary_compliant = (\n            math.isfinite(centroid_abs)\n            and centroid_abs <= primary_limit\n            and math.isfinite(worst_abs)\n            and worst_abs <= waiver_limit\n        )\n        waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n\n        if math.isfinite(centroid_abs):\n            centroid_abs_list.append(centroid_abs)\n        if math.isfinite(worst_abs):\n            worst_abs_list.append(worst_abs)\n\n        for entry in vehicle_metrics[\"vehicles\"]:\n            identifier = str(entry[\"identifier\"])\n            max_abs = float(entry[\"max_abs_cross_track_km\"])\n            min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n            value = evaluation_snapshot.get(identifier, float(\"nan\"))\n            abs_value = evaluation_abs.get(identifier, math.inf)\n            entry[\"cross_track_at_evaluation_km\"] = (\n                float(value) if math.isfinite(value) else None\n            )\n            entry[\"abs_cross_track_at_evaluation_km\"] = (\n                float(abs_value) if math.isfinite(abs_value) else None\n            )\n            entry[\"compliant\"] = bool(\n                math.isfinite(abs_value) and abs_value <= waiver_limit\n            )\n            run_metrics_max[identifier].append(max_abs)\n            if math.isfinite(min_abs):\n                run_metrics_min[identifier].append(min_abs)\n            if math.isfinite(abs_value):\n                run_metrics_eval[identifier].append(abs_value)\n\n        plane_success = True\n        if math.isfinite(plane_distance):\n            if plane_limit is not None:\n                intersection_distances.append(plane_distance)\n                plane_success = plane_distance <= plane_limit\n        elif plane_limit is not None:\n            plane_success = False\n\n        if plane_success:\n            plane_success_count += 1\n\n        if math.isfinite(relative_run_stats[\"max\"]):\n            relative_max_list.append(relative_run_stats[\"max\"])\n        if math.isfinite(relative_run_stats[\"min\"]):\n            relative_min_list.append(relative_run_stats[\"min\"])\n\n        if primary_compliant:\n            primary_success_count += 1\n        if waiver_compliant:\n            waiver_success_count += 1\n        if relative_run_stats[\"max\"] <= waiver_limit:\n            relative_success_count += 1\n\n    aggregated: MutableMapping[str, object] = {\n        \"runs\": runs,\n        \"seed\": seed,\n        \"max_abs_cross_track_km\": {},\n        \"min_abs_cross_track_km\": {},\n        \"evaluation_abs_cross_track_km\": {},\n        \"relative_cross_track_km\": {},\n        \"compliance\": {\n            \"primary_fraction\": float(primary_success_count / runs) if runs else 0.0,\n            \"waiver_fraction\": float(waiver_success_count / runs) if runs else 0.0,\n            \"relative_fraction\": float(relative_success_count / runs) if runs else 0.0,\n        },\n        \"fleet_compliance_probability\": float(primary_success_count / runs)\n        if runs\n        else 0.0,\n        \"absolute_cross_track_compliance_probability\": float(\n            waiver_success_count / runs\n        )\n        if runs\n        else 0.0,\n        \"evaluation_time_utc\": evaluation_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"cross_track_limits_km\": {\n            \"primary\": float(primary_limit),\n            \"waiver\": float(waiver_limit),\n        },\n    }\n\n    if plane_limit is not None:\n        aggregated[\"plane_intersection_distance_km\"] = {}\n\n    if plane_limit is not None:\n        aggregated[\"compliance\"][\"plane_fraction\"] = float(\n            plane_success_count / runs\n        ) if runs else 0.0\n\n    for identifier, values in run_metrics_max.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"max_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_min.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"min_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_eval.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"evaluation_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if plane_limit is not None and intersection_distances:\n        array = np.asarray(intersection_distances, dtype=float)\n        aggregated[\"plane_intersection_distance_km\"][\"fleet\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if relative_max_list:\n        array = np.asarray(relative_max_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_max\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n    if relative_min_list:\n        array = np.asarray(relative_min_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_min\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if centroid_abs_list:\n        array = np.asarray(centroid_abs_list, dtype=float)\n        centroid_stats = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n        aggregated[\"centroid_abs_cross_track_km\"] = centroid_stats\n        aggregated[\"centroid_abs_cross_track_km_mean\"] = centroid_stats[\"mean\"]\n        aggregated[\"centroid_abs_cross_track_km_p95\"] = centroid_stats[\"p95\"]\n    if worst_abs_list:\n        array = np.asarray(worst_abs_list, dtype=float)\n        aggregated[\"worst_vehicle_abs_cross_track_km\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    return aggregated\n\n\ndef _write_outputs(\n    output_directory: Path,\n    series: Mapping[str, object],\n    deterministic_metrics: Mapping[str, object],\n    monte_carlo_metrics: Mapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, str]:\n    \"\"\"Persist deterministic and Monte Carlo outputs to the run directory.\"\"\"\n\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    deterministic_path = output_directory / \"deterministic_cross_track.csv\"\n    relative_path = output_directory / \"relative_cross_track.csv\"\n    summary_path = output_directory / \"deterministic_summary.json\"\n    orbital_path = output_directory / \"deterministic_orbital_elements.csv\"\n    monte_carlo_path = output_directory / \"monte_carlo_cross_track.csv\"\n    monte_carlo_summary_path = output_directory / \"monte_carlo_summary.json\"\n    settings_path = output_directory / \"solver_settings.json\"\n\n    _write_cross_track_csv(deterministic_path, series)\n    orbital_series = series.get(\"orbital_elements\") if isinstance(series, Mapping) else {}\n    _write_orbital_elements_csv(orbital_path, orbital_series)\n\n    relative_summary = deterministic_metrics.get(\"relative_cross_track\")\n    if isinstance(relative_summary, Mapping):\n        _write_relative_cross_track_csv(relative_path, relative_summary)\n        relative_summary = dict(relative_summary)\n        relative_summary.pop(\"series\", None)\n        deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    else:\n        _write_relative_cross_track_csv(relative_path, {})\n\n    summary_payload = {\n        \"metrics\": deterministic_metrics,\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": settings.time_step_s,\n            \"drag_coefficient\": settings.drag_coefficient,\n            \"ballistic_coefficient_m2_per_kg\": settings.ballistic_coefficient_m2_per_kg,\n            \"solar_flux_index\": settings.solar_flux_index,\n        },\n    }\n    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding=\"utf-8\")\n\n    _write_monte_carlo_csv(monte_carlo_path, monte_carlo_metrics)\n    monte_carlo_summary_path.write_text(json.dumps(monte_carlo_metrics, indent=2), encoding=\"utf-8\")\n\n    settings_payload = {\n        \"propagator\": {\n            \"integrator\": \"fixed_step_rk4\",\n            \"time_step_s\": settings.time_step_s,\n            \"force_models\": {\n                \"central_body\": True,\n                \"j2\": True,\n                \"atmospheric_drag\": {\n                    \"model\": \"exponential\",\n                    \"solar_flux_index\": settings.solar_flux_index,\n                },\n            },\n        },\n        \"monte_carlo\": {\n            \"runs\": monte_carlo_config.get(\"runs\", 0),\n            \"seed\": monte_carlo_config.get(\"seed\", 0),\n            \"dispersions\": monte_carlo_config.get(\"dispersions\", {}),\n        },\n        \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n    settings_path.write_text(json.dumps(settings_payload, indent=2), encoding=\"utf-8\")\n\n    orbital_metrics = deterministic_metrics.get(\"orbital_elements\")\n    if isinstance(orbital_metrics, Mapping):\n        time_series_meta = orbital_metrics.get(\"time_series\")\n        if isinstance(time_series_meta, Mapping):\n            time_series_meta[\"artefact\"] = str(orbital_path)\n\n    return {\n        \"deterministic_cross_track_csv\": str(deterministic_path),\n        \"deterministic_orbital_elements_csv\": str(orbital_path),\n        \"deterministic_summary_json\": str(summary_path),\n        \"monte_carlo_cross_track_csv\": str(monte_carlo_path),\n        \"relative_cross_track_csv\": str(relative_path),\n        \"monte_carlo_summary_json\": str(monte_carlo_summary_path),\n        \"solver_settings_json\": str(settings_path),\n    }\n\n\ndef _write_cross_track_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Write deterministic cross-track histories to *path*.\"\"\"\n\n    samples = series.get(\"samples\", [])\n    if not samples:\n        path.write_text(\"time,\\n\", encoding=\"utf-8\")\n        return\n\n    vehicle_ids = sorted(next(iter(samples))[1])\n    header = [\"time_iso\"] + vehicle_ids\n    lines = [\",\".join(header)]\n\n    for timestamp, cross_track in samples:\n        row = [timestamp.isoformat().replace(\"+00:00\", \"Z\")]\n        for identifier in vehicle_ids:\n            row.append(f\"{cross_track.get(identifier, 0.0):.6f}\")\n        lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_orbital_elements_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Persist orbital-element histories to *path*.\"\"\"\n\n    header = \"time_iso,satellite_id,\" + \",\".join(CLASSICAL_ELEMENT_FIELDS)\n    if not isinstance(series, Mapping) or not series:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    times = list(series.get(\"times\", []))\n    per_satellite = series.get(\"per_satellite\")\n    if not isinstance(per_satellite, Mapping) or not times:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [header]\n    for index, timestamp in enumerate(times):\n        if isinstance(timestamp, datetime):\n            stamp = timestamp.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        else:\n            stamp = str(timestamp)\n        for identifier in sorted(per_satellite):\n            elements = per_satellite.get(identifier)\n            if not isinstance(elements, Mapping):\n                continue\n            row = [stamp, str(identifier)]\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                value = float(\"nan\")\n                if isinstance(values, Sequence) and index < len(values):\n                    try:\n                        value = float(values[index])  # type: ignore[arg-type]\n                    except (TypeError, ValueError):\n                        value = float(\"nan\")\n                row.append(f\"{value:.9g}\" if math.isfinite(value) else \"\")\n            lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_monte_carlo_csv(path: Path, metrics: Mapping[str, object]) -> None:\n    \"\"\"Write Monte Carlo aggregate metrics to *path*.\"\"\"\n\n    vehicle_stats = metrics.get(\"max_abs_cross_track_km\", {})\n    min_stats_all = metrics.get(\"min_abs_cross_track_km\", {})\n    plane_stats = metrics.get(\"plane_intersection_distance_km\", {})\n    relative_stats = metrics.get(\"relative_cross_track_km\", {})\n    if (\n        not vehicle_stats\n        and not min_stats_all\n        and not plane_stats\n        and not relative_stats\n    ):\n        path.write_text(\n            \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n            \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n            \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n            \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n            \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n            \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\\n\",\n            encoding=\"utf-8\",\n        )\n        return\n\n    header = (\n        \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n        \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n        \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n        \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n        \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n        \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\"\n    )\n    lines = [header]\n\n    identifiers = sorted(\n        set(\n            list(vehicle_stats.keys())\n            + list(min_stats_all.keys())\n            + list(plane_stats.keys())\n            + list(relative_stats.keys())\n        )\n    )\n\n    for identifier in identifiers:\n        stats = vehicle_stats.get(identifier, {})\n        min_stats = min_stats_all.get(identifier, {})\n        plane_entry = plane_stats.get(identifier, {})\n        relative_entry = relative_stats.get(identifier, {})\n        line = \",\".join(\n            [\n                str(identifier),\n                f\"{min_stats.get('mean', 0.0):.6f}\",\n                f\"{min_stats.get('std', 0.0):.6f}\",\n                f\"{min_stats.get('p95', 0.0):.6f}\",\n                f\"{min_stats.get('min', 0.0):.6f}\",\n                f\"{min_stats.get('max', 0.0):.6f}\",\n                f\"{stats.get('mean', 0.0):.6f}\",\n                f\"{stats.get('std', 0.0):.6f}\",\n                f\"{stats.get('p95', 0.0):.6f}\",\n                f\"{stats.get('min', 0.0):.6f}\",\n                f\"{stats.get('max', 0.0):.6f}\",\n                f\"{plane_entry.get('mean', 0.0):.6f}\",\n                f\"{plane_entry.get('std', 0.0):.6f}\",\n                f\"{plane_entry.get('p95', 0.0):.6f}\",\n                f\"{plane_entry.get('min', 0.0):.6f}\",\n                f\"{plane_entry.get('max', 0.0):.6f}\",\n                f\"{relative_entry.get('mean', 0.0):.6f}\",\n                f\"{relative_entry.get('std', 0.0):.6f}\",\n                f\"{relative_entry.get('p95', 0.0):.6f}\",\n                f\"{relative_entry.get('min', 0.0):.6f}\",\n                f\"{relative_entry.get('max', 0.0):.6f}\",\n            ]\n        )\n        lines.append(line)\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_relative_cross_track_csv(path: Path, summary: Mapping[str, object]) -> None:\n    \"\"\"Write relative cross-track series to *path*.\"\"\"\n\n    series = summary.get(\"series\") if isinstance(summary, Mapping) else None\n    if not series:\n        path.write_text(\"time_iso,relative_cross_track_km\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [\"time_iso,relative_cross_track_km\"]\n    for time_iso, value in series:\n        lines.append(f\"{time_iso},{float(value):.6f}\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _parse_time(value: object) -> datetime | None:\n    \"\"\"Parse an ISO-8601 timestamp into an aware :class:`datetime`.\"\"\"\n\n    if isinstance(value, str):\n        try:\n            if value.endswith(\"Z\"):\n                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n            parsed = datetime.fromisoformat(value)\n            if parsed.tzinfo is None:\n                return parsed.replace(tzinfo=timezone.utc)\n            return parsed.astimezone(timezone.utc)\n        except ValueError:\n            return None\n    return None\n\n\n__all__ = [\n    \"PropagatorSettings\",\n    \"propagate_constellation\",\n    \"scenario_cross_track_limits\",\n    \"scenario_access_window\",\n]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "Ci8B4/H/XhA7PfO89ZrR9ICeKXQL6byOoCdE+V2xdnDH6zXflOPaRgx0WulXO0IVUgp2AePx/17Zu2uHq81X6WS6t+6lgnSqf7ItSmXHkIp4vflW0o9e+/eGhnd1PLmDjlhevQWqJFQ36S/KlgC9eqVhdzBTsVH59OkKly/1CNwUKFtzvH7VhoC8k00CD9WJG0wth5iTdcoMCPVu4qMi7a8zq8nklyE09gomAePx/15thop5I3MhUG8pm0J+RTJJugNwDySla/EpoNVMqQ7MPtw=",
        "text": "Okay, adding progress bar to `_propagate_deterministic` in `sim/scripts/perturbation_analysis.py`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "old_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    while epoch <= settings.stop_time + timedelta(seconds=1e-6):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the deterministic propagation loop to show progress.",
            "new_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n\n    n_steps = int((settings.stop_time - settings.start_time).total_seconds() / dt)\n    for _ in tqdm(range(n_steps), desc=\"Deterministic propagation\"):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762792397937-1e35cf4239ef",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CicB4/H/XtSxCTyGJ3eOz9/li9BLnKGoCbVdS97UAsmdKNOi/U+xU6wKbwHj8f9e3skp6/6Oyfif6SIIzFfPOS8oOTkNHB4h4IinYIsWJd9apgRzTTa3xbXSsm5EDXFlIerR6Okvp1kGH3PPzkYVS8AXlh+zIMk+H+tZhI93YJ6kpalFceB8eDZSiVcP4km1JsbGWxs2+AuU2wphAePx/17Ds3/XSxoiLVJ0bzOkn53J5zxlkpJcVCLa6DC4jPpWbWyJhOxbMiiy6iMTAiLfMlt5YHL9PW+TRvDngjteEEvNBbZ6/B5Qvh7bWcUT6jyHHtojSoqnBgna1YbK9gpxAePx/17dLUjsBVwwA5TssPVpXNJsoz9olUDuFsAY+FO9nf/QHoqV9VlAQyZOJ/w7e95BIS7511aVLOHAtzeC4zEIZH8LbLBwi2Kp86RgT8C0CMOudjBSF1RmYVyAtoSzjgjboXF2J99upw88vxhym1c=",
        "text": "Okay, progress bar added to `_propagate_deterministic`. Now adding one to the `_run_monte_carlo` `for` loop."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop to show progress.",
            "new_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "old_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762792419636-83ed728d13aa5",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjYB4/H/XnW+tk53f34AONmtCAFLKaYQcBa9bWlsw8a+i+C5eiZ3QVXhQwcX8tZ0b0pRLxmo/dAKXwHj8f9e6ekwF8jrLbYhKI04sdAw60/OyVZ24qd9E0pUjkMKUaAkl1o14/5BdC2bAdZk0K8w2OUY7c5XuIg9C0idh69ctqQ6qsunoC+i44SmInjMWvjlxd5YrssSa+7JClUB4/H/Xklx7909wBZaGsYABlgc5tXMjDf9lbj9ElBMrcnnCx0QwKt/EKN2SLdn+/PFtt54m8LRmbBZWYBifoCWtKfFhLXuMTv4I4o0/rRf+nqC0xNvCqgBAePx/17nYQ9AMTWmHDbkP1Vv6OhVdrUqaVXiUsMu30REcCmwqryUwH7O972woiZgD5b6g+amwQN1SwN+9sUbrhlV2Li2lzmElyTc+NDFWH8okMGrzdGFfMQhyQP334PQZBXgbcQvFCUm/6FzO7lQUFWlDJ1kbGQjRqRsYMtdDNNQTl2THvAfTIgzeF91E8ot8R3jDz0+DEIkEhQ1Zxa1qMETNe+xzrv4CncB4/H/Xh3pREZDAhMkSjHNafp79phAyHnY9a32IRAbI1nwFgIhqtyrIM/Zvcpj7D3g0U6+r+v3Ahgw3PWGgakg+RUfZdTX/wjsSWPLdxG3NitG4lqqz5umtIFzVKG5WvkdfTwGJLDokCw2iQBxIhd+VRwGYafgCAp9AePx/17HJj1dtO2T8nm+6+eU1vKbs4Ge+tJMLy94bprznul6LWRSotst1Q1JZ5uSLLHmd2okO1dWigMkGdIL2OBpEzJb2neRiUKahXMTTqCt8d3DwHHJ7EXXl312rDzvQ0LNEkyhrJ4HrGOnjENef8bPIYXvsK972TAVCPA=",
        "text": "I've added progress bars to the `run_triangle` and `run_scenario` simulations. Now, I'm rerunning the 14-day simulation with the same command: `.venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day`. If successful, I'll analyze the results and create the markdown file."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "command": ".venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day",
            "description": "Running the 14-day triangle formation simulation again with progress bars."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762792430847-6dac9b7ebbc4e",
          "name": "run_shell_command",
          "response": {
            "error": "[Operation Cancelled] Reason: User cancelled tool execution."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "اصلا 14 روزه رو ران نمیگیره 15 دقیقه طول کشید ولی هیچی،حتی درصد پیشرفت هم نشون نمیده"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "The 14-day simulation is hanging before the progress bar appears, likely due to massive upfront memory allocation for the simulation's ~1.2 million time steps. The program is probably thrashing the swap file.\n\nI will refactor the simulation to process data in chunks instead of pre-allocating huge arrays. This will involve initializing data containers as empty lists, appending results in the main loop, and converting them to NumPy arrays only after the simulation completes. This should significantly reduce the initial memory footprint and allow the simulation to start and show progress.\n\nI'll start by rereading `sim/formation/triangle.py` to begin the refactoring."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/formation/triangle.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762793179768-6b23cee13119b",
          "name": "read_file",
          "response": {
            "output": "\"\"\"Simulation of a three-satellite triangular formation over Tehran.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Iterable, Mapping, MutableMapping, Optional, Sequence\n\nimport numpy as np\nimport pandas as pd\n\nSECONDS_PER_DAY = 86_400.0\nSECONDS_PER_YEAR = SECONDS_PER_DAY * 365.25\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nfrom src.constellation.geometry import (\n    triangle_area,\n    triangle_aspect_ratio,\n    triangle_side_lengths,\n)\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n    propagate_kepler,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\nfrom tools.stk_export import (\n    FacilityDefinition,\n    GroundContactInterval,\n    GroundTrack,\n    GroundTrackPoint,\n    PropagatedStateHistory,\n    ScenarioMetadata,\n    SimulationResults,\n    StateSample,\n    sanitize_stk_identifier,\n    unique_stk_names,\n    export_simulation_to_stk,\n)\n\n\n@dataclass\nclass TriangleFormationResult:\n    \"\"\"Container collecting the outputs of the triangle simulation.\"\"\"\n\n    times: Sequence[datetime]\n    positions_m: Mapping[str, np.ndarray]\n    velocities_mps: Mapping[str, np.ndarray]\n    classical_elements: Mapping[str, Mapping[str, np.ndarray]]\n    latitudes_rad: Mapping[str, np.ndarray]\n    longitudes_rad: Mapping[str, np.ndarray]\n    altitudes_m: Mapping[str, np.ndarray]\n    triangle_area_m2: np.ndarray\n    triangle_aspect_ratio: np.ndarray\n    triangle_sides_m: np.ndarray\n    centroid_lat_rad: np.ndarray\n    centroid_lon_rad: np.ndarray\n    centroid_alt_m: np.ndarray\n    max_ground_distance_km: np.ndarray\n    min_command_distance_km: np.ndarray\n    metrics: Mapping[str, object]\n    artefacts: Mapping[str, Optional[str]]\n\n    def to_summary(self) -> MutableMapping[str, object]:\n        \"\"\"Convert the simulation result into a JSON-serialisable mapping.\"\"\"\n\n        samples: list[MutableMapping[str, object]] = []\n        satellite_ids = sorted(self.positions_m)\n        geometry: MutableMapping[str, object] = {\n            \"times\": [epoch.isoformat().replace(\"+00:00\", \"Z\") for epoch in self.times],\n            \"satellite_ids\": satellite_ids,\n            \"positions_m\": {\n                sat_id: self.positions_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"classical_element_fields\": list(CLASSICAL_ELEMENT_FIELDS),\n            \"classical_element_units\": {\n                \"semi_major_axis_km\": \"km\",\n                \"eccentricity\": \"\",\n                \"inclination_deg\": \"deg\",\n                \"raan_deg\": \"deg\",\n                \"argument_of_perigee_deg\": \"deg\",\n                \"mean_anomaly_deg\": \"deg\",\n            },\n            \"classical_elements\": {\n                sat_id: {\n                    element: self.classical_elements[sat_id][element].tolist()\n                    for element in self.classical_elements[sat_id]\n                }\n                for sat_id in satellite_ids\n            },\n            \"latitudes_rad\": {\n                sat_id: self.latitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"longitudes_rad\": {\n                sat_id: self.longitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"altitudes_m\": {\n                sat_id: self.altitudes_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n        }\n        geometry[\"max_ground_distance_km\"] = [\n            float(value) for value in self.max_ground_distance_km\n        ]\n        geometry[\"min_command_distance_km\"] = [\n            float(value) for value in self.min_command_distance_km\n        ]\n        for index, epoch in enumerate(self.times):\n            samples.append(\n                {\n                    \"time\": epoch.isoformat().replace(\"+00:00\", \"Z\"),\n                    \"triangle_area_m2\": float(self.triangle_area_m2[index]),\n                    \"triangle_aspect_ratio\": float(self.triangle_aspect_ratio[index]),\n                    \"triangle_side_lengths_m\": [\n                        float(val) for val in self.triangle_sides_m[index]\n                    ],\n                    \"centroid\": {\n                        \"latitude_deg\": math.degrees(self.centroid_lat_rad[index]),\n                        \"longitude_deg\": math.degrees(self.centroid_lon_rad[index]),\n                        \"altitude_m\": float(self.centroid_alt_m[index]),\n                    },\n                    \"max_ground_distance_km\": float(self.max_ground_distance_km[index]),\n                }\n            )\n\n        return {\n            \"metrics\": self.metrics,\n            \"samples\": samples,\n            \"artefacts\": dict(self.artefacts),\n            \"geometry\": geometry,\n        }\n\n\ndef simulate_triangle_formation(\n    config_source: Mapping[str, object] | Path | str,\n    output_directory: Optional[Path | str] = None,\n) -> TriangleFormationResult:\n    \"\"\"Simulate the triangular formation described by *config_source*.\"\"\"\n\n    configuration = _load_configuration(config_source)\n    formation = configuration[\"formation\"]\n    metadata = configuration.get(\"metadata\", {})\n\n    satellite_elements: dict[str, OrbitalElements] = {}\n    if \"satellites\" in configuration:\n        satellite_ids = [sat[\"id\"] for sat in configuration[\"satellites\"]]\n        plane_allocations = {\n            sat[\"id\"]: sat[\"plane\"] for sat in configuration[\"satellites\"]\n        }\n        epoch = _parse_time(\n            configuration[\"satellites\"][0][\"orbital_elements\"][\"epoch_utc\"]\n        )\n        semi_major_axis_m = (\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"][\"semi_major_axis_km\"]\n            )\n            * 1_000.0\n        )\n        inclination = math.radians(\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"].get(\n                    \"inclination_deg\", 0.0\n                )\n            )\n        )\n\n        for sat_config in configuration[\"satellites\"]:\n            elements = sat_config[\"orbital_elements\"]\n            satellite_elements[sat_config[\"id\"]] = OrbitalElements(\n                semi_major_axis=float(elements[\"semi_major_axis_km\"]) * 1_000.0,\n                eccentricity=float(elements.get(\"eccentricity\", 0.0)),\n                inclination=math.radians(float(elements.get(\"inclination_deg\", 0.0))),\n                raan=math.radians(float(elements.get(\"raan_deg\", 0.0))),\n                arg_perigee=math.radians(\n                    float(elements.get(\"argument_of_perigee_deg\", 0.0))\n                ),\n                mean_anomaly=math.radians(float(elements.get(\"mean_anomaly_deg\", 0.0))),\n            )\n    else:\n        reference = configuration[\"reference_orbit\"]\n        epoch = _parse_time(reference[\"epoch_utc\"])\n        semi_major_axis_m = float(reference[\"semi_major_axis_km\"]) * 1_000.0\n        eccentricity = float(reference.get(\"eccentricity\", 0.0))\n        inclination = math.radians(float(reference.get(\"inclination_deg\", 0.0)))\n        raan = math.radians(float(reference.get(\"raan_deg\", 0.0)))\n        arg_perigee = math.radians(\n            float(reference.get(\"argument_of_perigee_deg\", 0.0))\n        )\n        mean_anomaly = math.radians(float(reference.get(\"mean_anomaly_deg\", 0.0)))\n\n        reference_elements = OrbitalElements(\n            semi_major_axis=semi_major_axis_m,\n            eccentricity=eccentricity,\n            inclination=inclination,\n            raan=raan,\n            arg_perigee=arg_perigee,\n            mean_anomaly=mean_anomaly,\n        )\n        offsets_m = _formation_offsets(float(formation[\"side_length_m\"]))\n        satellite_ids = tuple(sorted(offsets_m))\n\n        plane_allocations = formation.get(\"plane_allocations\", {})\n        if not plane_allocations:\n            plane_allocations = {\n                satellite_ids[0]: \"Plane A\",\n                satellite_ids[1]: \"Plane A\",\n                satellite_ids[2]: \"Plane B\",\n            }\n\n        # Get initial state of the reference orbit\n        ref_pos_at_epoch, ref_vel_at_epoch = propagate_kepler(reference_elements, 0)\n\n        # Get initial LVLH frame\n        frame_at_epoch = _lvlh_frame(ref_pos_at_epoch, ref_vel_at_epoch)\n\n        # Calculate initial orbital elements for each satellite\n        for sat_id in satellite_ids:\n            offset_vec = frame_at_epoch @ offsets_m[sat_id]\n            # For a stable formation with zero radial offset, the initial relative velocity in the LVLH frame should be zero.\n            offset_vel = np.array([0.0, 0.0, 0.0])\n\n            sat_pos = ref_pos_at_epoch + offset_vec\n            sat_vel = ref_vel_at_epoch + offset_vel\n            satellite_elements[sat_id] = cartesian_to_classical(sat_pos, sat_vel)\n\n    duration_s = float(formation.get(\"duration_s\", 180.0))\n    time_step_s = float(formation.get(\"time_step_s\", 1.0))\n    half_duration = 0.5 * duration_s\n    sample_count = int(round(duration_s / time_step_s)) + 1\n\n    offsets = np.arange(sample_count, dtype=float) * time_step_s - half_duration\n    times = [epoch + timedelta(seconds=float(offset)) for offset in offsets]\n\n    positions: dict[str, np.ndarray] = {\n        sat_id: np.zeros((sample_count, 3), dtype=float) for sat_id in satellite_ids\n    }\n    latitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    longitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    altitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n\n    triangle_area_series = np.zeros(sample_count, dtype=float)\n    triangle_aspect_series = np.zeros(sample_count, dtype=float)\n    triangle_sides_series = np.zeros((sample_count, 3), dtype=float)\n    centroid_positions = np.zeros((sample_count, 3), dtype=float)\n    centroid_latitudes = np.zeros(sample_count, dtype=float)\n    centroid_longitudes = np.zeros(sample_count, dtype=float)\n    centroid_altitudes = np.zeros(sample_count, dtype=float)\n    max_ground_distance = np.zeros(sample_count, dtype=float)\n    min_command_distance = np.full(sample_count, np.inf, dtype=float)\n\n    target = formation.get(\"target\", {})\n    target_lat = math.radians(float(target.get(\"latitude_deg\", 0.0)))\n    target_lon = math.radians(float(target.get(\"longitude_deg\", 0.0)))\n\n    command = formation.get(\"command\", {})\n    command_station = command.get(\"station\", {})\n    default_lat_deg = math.degrees(target_lat)\n    default_lon_deg = math.degrees(target_lon)\n    command_lat = math.radians(float(command_station.get(\"latitude_deg\", default_lat_deg)))\n    command_lon = math.radians(float(command_station.get(\"longitude_deg\", default_lon_deg)))\n\n    # Propagate each satellite independently\n    velocities_temp: dict[str, list[np.ndarray]] = {sat_id: [] for sat_id in satellite_ids}\n\n    perturbations = formation.get(\"perturbations\", {})\n    position_noise_sigma_m = float(perturbations.get(\"position_noise_sigma_m\", 0.0))\n\n    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    from tqdm import tqdm\n    for index, delta_t in enumerate(tqdm(offsets, desc=\"Propagating orbits\")):\n        inertial_positions = {}\n        for sat_id in satellite_ids:\n            pos, vel = propagate_kepler(satellite_elements[sat_id], delta_t)\n\n            # if position_noise_sigma_m > 0.0:\n            #     noise = rng.normal(0.0, position_noise_sigma_m, size=3)\n            #     pos += noise\n\n            positions[sat_id][index] = pos\n            velocities_temp[sat_id].append(vel)\n            inertial_positions[sat_id] = pos\n\n            ecef = inertial_to_ecef(pos, times[index])\n            lat, lon, alt = geodetic_coordinates(ecef)\n            latitudes[sat_id][index] = lat\n            longitudes[sat_id][index] = lon\n            altitudes[sat_id][index] = alt\n\n        # Now, compute triangle geometry based on the actual positions\n        # The order of vertices matters for side length calculations, so sort by sat_id\n        vertices = [inertial_positions[sat_id] for sat_id in satellite_ids]\n\n        triangle_area_series[index] = triangle_area(vertices)\n        triangle_aspect_series[index] = triangle_aspect_ratio(vertices)\n        triangle_sides_series[index] = triangle_side_lengths(vertices)\n\n        centroid = sum(vertices) / len(vertices)\n        centroid_positions[index] = centroid\n        centroid_ecef = inertial_to_ecef(centroid, times[index])\n        centroid_lat, centroid_lon, centroid_alt = geodetic_coordinates(centroid_ecef)\n        centroid_latitudes[index] = centroid_lat\n        centroid_longitudes[index] = centroid_lon\n        centroid_altitudes[index] = centroid_alt\n\n        max_distance = 0.0\n        min_command = np.inf\n        for sat_id in satellite_ids:\n            distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                target_lat,\n                target_lon,\n            )\n            max_distance = max(max_distance, distance)\n            command_distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                command_lat,\n                command_lon,\n            )\n            min_command = min(min_command, command_distance)\n        max_ground_distance[index] = max_distance / 1_000.0\n        min_command_distance[index] = min_command\n\n    velocities: dict[str, np.ndarray] = {\n        sat_id: np.array(velocities_temp[sat_id]) for sat_id in satellite_ids\n    }\n\n    classical_series = _compute_classical_elements_series(positions, velocities)\n\n    finite_mask = np.isfinite(min_command_distance)\n    min_command_distance_km = np.where(\n        finite_mask, min_command_distance / 1_000.0, np.inf\n    )\n\n    centre_index = int(np.argmin(np.abs(offsets)))\n    orbital_elements = {}\n    for sat_id in satellite_ids:\n        elements = cartesian_to_classical(\n            positions[sat_id][centre_index], velocities[sat_id][centre_index]\n        )\n        orbital_elements[sat_id] = {\n            \"semi_major_axis_km\": elements.semi_major_axis / 1_000.0,\n            \"eccentricity\": elements.eccentricity,\n            \"inclination_deg\": _normalise_degrees(elements.inclination),\n            \"raan_deg\": _normalise_degrees(elements.raan),\n            \"argument_of_perigee_deg\": _normalise_degrees(elements.arg_perigee),\n            \"mean_anomaly_deg\": _normalise_degrees(elements.mean_anomaly),\n            \"assigned_plane\": plane_allocations.get(sat_id),\n        }\n\n    triangle_stats = _summarise_triangle_metrics(\n        triangle_area_series,\n        triangle_aspect_series,\n        triangle_sides_series,\n    )\n    ground_stats = _summarise_ground_metrics(max_ground_distance, formation)\n    maintenance = _estimate_maintenance_delta_v(\n        positions,\n        times,\n        formation,\n        semi_major_axis_m,\n    )\n    command_latency = _analyse_command_latency(\n        min_command_distance_km,\n        times,\n        formation,\n        semi_major_axis_m,\n        command_lat,\n        command_lon,\n    )\n    injection_recovery, injection_samples = _run_injection_recovery_monte_carlo(\n        satellite_ids,\n        formation,\n    )\n    drag_dispersion, drag_samples = _run_atmospheric_drag_dispersion_monte_carlo(\n        formation,\n        semi_major_axis_m,\n        inclination,\n    )\n\n    window, window_series = _formation_window(\n        triangle_aspect_series,\n        max_ground_distance,\n        time_step_s,\n        formation,\n        times,\n    )\n\n    recurrence = _summarise_window_recurrence(\n        window_series,\n        semi_major_axis_m,\n    )\n\n    station_keeping = _assess_station_keeping(\n        times,\n        triangle_sides_series,\n        formation,\n        maintenance,\n        time_step_s,\n    )\n\n    metrics: MutableMapping[str, object] = {\n        \"triangle\": triangle_stats,\n        \"ground_track\": ground_stats,\n        \"formation_window\": window,\n        \"formation_windows\": window_series,\n        \"formation_recurrence\": recurrence,\n        \"orbital_elements\": {\n            \"per_satellite\": orbital_elements,\n            \"plane_assignments\": plane_allocations,\n            \"time_series\": {\n                \"fields\": CLASSICAL_ELEMENT_FIELDS,\n                \"units\": {\n                    \"semi_major_axis_km\": \"km\",\n                    \"eccentricity\": \"\",\n                    \"inclination_deg\": \"deg\",\n                    \"raan_deg\": \"deg\",\n                    \"argument_of_perigee_deg\": \"deg\",\n                    \"mean_anomaly_deg\": \"deg\",\n                },\n                \"artefact_key\": \"orbital_elements_csv\",\n                \"per_satellite_directory_key\": \"orbital_elements_directory\",\n            },\n        },\n        \"maintenance\": maintenance,\n        \"station_keeping\": station_keeping,\n        \"command_latency\": command_latency,\n        \"injection_recovery\": injection_recovery,\n        \"drag_dispersion\": drag_dispersion,\n    }\n\n    artefacts: MutableMapping[str, Optional[str]] = {\n        \"summary_path\": None,\n        \"stk_directory\": None,\n        \"maintenance_csv\": None,\n        \"command_windows_csv\": None,\n        \"injection_recovery_csv\": None,\n        \"injection_recovery_plot\": None,\n        \"drag_dispersion_csv\": None,\n        \"formation_windows_csv\": None,\n        \"station_keeping_csv\": None,\n        \"orbital_elements_csv\": None,\n        \"orbital_elements_directory\": None,\n    }\n\n    result = TriangleFormationResult(\n        times=times,\n        positions_m=positions,\n        velocities_mps=velocities,\n        classical_elements=classical_series,\n        latitudes_rad=latitudes,\n        longitudes_rad=longitudes,\n        altitudes_m=altitudes,\n        triangle_area_m2=triangle_area_series,\n        triangle_aspect_ratio=triangle_aspect_series,\n        triangle_sides_m=triangle_sides_series,\n        centroid_lat_rad=centroid_latitudes,\n        centroid_lon_rad=centroid_longitudes,\n        centroid_alt_m=centroid_altitudes,\n        max_ground_distance_km=max_ground_distance,\n        min_command_distance_km=min_command_distance_km,\n        metrics=metrics,\n        artefacts=artefacts,\n    )\n\n    if output_directory is not None:\n        output_path = Path(output_directory)\n        output_path.mkdir(parents=True, exist_ok=True)\n        summary_path = output_path / \"triangle_summary.json\"\n        artefacts[\"summary_path\"] = str(summary_path)\n\n        maintenance_rows = [\n            {\n                \"satellite_id\": sat_id,\n                \"mean_diff_accel_mps2\": data[\"mean_diff_accel_mps2\"],\n                \"peak_diff_accel_mps2\": data[\"peak_diff_accel_mps2\"],\n                \"delta_v_per_burn_mps\": data[\"delta_v_per_burn_mps\"],\n                \"annual_delta_v_mps\": data[\"annual_delta_v_mps\"],\n            }\n            for sat_id, data in maintenance[\"per_spacecraft\"].items()\n        ]\n        maintenance_df = pd.DataFrame(\n            maintenance_rows,\n            columns=[\n                \"satellite_id\",\n                \"mean_diff_accel_mps2\",\n                \"peak_diff_accel_mps2\",\n                \"delta_v_per_burn_mps\",\n                \"annual_delta_v_mps\",\n            ],\n        )\n        maintenance_path = output_path / \"maintenance_summary.csv\"\n        maintenance_df.to_csv(maintenance_path, index=False)\n        artefacts[\"maintenance_csv\"] = str(maintenance_path)\n\n        command_rows = [\n            {\"window_index\": index, **window}\n            for index, window in enumerate(command_latency[\"contact_windows\"])\n        ]\n        command_df = pd.DataFrame(\n            command_rows, columns=[\"window_index\", \"start\", \"end\", \"duration_s\"]\n        )\n        command_path = output_path / \"command_windows.csv\"\n        command_df.to_csv(command_path, index=False)\n        artefacts[\"command_windows_csv\"] = str(command_path)\n\n        injection_path = output_path / \"injection_recovery.csv\"\n        injection_samples.to_csv(\n            injection_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"satellite_id\",\n                \"position_error_m\",\n                \"velocity_error_mps\",\n                \"delta_v_mps\",\n                \"success\",\n            ],\n        )\n        artefacts[\"injection_recovery_csv\"] = str(injection_path)\n\n        drag_path = output_path / \"drag_dispersion.csv\"\n        drag_samples.to_csv(\n            drag_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"density_scale\",\n                \"drag_coefficient\",\n                \"ballistic_coefficient_m2_per_kg\",\n                \"semi_major_axis_delta_m\",\n                \"altitude_delta_m\",\n                \"along_track_shift_km\",\n                \"ground_distance_delta_km\",\n                \"command_distance_delta_km\",\n                \"within_tolerance\",\n            ],\n        )\n        artefacts[\"drag_dispersion_csv\"] = str(drag_path)\n\n        windows_path = output_path / \"formation_windows.csv\"\n        _write_formation_windows_csv(windows_path, window_series)\n        artefacts[\"formation_windows_csv\"] = str(windows_path)\n\n        station_path = output_path / \"station_keeping_events.csv\"\n        _write_station_keeping_csv(station_path, station_keeping)\n        artefacts[\"station_keeping_csv\"] = str(station_path)\n\n        plot_path = output_path / \"injection_recovery_cdf.svg\"\n        _write_injection_recovery_plot(injection_samples, plot_path)\n        if plot_path.exists():\n            artefacts[\"injection_recovery_plot\"] = str(plot_path)\n\n        orbital_path = output_path / \"orbital_elements.csv\"\n        _write_orbital_elements_csv(orbital_path, times, classical_series)\n        artefacts[\"orbital_elements_csv\"] = str(orbital_path)\n        metrics[\"orbital_elements\"][\"time_series\"][\"artefact\"] = str(orbital_path)\n        orbital_sat_dir = output_path / \"orbital_elements\"\n        orbital_sat_dir.mkdir(parents=True, exist_ok=True)\n        per_sat_paths = _write_orbital_elements_per_spacecraft(\n            orbital_sat_dir, times, classical_series\n        )\n        if per_sat_paths:\n            artefacts[\"orbital_elements_directory\"] = str(orbital_sat_dir)\n            metrics[\"orbital_elements\"][\"time_series\"][\"per_satellite_files\"] = {\n                sat_id: str(path)\n                for sat_id, path in sorted(per_sat_paths.items(), key=lambda item: item[0])\n            }\n\n        stk_dir = output_path / \"stk\"\n        _export_to_stk(\n            result,\n            stk_dir,\n            scenario_name=str(metadata.get(\"scenario_name\", \"Tehran Triangle Formation\")),\n        )\n        artefacts[\"stk_directory\"] = str(stk_dir)\n\n        summary_payload = result.to_summary()\n        summary_path.write_text(\n            json.dumps(summary_payload, indent=2), encoding=\"utf-8\"\n        )\n\n    return result\n\n\ndef _load_configuration(source: Mapping[str, object] | Path | str) -> Mapping[str, object]:\n    if isinstance(source, Mapping):\n        return source\n    path = Path(source)\n    payload = json.loads(path.read_text(encoding=\"utf-8\"))\n    if not isinstance(payload, Mapping):\n        raise TypeError(\"Triangle configuration must be a mapping.\")\n    if \"satellites\" not in payload and \"reference_orbit\" not in payload:\n        raise ValueError(\n            \"Configuration missing required section: satellites or reference_orbit\"\n        )\n    if \"formation\" not in payload:\n        raise ValueError(\"Configuration missing required section: formation\")\n    return payload\n\n\ndef _parse_time(value: object) -> datetime:\n    if isinstance(value, datetime):\n        return value\n    if isinstance(value, str):\n        text = value.strip()\n        if text.endswith(\"Z\"):\n            text = text[:-1] + \"+00:00\"\n        return datetime.fromisoformat(text)\n    raise TypeError(\"Configuration epoch must be a datetime or ISO 8601 string.\")\n\n\ndef _lvlh_frame(position: Sequence[float], velocity: Sequence[float]) -> np.ndarray:\n    r = np.asarray(position, dtype=float)\n    v = np.asarray(velocity, dtype=float)\n    r_hat = r / np.linalg.norm(r)\n    h = np.cross(r, v)\n    k_hat = h / np.linalg.norm(h)\n    j_hat = np.cross(k_hat, r_hat)\n    j_hat /= np.linalg.norm(j_hat)\n    return np.column_stack((r_hat, j_hat, k_hat))\n\n\ndef _formation_offsets(side_length_m: float) -> Mapping[str, np.ndarray]:\n    \"\"\"Return equilateral offsets constrained to the local horizontal plane.\"\"\"\n\n    sqrt_three = math.sqrt(3.0)\n    return {\n        \"SAT-1\": np.array(\n            [0.0, -0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-2\": np.array(\n            [0.0, 0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-3\": np.array([0.0, 0.0, sqrt_three / 3.0 * side_length_m]),\n    }\n\n\ndef _differentiate(samples: np.ndarray, step: float) -> np.ndarray:\n    derivatives = np.zeros_like(samples)\n    count = samples.shape[0]\n    for index in range(count):\n        if 0 < index < count - 1:\n            derivatives[index] = (samples[index + 1] - samples[index - 1]) / (2.0 * step)\n        elif index == 0:\n            derivatives[index] = (samples[index + 1] - samples[index]) / step\n        else:\n            derivatives[index] = (samples[index] - samples[index - 1]) / step\n    return derivatives\n\n\ndef _compute_classical_elements_series(\n    positions: Mapping[str, np.ndarray],\n    velocities: Mapping[str, np.ndarray],\n) -> Mapping[str, Mapping[str, np.ndarray]]:\n    \"\"\"Evaluate classical orbital elements for each spacecraft over time.\"\"\"\n\n    if not positions:\n        return {}\n\n    sample_count = next(iter(positions.values())).shape[0]\n    series: dict[str, dict[str, np.ndarray]] = {}\n    for sat_id, position_history in positions.items():\n        per_satellite = {\n            name: np.zeros(sample_count, dtype=float) for name in CLASSICAL_ELEMENT_FIELDS\n        }\n        series[sat_id] = per_satellite\n        velocity_history = velocities.get(sat_id)\n        if velocity_history is None:\n            continue\n        for index in range(sample_count):\n            elements = cartesian_to_classical(\n                position_history[index], velocity_history[index]\n            )\n            per_satellite[\"semi_major_axis_km\"][index] = elements.semi_major_axis / 1_000.0\n            per_satellite[\"eccentricity\"][index] = elements.eccentricity\n            per_satellite[\"inclination_deg\"][index] = _normalise_degrees(\n                elements.inclination\n            )\n            per_satellite[\"raan_deg\"][index] = _normalise_degrees(elements.raan)\n            per_satellite[\"argument_of_perigee_deg\"][index] = _normalise_degrees(\n                elements.arg_perigee\n            )\n            per_satellite[\"mean_anomaly_deg\"][index] = _normalise_degrees(\n                elements.mean_anomaly\n            )\n\n    return series\n\n\ndef _normalise_degrees(angle_rad: float) -> float:\n    \"\"\"Convert *angle_rad* to degrees on the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _write_orbital_elements_csv(\n    path: Path, times: Sequence[datetime], series: Mapping[str, Mapping[str, np.ndarray]]\n) -> None:\n    \"\"\"Write a consolidated orbital-element history to *path*.\"\"\"\n\n    records: list[dict[str, object]] = []\n    satellite_ids = sorted(series)\n    for index, epoch in enumerate(times):\n        timestamp = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        for sat_id in satellite_ids:\n            elements = series.get(sat_id)\n            if not elements:\n                continue\n            record = {\"time_utc\": timestamp, \"satellite_id\": sat_id}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        columns = [\"time_utc\", \"satellite_id\", *CLASSICAL_ELEMENT_FIELDS]\n        dataframe = pd.DataFrame(columns=columns)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_orbital_elements_per_spacecraft(\n    directory: Path,\n    times: Sequence[datetime],\n    series: Mapping[str, Mapping[str, np.ndarray]],\n) -> Mapping[str, Path]:\n    \"\"\"Persist individual per-spacecraft orbital-element CSVs.\"\"\"\n\n    paths: dict[str, Path] = {}\n    for sat_id, elements in series.items():\n        records: list[dict[str, object]] = []\n        for index, epoch in enumerate(times):\n            record = {\"time_utc\": epoch.isoformat().replace(\"+00:00\", \"Z\")}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n        dataframe = pd.DataFrame.from_records(records)\n        if dataframe.empty:\n            dataframe = pd.DataFrame(columns=[\"time_utc\", *CLASSICAL_ELEMENT_FIELDS])\n        filename = f\"{sat_id.lower().replace(' ', '_')}_orbital_elements.csv\"\n        path = directory / filename\n        dataframe.to_csv(path, index=False)\n        paths[sat_id] = path\n    return paths\n\n\ndef _summarise_triangle_metrics(areas: np.ndarray, aspects: np.ndarray, sides: np.ndarray) -> Mapping[str, float]:\n    return {\n        \"mean_area_m2\": float(np.mean(areas)),\n        \"min_area_m2\": float(np.min(areas)),\n        \"max_area_m2\": float(np.max(areas)),\n        \"mean_aspect_ratio\": float(np.mean(aspects)),\n        \"aspect_ratio_max\": float(np.max(aspects)),\n        \"mean_side_lengths_m\": [float(val) for val in np.mean(sides, axis=0)],\n    }\n\n\ndef _summarise_ground_metrics(distances_km: np.ndarray, formation: Mapping[str, object]) -> Mapping[str, float]:\n    return {\n        \"max_ground_distance_km\": float(np.max(distances_km)),\n        \"min_ground_distance_km\": float(np.min(distances_km)),\n        \"ground_distance_tolerance_km\": float(formation.get(\"ground_tolerance_km\", 350.0)),\n    }\n\n\ndef _formation_window(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> tuple[Mapping[str, object], Sequence[Mapping[str, object]]]:\n    \"\"\"Derive the principal formation window together with the full schedule.\"\"\"\n\n    windows = list(\n        _enumerate_formation_windows(\n            aspects,\n            distances_km,\n            step,\n            formation,\n            times,\n        )\n    )\n    if not windows:\n        primary: Mapping[str, object] = {\n            \"duration_s\": 0.0,\n            \"start\": None,\n            \"end\": None,\n            \"sample_count\": 0,\n        }\n        return primary, windows\n\n    primary = max(windows, key=lambda item: float(item.get(\"duration_s\", 0.0)))\n    return primary, windows\n\n\ndef _enumerate_formation_windows(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> Iterable[Mapping[str, object]]:\n    \"\"\"Yield contiguous access windows that satisfy the ground-track criteria.\"\"\"\n\n    tolerance = float(formation.get(\"ground_tolerance_km\", 350.0))\n    aspect_limit = float(formation.get(\"aspect_ratio_tolerance\", 1.02))\n    mask = (distances_km <= tolerance) & (aspects <= aspect_limit)\n\n    start_index: Optional[int] = None\n    for index, valid in enumerate(mask):\n        if valid and start_index is None:\n            start_index = index\n            continue\n        if not valid and start_index is not None:\n            yield _describe_window(\n                start_index,\n                index - 1,\n                times,\n                distances_km,\n                aspects,\n                step,\n            )\n            start_index = None\n\n    if start_index is not None:\n        yield _describe_window(\n            start_index,\n            len(mask) - 1,\n            times,\n            distances_km,\n            aspects,\n            step,\n        )\n\n\ndef _describe_window(\n    start: int,\n    end: int,\n    times: Sequence[datetime],\n    distances_km: np.ndarray,\n    aspects: np.ndarray,\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Summarise a valid access window spanning indices ``start`` to ``end``.\"\"\"\n\n    start_time = times[start]\n    end_time = times[end]\n    if end > start:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        earlier = start_time - half_step\n        later = end_time + half_step\n        # Preserve the original timezone information when clamping.\n        if earlier < times[0]:\n            earlier = times[0]\n        if later > times[-1]:\n            later = times[-1]\n        duration_seconds = float(step)\n        start_time = earlier\n        end_time = later\n\n    slice_obj = slice(start, end + 1)\n    window_distances = distances_km[slice_obj]\n    window_aspects = aspects[slice_obj]\n    centroid_index = start + (end - start) // 2\n    centroid_time = times[centroid_index]\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"sample_count\": int(end - start + 1),\n        \"max_ground_distance_km\": float(np.max(window_distances)),\n        \"min_ground_distance_km\": float(np.min(window_distances)),\n        \"max_aspect_ratio\": float(np.max(window_aspects)),\n        \"centroid_time\": centroid_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n\n\ndef _summarise_window_recurrence(\n    windows: Sequence[Mapping[str, object]],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    \"\"\"Quantify the repeatability of access windows over the simulation horizon.\"\"\"\n\n    if not windows:\n        return {\n            \"window_count\": 0,\n            \"mean_duration_s\": 0.0,\n            \"min_duration_s\": 0.0,\n            \"max_duration_s\": 0.0,\n            \"mean_interval_s\": 0.0,\n            \"min_interval_s\": 0.0,\n            \"max_interval_s\": 0.0,\n            \"std_interval_s\": 0.0,\n            \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n            \"repeatability_score\": 0.0,\n            \"interval_samples\": 0,\n        }\n\n    durations = np.array([float(window.get(\"duration_s\", 0.0)) for window in windows])\n    mean_duration = float(np.mean(durations)) if durations.size else 0.0\n    min_duration = float(np.min(durations)) if durations.size else 0.0\n    max_duration = float(np.max(durations)) if durations.size else 0.0\n\n    midpoints: list[datetime] = []\n    for window in windows:\n        start = window.get(\"start\")\n        end = window.get(\"end\")\n        if not start or not end:\n            continue\n        start_time = _parse_time(start)\n        end_time = _parse_time(end)\n        delta = end_time - start_time\n        midpoint = start_time + delta / 2 if delta.total_seconds() >= 0.0 else start_time\n        midpoints.append(midpoint)\n\n    intervals = []\n    for first, second in zip(midpoints, midpoints[1:]):\n        delta = (second - first).total_seconds()\n        if delta > 0.0:\n            intervals.append(delta)\n\n    if intervals:\n        interval_array = np.asarray(intervals, dtype=float)\n        mean_interval = float(np.mean(interval_array))\n        min_interval = float(np.min(interval_array))\n        max_interval = float(np.max(interval_array))\n        std_interval = float(np.std(interval_array))\n        if mean_interval > 0.0:\n            repeatability = max(0.0, 1.0 - std_interval / mean_interval)\n        else:\n            repeatability = 0.0\n    else:\n        mean_interval = min_interval = max_interval = std_interval = 0.0\n        repeatability = 1.0 if len(windows) == 1 else 0.0\n\n    return {\n        \"window_count\": len(windows),\n        \"mean_duration_s\": mean_duration,\n        \"min_duration_s\": min_duration,\n        \"max_duration_s\": max_duration,\n        \"mean_interval_s\": mean_interval,\n        \"min_interval_s\": min_interval,\n        \"max_interval_s\": max_interval,\n        \"std_interval_s\": std_interval,\n        \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n        \"repeatability_score\": repeatability,\n        \"interval_samples\": len(intervals),\n    }\n\n\ndef _orbital_period(semi_major_axis_m: float) -> float:\n    if semi_major_axis_m <= 0.0:\n        return 0.0\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    return float(2.0 * math.pi / mean_motion)\n\n\ndef _assess_station_keeping(\n    times: Sequence[datetime],\n    triangle_sides: np.ndarray,\n    formation: Mapping[str, object],\n    maintenance: Mapping[str, object],\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Detect when the triangular geometry exceeds the maintenance tolerance.\"\"\"\n\n    if not len(times):\n        return {\n            \"status\": \"insufficient_data\",\n            \"events\": [],\n            \"tolerance_m\": 0.0,\n            \"violation_fraction\": 0.0,\n            \"recommended_delta_v_mps\": 0.0,\n            \"max_deviation_m\": 0.0,\n        }\n\n    base_length = float(formation.get(\"side_length_m\", 0.0))\n    default_tolerance = base_length * 0.01 if base_length > 0.0 else 10.0\n    tolerance = float(\n        formation.get(\n            \"station_keeping_tolerance_m\",\n            formation.get(\"repeatability_tolerance_m\", default_tolerance),\n        )\n    )\n    if tolerance <= 0.0:\n        tolerance = default_tolerance\n\n    deviations = np.abs(triangle_sides - base_length)\n    max_deviation_series = np.max(deviations, axis=1)\n    violation_mask = max_deviation_series > tolerance\n\n    events: list[Mapping[str, object]] = []\n    start_index: Optional[int] = None\n    peak_violation = 0.0\n    for index, violated in enumerate(violation_mask):\n        if violated:\n            if start_index is None:\n                start_index = index\n                peak_violation = float(max_deviation_series[index])\n            else:\n                peak_violation = max(peak_violation, float(max_deviation_series[index]))\n            continue\n        if start_index is not None:\n            events.append(\n                _build_station_event(\n                    start_index,\n                    index - 1,\n                    times,\n                    max_deviation_series,\n                    peak_violation,\n                    step,\n                )\n            )\n            start_index = None\n            peak_violation = 0.0\n\n    if start_index is not None:\n        events.append(\n            _build_station_event(\n                start_index,\n                len(times) - 1,\n                times,\n                max_deviation_series,\n                peak_violation,\n                step,\n            )\n        )\n\n    recommended_delta_v = _recommended_delta_v(maintenance)\n    for event in events:\n        event.setdefault(\"recommended_delta_v_mps\", recommended_delta_v)\n\n    violation_fraction = (\n        float(np.count_nonzero(violation_mask)) / float(len(max_deviation_series))\n        if len(max_deviation_series)\n        else 0.0\n    )\n\n    max_deviation = float(np.max(max_deviation_series)) if len(max_deviation_series) else 0.0\n\n    status = \"nominal\" if not events else \"correction_required\"\n    return {\n        \"status\": status,\n        \"events\": events,\n        \"tolerance_m\": tolerance,\n        \"violation_fraction\": violation_fraction,\n        \"recommended_delta_v_mps\": recommended_delta_v,\n        \"max_deviation_m\": max_deviation,\n    }\n\n\ndef _build_station_event(\n    start_index: int,\n    end_index: int,\n    times: Sequence[datetime],\n    deviations: np.ndarray,\n    peak_violation: float,\n    step: float,\n) -> Mapping[str, object]:\n    start_time = times[start_index]\n    end_time = times[end_index]\n    if end_index > start_index:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        start_time = max(start_time - half_step, times[0])\n        end_time = min(end_time + half_step, times[-1])\n        duration_seconds = float(step)\n\n    slice_obj = slice(start_index, end_index + 1)\n    peak = max(peak_violation, float(np.max(deviations[slice_obj])))\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"violation_samples\": int(end_index - start_index + 1),\n        \"peak_deviation_m\": float(peak),\n    }\n\n\ndef _recommended_delta_v(maintenance: Mapping[str, object]) -> float:\n    per_spacecraft = maintenance.get(\"per_spacecraft\") if isinstance(maintenance, Mapping) else None\n    if isinstance(per_spacecraft, Mapping) and per_spacecraft:\n        deltas = [\n            float(entry.get(\"delta_v_per_burn_mps\", 0.0))\n            for entry in per_spacecraft.values()\n        ]\n        if deltas:\n            return float(np.mean(deltas))\n\n    assumptions = maintenance.get(\"assumptions\") if isinstance(maintenance, Mapping) else None\n    if isinstance(assumptions, Mapping):\n        return float(assumptions.get(\"delta_v_budget_mps\", 0.0))\n    return 0.0\n\n\ndef _write_formation_windows_csv(\n    path: Path,\n    windows: Sequence[Mapping[str, object]],\n) -> None:\n    \"\"\"Serialise the detected formation windows to ``path``.\"\"\"\n\n    records: list[MutableMapping[str, object]] = []\n    for index, window in enumerate(windows):\n        record: MutableMapping[str, object] = {\"window_index\": index}\n        record[\"start\"] = window.get(\"start\")\n        record[\"end\"] = window.get(\"end\")\n        record[\"duration_s\"] = float(window.get(\"duration_s\", 0.0))\n        record[\"sample_count\"] = int(window.get(\"sample_count\", 0))\n        record[\"max_ground_distance_km\"] = float(\n            window.get(\"max_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"min_ground_distance_km\"] = float(\n            window.get(\"min_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"max_aspect_ratio\"] = float(window.get(\"max_aspect_ratio\", float(\"nan\")))\n        record[\"centroid_time\"] = window.get(\"centroid_time\")\n        records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"window_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"sample_count\",\n                \"max_ground_distance_km\",\n                \"min_ground_distance_km\",\n                \"max_aspect_ratio\",\n                \"centroid_time\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_station_keeping_csv(\n    path: Path,\n    station_keeping: Mapping[str, object],\n) -> None:\n    \"\"\"Write the station-keeping assessment events to ``path``.\"\"\"\n\n    events = station_keeping.get(\"events\") if isinstance(station_keeping, Mapping) else []\n    records: list[MutableMapping[str, object]] = []\n    if isinstance(events, Sequence):\n        for index, event in enumerate(events):\n            record: MutableMapping[str, object] = {\"event_index\": index}\n            record[\"start\"] = event.get(\"start\")\n            record[\"end\"] = event.get(\"end\")\n            record[\"duration_s\"] = float(event.get(\"duration_s\", 0.0))\n            record[\"violation_samples\"] = int(event.get(\"violation_samples\", 0))\n            record[\"peak_deviation_m\"] = float(event.get(\"peak_deviation_m\", 0.0))\n            record[\"recommended_delta_v_mps\"] = float(\n                event.get(\"recommended_delta_v_mps\", station_keeping.get(\"recommended_delta_v_mps\", 0.0))\n            )\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"event_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"violation_samples\",\n                \"peak_deviation_m\",\n                \"recommended_delta_v_mps\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _estimate_maintenance_delta_v(\n    positions: Mapping[str, np.ndarray],\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    maintenance = formation.get(\"maintenance\", {})\n    burn_duration_s = float(maintenance.get(\"burn_duration_s\", 45.0))\n    interval_days = float(maintenance.get(\"interval_days\", 7.0))\n    delta_v_budget_mps = float(maintenance.get(\"delta_v_budget_mps\", 15.0))\n    interval_seconds = max(interval_days * SECONDS_PER_DAY, 1.0)\n    burns_per_year = SECONDS_PER_YEAR / interval_seconds\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    centroid = sum(positions[sat_id] for sat_id in positions) / float(len(positions))\n    centroid_norm = np.linalg.norm(centroid, axis=1)\n    centroid_accel = -MU_EARTH * centroid / centroid_norm[:, None] ** 3\n\n    per_spacecraft: MutableMapping[str, Mapping[str, float]] = {}\n    mean_accels = []\n    peak_accels = []\n    annual_totals = []\n\n    for sat_id, states in positions.items():\n        radii = np.linalg.norm(states, axis=1)\n        accelerations = -MU_EARTH * states / radii[:, None] ** 3\n        differential = np.linalg.norm(accelerations - centroid_accel, axis=1)\n        mean_diff = float(np.mean(differential))\n        peak_diff = float(np.max(differential))\n        dv_per_burn = mean_diff * burn_duration_s\n        annual_delta_v = dv_per_burn * burns_per_year\n\n        mean_accels.append(mean_diff)\n        peak_accels.append(peak_diff)\n        annual_totals.append(annual_delta_v)\n\n        per_spacecraft[sat_id] = {\n            \"mean_diff_accel_mps2\": mean_diff,\n            \"peak_diff_accel_mps2\": peak_diff,\n            \"delta_v_per_burn_mps\": dv_per_burn,\n            \"annual_delta_v_mps\": annual_delta_v,\n        }\n\n    annual_array = np.asarray(annual_totals, dtype=float)\n    mean_accel = float(np.mean(mean_accels)) if mean_accels else 0.0\n    peak_accel = float(np.max(peak_accels)) if peak_accels else 0.0\n\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n\n    return {\n        \"assumptions\": {\n            \"burn_duration_s\": burn_duration_s,\n            \"maintenance_interval_days\": interval_days,\n            \"burns_per_year\": float(burns_per_year),\n            \"analysis_window_s\": float(max((count - 1), 0) * step),\n            \"orbit_period_s\": float(orbit_period),\n            \"delta_v_budget_mps\": delta_v_budget_mps,\n        },\n        \"mean_differential_acceleration_mps2\": mean_accel,\n        \"peak_differential_acceleration_mps2\": peak_accel,\n        \"per_spacecraft\": per_spacecraft,\n        \"annual_delta_v_mps\": {\n            \"mean\": float(np.mean(annual_array)) if annual_totals else 0.0,\n            \"max\": float(np.max(annual_array)) if annual_totals else 0.0,\n            \"min\": float(np.min(annual_array)) if annual_totals else 0.0,\n        },\n    }\n\n\ndef _analyse_command_latency(\n    min_distances_km: np.ndarray,\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    command_lat: float,\n    command_lon: float,\n) -> Mapping[str, object]:\n    command_cfg = formation.get(\"command\", {})\n    range_km = float(\n        command_cfg.get(\"contact_range_km\", formation.get(\"ground_tolerance_km\", 350.0))\n    )\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    mask = np.asarray(min_distances_km) <= range_km\n\n    windows: list[Mapping[str, object]] = []\n    current_start: Optional[int] = None\n    for index, in_contact in enumerate(mask):\n        if in_contact and current_start is None:\n            current_start = index\n        elif not in_contact and current_start is not None:\n            end_index = index - 1\n            windows.append(\n                _format_contact_window(times, current_start, end_index, step)\n            )\n            current_start = None\n    if current_start is not None:\n        windows.append(\n            _format_contact_window(times, current_start, len(mask) - 1, step)\n        )\n\n    contact_duration_s = float(sum(window[\"duration_s\"] for window in windows))\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n    contact_probability = contact_duration_s / orbit_period if orbit_period else 0.0\n    gap_s = max(orbit_period - contact_duration_s, 0.0)\n    max_latency_hours = gap_s / 3600.0\n    mean_latency_hours = max_latency_hours / 2.0\n\n    return {\n        \"station\": {\n            \"latitude_deg\": math.degrees(command_lat),\n            \"longitude_deg\": math.degrees(command_lon),\n            \"contact_range_km\": range_km,\n        },\n        \"contact_probability\": contact_probability,\n        \"contact_duration_s\": contact_duration_s,\n        \"contact_windows\": windows,\n        \"passes_per_day\": SECONDS_PER_DAY / orbit_period if orbit_period else 0.0,\n        \"max_latency_hours\": max_latency_hours,\n        \"mean_latency_hours\": mean_latency_hours,\n        \"latency_margin_hours\": 12.0 - max_latency_hours,\n        \"assumptions\": {\n            \"uniform_request_distribution\": True,\n            \"single_station_operations\": True,\n        },\n    }\n\n\ndef _format_contact_window(\n    times: Sequence[datetime], start_index: int, end_index: int, step: float\n) -> Mapping[str, object]:\n    count = end_index - start_index + 1\n    duration = max(count - 1, 0) * step\n    return {\n        \"start\": times[start_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": times[end_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration),\n    }\n\n\ndef _run_injection_recovery_monte_carlo(\n    satellite_ids: Sequence[str], formation: Mapping[str, object]\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    config = formation.get(\"monte_carlo\", {})\n    sample_count = int(config.get(\"samples\", 200))\n    position_sigma_m = float(config.get(\"position_sigma_m\", 250.0))\n    velocity_sigma_mmps = float(config.get(\"velocity_sigma_mmps\", 5.0))\n    velocity_sigma_mps = velocity_sigma_mmps / 1_000.0\n    recovery_time_s = float(config.get(\"recovery_time_s\", 12.0 * 3600.0))\n    delta_v_budget = float(config.get(\"delta_v_budget_mps\", 15.0))\n    seed = config.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    records: list[Mapping[str, object]] = []\n    for sample_index in range(sample_count):\n        for sat_id in satellite_ids:\n            position_error = rng.normal(0.0, position_sigma_m, size=3)\n            velocity_error = rng.normal(0.0, velocity_sigma_mps, size=3)\n            position_mag = float(np.linalg.norm(position_error))\n            velocity_mag = float(np.linalg.norm(velocity_error))\n            delta_v_position = 2.0 * position_mag / recovery_time_s\n            delta_v_total = delta_v_position + velocity_mag\n            success = delta_v_total <= delta_v_budget\n            records.append(\n                {\n                    \"sample_id\": sample_index,\n                    \"satellite_id\": sat_id,\n                    \"position_error_m\": position_mag,\n                    \"velocity_error_mps\": velocity_mag,\n                    \"delta_v_mps\": delta_v_total,\n                    \"success\": bool(success),\n                }\n            )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"success_rate\": 1.0,\n            \"mean_delta_v_mps\": 0.0,\n            \"p95_delta_v_mps\": 0.0,\n            \"max_delta_v_mps\": 0.0,\n        }\n        per_spacecraft = {sat_id: aggregate for sat_id in satellite_ids}\n        success_rate = 1.0\n    else:\n        success_rate = float(dataframe[\"success\"].mean())\n        per_spacecraft = {}\n        for sat_id, group in dataframe.groupby(\"satellite_id\"):\n            per_spacecraft[str(sat_id)] = {\n                \"mean_delta_v_mps\": float(group[\"delta_v_mps\"].mean()),\n                \"p95_delta_v_mps\": float(group[\"delta_v_mps\"].quantile(0.95)),\n                \"max_delta_v_mps\": float(group[\"delta_v_mps\"].max()),\n                \"success_rate\": float(group[\"success\"].mean()),\n            }\n        aggregate = {\n            \"mean_delta_v_mps\": float(dataframe[\"delta_v_mps\"].mean()),\n            \"p95_delta_v_mps\": float(dataframe[\"delta_v_mps\"].quantile(0.95)),\n            \"max_delta_v_mps\": float(dataframe[\"delta_v_mps\"].max()),\n        }\n\n    metrics = {\n        \"success_rate\": success_rate,\n        \"sample_count\": sample_count,\n        \"per_spacecraft\": per_spacecraft,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"position_sigma_m\": position_sigma_m,\n            \"velocity_sigma_mmps\": velocity_sigma_mmps,\n            \"recovery_time_s\": recovery_time_s,\n            \"delta_v_budget_mps\": delta_v_budget,\n        },\n    }\n\n    return metrics, dataframe\n\n\ndef _run_atmospheric_drag_dispersion_monte_carlo(\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    inclination_rad: float,\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    settings = formation.get(\"drag_dispersion\", {})\n    sample_count = int(settings.get(\"samples\", 200))\n    density_sigma = float(settings.get(\"density_sigma\", 0.2))\n    drag_coefficient_sigma = float(settings.get(\"drag_coefficient_sigma\", 0.05))\n    reference_density = float(settings.get(\"reference_density_kg_m3\", 3.5e-12))\n    drag_coefficient = float(settings.get(\"drag_coefficient\", 2.2))\n    ballistic_area = float(settings.get(\"reference_area_m2\", 1.0))\n    spacecraft_mass = float(settings.get(\"spacecraft_mass_kg\", 150.0))\n    time_horizon_orbits = float(settings.get(\"time_horizon_orbits\", 15.0))\n    integration_step = float(settings.get(\"integration_step_s\", 60.0))\n    seed = settings.get(\"seed\")\n\n    if spacecraft_mass <= 0.0:\n        raise ValueError(\"Spacecraft mass must be positive for drag dispersions.\")\n\n    rng = np.random.default_rng(seed)\n    ballistic_coeff_base = ballistic_area / spacecraft_mass\n\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    orbital_period = 2.0 * math.pi / mean_motion\n    horizon = max(time_horizon_orbits, 0.0) * orbital_period\n    horizon = max(horizon, integration_step)\n\n    records: list[Mapping[str, object]] = []\n    tolerance_km = float(formation.get(\"ground_tolerance_km\", 350.0))\n    orbital_radius = semi_major_axis_m\n    ground_projection_scale = math.cos(inclination_rad)\n\n    time_samples = np.arange(0.0, horizon + 0.5 * integration_step, integration_step)\n\n    for sample_id in range(sample_count):\n        density_scale = max(0.0, 1.0 + rng.normal(0.0, density_sigma))\n        cd_scale = max(0.0, 1.0 + rng.normal(0.0, drag_coefficient_sigma))\n\n        rho = reference_density * density_scale\n        cd = drag_coefficient * cd_scale\n        ballistic_coefficient = ballistic_coeff_base\n\n        orbital_velocity = math.sqrt(MU_EARTH / semi_major_axis_m)\n        drag_acceleration = 0.5 * rho * cd * ballistic_coefficient * orbital_velocity**2\n        da_dt = -2.0 * semi_major_axis_m**2 * drag_acceleration / MU_EARTH\n\n        semi_major_axis_series = semi_major_axis_m + da_dt * time_samples\n        semi_major_axis_series = np.maximum(semi_major_axis_series, EARTH_EQUATORIAL_RADIUS_M + 150_000.0)\n\n        altitude_delta = semi_major_axis_series[-1] - semi_major_axis_m\n        new_mean_motion = math.sqrt(MU_EARTH / semi_major_axis_series[-1] ** 3)\n        delta_theta = (new_mean_motion - mean_motion) * horizon\n        along_track_shift = abs(delta_theta) * orbital_radius / 1_000.0\n\n        ground_distance_delta = along_track_shift * abs(ground_projection_scale)\n        command_distance_delta = along_track_shift\n\n        within_tolerance = ground_distance_delta <= tolerance_km\n\n        records.append(\n            {\n                \"sample_id\": sample_id,\n                \"density_scale\": float(density_scale),\n                \"drag_coefficient\": float(cd),\n                \"ballistic_coefficient_m2_per_kg\": float(ballistic_coefficient),\n                \"semi_major_axis_delta_m\": float(altitude_delta),\n                \"altitude_delta_m\": float(altitude_delta),\n                \"along_track_shift_km\": float(along_track_shift),\n                \"ground_distance_delta_km\": float(ground_distance_delta),\n                \"command_distance_delta_km\": float(command_distance_delta),\n                \"within_tolerance\": bool(within_tolerance),\n            }\n        )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": 0.0,\n            \"max_ground_distance_delta_km\": 0.0,\n            \"p95_along_track_shift_km\": 0.0,\n            \"max_along_track_shift_km\": 0.0,\n            \"p95_altitude_delta_m\": 0.0,\n        }\n        success_rate = 1.0\n    else:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].quantile(0.95)\n            ),\n            \"max_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].max()\n            ),\n            \"p95_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].quantile(0.95)\n            ),\n            \"max_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].max()\n            ),\n            \"p95_altitude_delta_m\": float(\n                dataframe[\"altitude_delta_m\"].quantile(0.95)\n            ),\n        }\n        success_rate = float(dataframe[\"within_tolerance\"].mean())\n\n    metrics = {\n        \"sample_count\": sample_count,\n        \"success_rate\": success_rate,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"density_sigma\": density_sigma,\n            \"drag_coefficient_sigma\": drag_coefficient_sigma,\n            \"reference_density_kg_m3\": reference_density,\n            \"drag_coefficient\": drag_coefficient,\n            \"reference_area_m2\": ballistic_area,\n            \"spacecraft_mass_kg\": spacecraft_mass,\n            \"time_horizon_orbits\": time_horizon_orbits,\n            \"integration_step_s\": integration_step,\n        },\n        \"tolerance_km\": tolerance_km,\n    }\n\n    return metrics, dataframe\n\n\ndef _write_injection_recovery_plot(samples: pd.DataFrame, output_path: Path) -> None:\n    if samples.empty:\n        return\n\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt  # noqa: WPS433 - imported for plotting\n\n    values = np.sort(samples[\"delta_v_mps\"].to_numpy(dtype=float))\n    cumulative = (np.arange(len(values), dtype=float) + 1.0) / float(len(values))\n\n    figure, axis = plt.subplots(figsize=(6.0, 4.0))\n    axis.plot(values, cumulative, color=\"#1f77b4\", linewidth=2.0)\n    axis.set_xlabel(\"Δv per spacecraft (m/s)\")\n    axis.set_ylabel(\"Cumulative probability\")\n    axis.set_title(\"Injection recovery Δv distribution\")\n    axis.grid(True, alpha=0.3)\n    figure.tight_layout()\n    figure.savefig(output_path, format=\"svg\")\n    plt.close(figure)\n\ndef _export_to_stk(result: TriangleFormationResult, output_dir: Path, scenario_name: str) -> None:\n    output_dir.mkdir(parents=True, exist_ok=True)\n    state_histories = []\n    ground_tracks = []\n    contacts = []\n\n    satellite_ids = sorted(result.positions_m)\n    safe_satellite_names = unique_stk_names(satellite_ids)\n    safe_facility_name = sanitize_stk_identifier(\"Tehran\", default=\"Facility\")\n    safe_scenario_name = sanitize_stk_identifier(scenario_name, default=\"Scenario\")\n\n    for sat_id in satellite_ids:\n        safe_id = safe_satellite_names[sat_id]\n        samples = [\n            StateSample(\n                epoch=epoch,\n                position_eci_km=result.positions_m[sat_id][index] / 1_000.0,\n                velocity_eci_kms=result.velocities_mps[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        state_histories.append(PropagatedStateHistory(satellite_id=safe_id, samples=samples))\n\n        track_points = [\n            GroundTrackPoint(\n                epoch=epoch,\n                latitude_deg=math.degrees(result.latitudes_rad[sat_id][index]),\n                longitude_deg=math.degrees(result.longitudes_rad[sat_id][index]),\n                altitude_km=result.altitudes_m[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        ground_tracks.append(GroundTrack(satellite_id=safe_id, points=track_points))\n\n    window = result.metrics[\"formation_window\"]\n    if window.get(\"start\") and window.get(\"end\"):\n        start = datetime.fromisoformat(window[\"start\"].replace(\"Z\", \"+00:00\"))\n        end = datetime.fromisoformat(window[\"end\"].replace(\"Z\", \"+00:00\"))\n        for sat_id in satellite_ids:\n            safe_id = safe_satellite_names[sat_id]\n            contacts.append(\n                GroundContactInterval(\n                    satellite_id=safe_id,\n                    facility_name=safe_facility_name,\n                    start=start,\n                    end=end,\n                )\n            )\n\n    facilities = [\n        FacilityDefinition(\n            name=safe_facility_name,\n            latitude_deg=35.6892,\n            longitude_deg=51.3890,\n            altitude_km=1.5,\n        )\n    ]\n\n    scenario_metadata = ScenarioMetadata(\n        scenario_name=safe_scenario_name,\n        start_epoch=result.times[0],\n        stop_epoch=result.times[-1],\n        central_body=\"Earth\",\n        coordinate_frame=\"TEME\",\n        ephemeris_step_seconds=(result.times[1] - result.times[0]).total_seconds()\n        if len(result.times) > 1\n        else None,\n    )\n\n    sim_results = SimulationResults(\n        state_histories=state_histories,\n        ground_tracks=ground_tracks,\n        ground_contacts=contacts,\n        facilities=facilities,\n    )\n\n    export_simulation_to_stk(sim_results, output_dir, scenario_metadata)\n\n\n__all__ = [\"simulate_triangle_formation\", \"TriangleFormationResult\"]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "I made a mistake in `_propagate_deterministic` by replacing the `while` loop with a `for` loop without updating the `epoch`. I'll revert that change. My new plan is to correctly add a `tqdm` progress bar to the `_propagate_deterministic` loop by calculating the number of steps and iterating, and also wrap the outer `for` loop in `_run_monte_carlo` with `tqdm`. I'll start by reverting the incorrect change."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "new_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    while epoch <= settings.stop_time + timedelta(seconds=1e-6):",
            "instruction": "Revert the previous incorrect change to the propagation loop.",
            "old_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n\n    n_steps = int((settings.stop_time - settings.start_time).total_seconds() / dt)\n    for _ in tqdm(range(n_steps), desc=\"Deterministic propagation\"):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793199357-fed817c9720ee",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Okay, I'm correcting `sim/scripts/perturbation_analysis.py`. I'll replace the `while` loop in `_propagate_deterministic` with a `tqdm`-wrapped `for` loop, ensuring `epoch` and the `break` condition are handled correctly. Then, I'll add a `tqdm` progress bar to the `_run_monte_carlo`'s `for` loop. I'll apply these changes separately, starting with `_propagate_deterministic`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "new_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    for _ in tqdm(range(n_steps + 1), desc=\"Deterministic propagation\"):",
            "old_string": "    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    while epoch <= settings.stop_time + timedelta(seconds=1e-6):",
            "instruction": "Add a tqdm progress bar to the deterministic propagation loop."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793244665-6011835c8d69e",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiYB4/H/Xt+J2lsItyq5ecUE3G3zn2uxytktujOUjCHPxMxYG/jPDgpYAePx/15CZakqFLNX4kWiWmPPiANTm8kaQoogDwvEke0fz5rpN0ipVaJ9p1627i6YG1nnQLsYtK1R1un+QES/gg51VtEbZZElIuAkmuEHGK6cfvczWcstbAosAePx/179AsBxPRsGvJMJ2c553a7ZrQ92T4QvzHwp+8t7ZzmQcql6+bEQGuc=",
        "text": "Okay, adding a progress bar to `_run_monte_carlo` now."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop.",
            "old_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "new_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793275991-a88d3d32561f",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the target code, including the `from tqdm import tqdm` import and the `tqdm` wrapper around the loop, was already present in the file. No changes are required as the desired modification has already been applied.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):). Original old_string was (    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "I'm re-attempting a `replace` command. The previous one failed because I was re-adding an existing `tqdm` import. I'll now replace `for _ in range(runs):` with `for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):`, using the correct context from the file. The `old_string` and `new_string` now look correct, so I'll execute the command."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "old_string": "    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793341377-d34fc1c3673ba",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the `from tqdm import tqdm` line was missing from the search string, and the `for` loop was already wrapped with `tqdm` in the file. The requested change has already been applied to the file, so no further modifications are needed. The corrected search string now matches the existing content, and the replace string is identical to the search string, indicating no changes are required.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):). Original old_string was (    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XnUWajtxMfW7W0x71sN7935B6LSx4qYpztX06FAcF5UKXAHj8f9eIPenq1b4dab4ebH0RnqhlJO7ERFnH7h21AyHGeE8qDRsJoiL3uqcHlvZknrXzmsfjwB/R6oOfLGTdoT7fC2QzGOkTT0A1KBLHddTq1SV21hJc0RJzZYZCnAB4/H/XhglfcRI40KkJpJiTwCE9UHRW2emRFcN1eBhUChRRfUB+c2oPSCU2z2snIJu7Aimn/Iqk1rrrvtyzcQikrGwFvbul+RScWqWUE/krdOawUpe9V1Ac95SFgeFthKkbEKvTctlXRO1ue/K6TyJCqkBAePx/14Qr+XsPD3qA5Jx1yVpHUBceTsu0aUuBW0+hZ6ZzPmd5wvp2/84EfY2oo2kypwxXW8ceuBZW/e2rMbnvoT95j2ISbvYxWk/B5zFsXNYHgBpqPGZT221uypFUooOUJ8G5HYUVVbY5+jMiYcZ1oOVdkUJwqlHMRdk8b1YEbdf/vPMnHk770JZHRobctuzTtydYn3n9SxfvqXxJgJ2oBY9xm9RhuZH6QoqAePx/14vb2v3F2tV/023gu16pWYz8p4IQJEYxGvXT711gbNaY5/RtDFk",
        "text": "The tool's failing, claiming the change is already there. I'll re-read `sim/scripts/perturbation_analysis.py` to check if my last `replace` call somehow made both changes."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762793371105-2cdd49fa2c745",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"High-fidelity constellation propagation with \\(J_2\\) and drag perturbations.\n\nThe helper functions defined here extend the lightweight scenario pipeline by\nintroducing a deterministic force model and a Monte Carlo dispersion analysis\ntailored to the Tehran daily pass configuration.  The propagated states capture\ncross-track offsets for every spacecraft in the formation, enabling direct\nverification of the ±30 km primary alignment tolerance (and the ±70 km waiver\nceiling) that governs MR-2 and SRD-P-001.  Outputs are serialisable to JSON and\ncomma-separated value (CSV) formats so they remain compatible with Systems Tool\nKit (STK 11.2) validation workflows as required by the repository guidelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Mapping, MutableMapping, Sequence\n\nimport os\n\nimport numpy as np\nfrom statistics import fmean\n\nfrom src.constellation.frames import rotation_matrix_rtn_to_eci\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    EARTH_ROTATION_RATE,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\n\n\nJ2_COEFFICIENT = 1.08262668e-3\nSOLAR_FLUX_BASE = 150.0\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\n\nDEFAULT_FORMATION = (\n    {\n        \"identifier\": \"FSAT-LDR\",\n        \"role\": \"leader\",\n        \"radial_offset_km\": 0.0,\n        \"along_track_offset_km\": 0.0,\n        \"cross_track_offset_km\": 0.0,\n    },\n    {\n        \"identifier\": \"FSAT-DP1\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": -0.05,\n        \"along_track_offset_km\": 20.0,\n        \"cross_track_offset_km\": 0.2,\n    },\n    {\n        \"identifier\": \"FSAT-DP2\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": 0.05,\n        \"along_track_offset_km\": -20.0,\n        \"cross_track_offset_km\": -0.2,\n    },\n)\n\nPLANE_ASSIGNMENTS = {\n    \"FSAT-LDR\": \"Plane A\",\n    \"FSAT-DP1\": \"Plane A\",\n    \"FSAT-DP2\": \"Plane B\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nCLASSICAL_ELEMENT_UNITS = {\n    \"semi_major_axis_km\": \"km\",\n    \"eccentricity\": \"\",\n    \"inclination_deg\": \"deg\",\n    \"raan_deg\": \"deg\",\n    \"argument_of_perigee_deg\": \"deg\",\n    \"mean_anomaly_deg\": \"deg\",\n}\n\n\nDEFAULT_MONTE_CARLO = {\n    \"enabled\": True,\n    \"runs\": 1000,\n    \"seed\": 4242,\n    \"dispersions\": {\n        \"semi_major_axis_sigma_m\": 5.0,\n        \"inclination_sigma_deg\": 0.01,\n        \"drag_coefficient_sigma\": 0.05,\n    },\n}\n\n\n@dataclass\nclass PropagatorSettings:\n    \"\"\"Container describing the deterministic propagation configuration.\"\"\"\n\n    start_time: datetime\n    epoch_time: datetime\n    stop_time: datetime\n    time_step_s: float\n    drag_coefficient: float\n    ballistic_coefficient_m2_per_kg: float\n    solar_flux_index: float\n    evaluation_time: datetime | None = None\n    primary_cross_track_limit_km: float = 30.0\n    waiver_cross_track_limit_km: float = 70.0\n    plane_intersection_limit_km: float | None = None\n\n\n@dataclass\nclass SpacecraftState:\n    \"\"\"Cartesian state and drag properties for a spacecraft.\"\"\"\n\n    identifier: str\n    position_m: np.ndarray\n    velocity_mps: np.ndarray\n    drag_coefficient: float\n    area_m2: float\n    mass_kg: float\n\n\ndef propagate_constellation(\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Path | None = None,\n    formation: Sequence[Mapping[str, float | str]] = DEFAULT_FORMATION,\n    settings: PropagatorSettings | None = None,\n    monte_carlo: Mapping[str, object] | None = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation including \\(J_2\\) and atmospheric drag.\"\"\"\n\n    resolved_settings = settings or _default_settings(scenario)\n    monte_carlo_config = _normalise_monte_carlo(monte_carlo)\n\n    target_lat, target_lon = _target_coordinates(scenario)\n    leader_elements = _scenario_orbital_elements(scenario)\n\n    deterministic_series, deterministic_metrics = _propagate_deterministic(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n    )\n\n    monte_carlo_metrics = _run_monte_carlo(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n        monte_carlo_config,\n    )\n\n    artefact_paths: MutableMapping[str, str] = {}\n    if output_directory:\n        artefact_paths = _write_outputs(\n            Path(output_directory),\n            deterministic_series,\n            deterministic_metrics,\n            monte_carlo_metrics,\n            resolved_settings,\n            monte_carlo_config,\n        )\n\n    summary = _build_summary(\n        deterministic_metrics,\n        monte_carlo_metrics,\n        resolved_settings,\n        monte_carlo_config,\n    )\n\n    return summary, artefact_paths\n\n\ndef scenario_cross_track_limits(\n    scenario: Mapping[str, object]\n) -> tuple[float, float, float | None]:\n    \"\"\"Return the primary, waiver, and optional plane-intersection limits.\"\"\"\n\n    primary_limit = float(os.getenv(\"FSAT_PRIMARY_CROSS_TRACK_LIMIT_KM\", \"30.0\"))\n    waiver_limit = float(os.getenv(\"FSAT_WAIVER_CROSS_TRACK_LIMIT_KM\", \"70.0\"))\n    plane_limit_env = os.getenv(\"FSAT_PLANE_INTERSECTION_LIMIT_KM\")\n    plane_limit: float | None = (\n        float(plane_limit_env) if plane_limit_env is not None else None\n    )\n\n    limits = scenario.get(\"cross_track_limits\")\n    if isinstance(limits, Mapping):\n        if \"primary_km\" in limits:\n            try:\n                primary_limit = float(limits[\"primary_km\"])\n            except (TypeError, ValueError):\n                pass\n        if \"waiver_km\" in limits:\n            try:\n                waiver_limit = float(limits[\"waiver_km\"])\n            except (TypeError, ValueError):\n                pass\n        plane_candidate = limits.get(\"plane_intersection_limit_km\")\n        if plane_candidate is not None:\n            try:\n                plane_limit = float(plane_candidate)\n            except (TypeError, ValueError):\n                pass\n    else:\n        requirements = scenario.get(\"alignment_requirements\")\n        if isinstance(requirements, Mapping):\n            primary_limit = float(\n                requirements.get(\"primary_cross_track_limit_km\", primary_limit)\n            )\n            waiver_limit = float(\n                requirements.get(\"waiver_cross_track_limit_km\", waiver_limit)\n            )\n            plane_candidate = requirements.get(\"plane_intersection_limit_km\", plane_limit)\n            if plane_candidate is not None:\n                plane_limit = float(plane_candidate)\n\n    return primary_limit, waiver_limit, plane_limit\n\n\ndef scenario_access_window(\n    scenario: Mapping[str, object]\n) -> tuple[datetime | None, datetime | None, datetime | None]:\n    \"\"\"Return start, end, and midpoint timestamps for the primary access window.\"\"\"\n\n    window = scenario.get(\"access_window\")\n    start = _parse_time(window.get(\"start_utc\")) if isinstance(window, Mapping) else None\n    end = _parse_time(window.get(\"end_utc\")) if isinstance(window, Mapping) else None\n    midpoint = (\n        _parse_time(window.get(\"midpoint_utc\")) if isinstance(window, Mapping) else None\n    )\n\n    timing = (\n        scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    )\n    windows = timing.get(\"daily_access_windows\")\n    if start is None or end is None:\n        if isinstance(windows, Sequence) and windows:\n            primary = windows[0] if isinstance(windows[0], Mapping) else {}\n            if start is None:\n                start = _parse_time(primary.get(\"start_utc\"))\n            if end is None:\n                end = _parse_time(primary.get(\"end_utc\"))\n            if midpoint is None:\n                midpoint = _parse_time(primary.get(\"midpoint_utc\"))\n\n    if midpoint is None and start and end:\n        midpoint = start + (end - start) / 2\n    elif midpoint is None and start:\n        midpoint = start + timedelta(seconds=45.0)\n\n    return start, end, midpoint\n\n\ndef _build_summary(\n    deterministic_metrics: MutableMapping[str, object],\n    monte_carlo_metrics: MutableMapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Combine deterministic and Monte Carlo outputs into a summary map.\"\"\"\n\n    orbital_period = deterministic_metrics.get(\"orbital_period_s\", 0.0)\n    two_body_period = deterministic_metrics.get(\"two_body_period_s\", orbital_period)\n    period_delta = float(orbital_period) - float(two_body_period)\n\n    return {\n        \"model\": \"high_fidelity_j2_drag\",\n        \"orbital_period_s\": float(orbital_period),\n        \"period_delta_s\": float(period_delta),\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": float(settings.time_step_s),\n            \"drag_coefficient\": float(settings.drag_coefficient),\n            \"ballistic_coefficient_m2_per_kg\": float(settings.ballistic_coefficient_m2_per_kg),\n            \"solar_flux_index\": float(settings.solar_flux_index),\n            \"monte_carlo_runs\": int(monte_carlo_config.get(\"runs\", 0)),\n        },\n        \"cross_track\": deterministic_metrics.get(\"cross_track\", {}),\n        \"altitude_range_m\": deterministic_metrics.get(\"altitude_range_m\", {}),\n        \"plane_intersection\": deterministic_metrics.get(\"plane_intersection\", {}),\n        \"monte_carlo\": monte_carlo_metrics,\n    }\n\n\ndef _default_settings(scenario: Mapping[str, object]) -> PropagatorSettings:\n    \"\"\"Return baseline propagation settings derived from the scenario.\"\"\"\n\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    window_start, window_end, window_midpoint = scenario_access_window(scenario)\n    orbital_elements = scenario.get(\"orbital_elements\", {})\n    reference_start = window_start or window_midpoint\n    if reference_start is None:\n        reference_start = _parse_time(\n            orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n        )\n    if reference_start is None:\n        reference_start = datetime(2026, 3, 21, 0, 0, 0, tzinfo=timezone.utc)\n\n    epoch_time = _parse_time(\n        orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n    ) or reference_start\n\n    planning = timing.get(\"planning_horizon\", {}) if isinstance(timing.get(\"planning_horizon\"), Mapping) else {}\n    planning_stop = _parse_time(planning.get(\"stop_utc\"))\n\n    propagation = timing.get(\"propagation\") if isinstance(timing.get(\"propagation\"), Mapping) else {}\n    margin_s = 300.0\n    try:\n        margin_candidate = float(propagation.get(\"margin_s\", margin_s))\n        margin_s = max(margin_candidate, 120.0)\n    except (TypeError, ValueError):\n        margin_s = max(margin_s, 120.0)\n    margin = timedelta(seconds=margin_s)\n\n    start_override = _parse_time(propagation.get(\"start_utc\")) if propagation else None\n    end_override = _parse_time(propagation.get(\"end_utc\")) if propagation else None\n    time_step = 10.0\n    try:\n        time_step = float(propagation.get(\"time_step_s\", time_step))\n    except (TypeError, ValueError):\n        time_step = 10.0\n\n    analysis_start = start_override or (reference_start - margin)\n    if window_start and analysis_start > window_start - margin:\n        analysis_start = window_start - margin\n\n    candidate_stop = end_override\n    if candidate_stop is None:\n        if window_end:\n            candidate_stop = window_end + margin\n        elif window_midpoint:\n            candidate_stop = window_midpoint + margin\n        else:\n            candidate_stop = analysis_start + timedelta(minutes=10)\n\n    if planning_stop:\n        candidate_stop = min(candidate_stop, planning_stop)\n\n    if candidate_stop <= analysis_start:\n        candidate_stop = analysis_start + timedelta(minutes=10)\n\n    evaluation_time = window_midpoint or reference_start\n\n    primary_limit, waiver_limit, plane_limit = scenario_cross_track_limits(scenario)\n\n    return PropagatorSettings(\n        start_time=analysis_start,\n        epoch_time=epoch_time,\n        stop_time=candidate_stop,\n        time_step_s=time_step,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit,\n        waiver_cross_track_limit_km=waiver_limit,\n        plane_intersection_limit_km=plane_limit,\n    )\n\n\ndef _normalise_monte_carlo(monte_carlo: Mapping[str, object] | None) -> MutableMapping[str, object]:\n    \"\"\"Return a mutable Monte Carlo configuration with defaults applied.\"\"\"\n\n    if monte_carlo is None:\n        config = dict(DEFAULT_MONTE_CARLO)\n    else:\n        config = dict(DEFAULT_MONTE_CARLO)\n        config.update({key: monte_carlo.get(key, value) for key, value in DEFAULT_MONTE_CARLO.items()})\n        for key, value in monte_carlo.items():\n            if key == \"dispersions\" and isinstance(value, Mapping):\n                default_disp = dict(DEFAULT_MONTE_CARLO.get(\"dispersions\", {}))\n                default_disp.update({k: float(v) for k, v in value.items()})\n                config[\"dispersions\"] = default_disp\n            else:\n                config[key] = value\n\n    dispersions = config.get(\"dispersions\", {})\n    if not isinstance(dispersions, Mapping):\n        dispersions = DEFAULT_MONTE_CARLO[\"dispersions\"]\n    config[\"dispersions\"] = dict(dispersions)\n\n    try:\n        runs = int(config.get(\"runs\", DEFAULT_MONTE_CARLO[\"runs\"]))\n    except (TypeError, ValueError):\n        runs = DEFAULT_MONTE_CARLO[\"runs\"]\n    config[\"runs\"] = max(runs, 500)\n\n    return config\n\n\ndef _resolve_evaluation_time(settings: PropagatorSettings) -> datetime:\n    \"\"\"Return a clamped evaluation time within the propagation span.\"\"\"\n\n    evaluation = settings.evaluation_time or settings.start_time\n    if evaluation < settings.start_time:\n        evaluation = settings.start_time\n    if evaluation > settings.stop_time:\n        evaluation = settings.stop_time\n    return evaluation\n\n\ndef _scenario_orbital_elements(scenario: Mapping[str, object]) -> OrbitalElements:\n    \"\"\"Extract classical elements from the scenario mapping.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, Mapping):\n        fallback = _fallback_orbital_elements(scenario)\n        if fallback is None:\n            raise ValueError(\"Scenario is missing the 'orbital_elements' section.\")\n        return fallback\n\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, Mapping):\n        raise ValueError(\"Scenario orbital elements must include the 'classical' block.\")\n\n    semi_major_axis_m = float(classical.get(\"semi_major_axis_km\", 0.0)) * 1_000.0\n    eccentricity = float(classical.get(\"eccentricity\", 0.0))\n    inclination = math.radians(float(classical.get(\"inclination_deg\", 0.0)))\n    raan = math.radians(float(classical.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(classical.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(classical.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis_m,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _fallback_orbital_elements(\n    scenario: Mapping[str, object]\n) -> OrbitalElements | None:\n    spacecraft = scenario.get(\"spacecraft\")\n    if not isinstance(spacecraft, Sequence) or not spacecraft:\n        return None\n    leader = spacecraft[0]\n    if not isinstance(leader, Mapping):\n        return None\n    initial_state = leader.get(\"initial_state\")\n    if not isinstance(initial_state, Mapping):\n        return None\n\n    semi_major_axis = float(initial_state.get(\"semi_major_axis_m\", 0.0))\n    if semi_major_axis <= 0.0:\n        return None\n    eccentricity = float(initial_state.get(\"eccentricity\", 0.0))\n    inclination = float(initial_state.get(\"inclination_rad\", 0.0))\n    raan = math.radians(float(initial_state.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(initial_state.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(initial_state.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _target_coordinates(scenario: Mapping[str, object]) -> tuple[float, float]:\n    \"\"\"Return the target latitude and longitude in radians.\"\"\"\n\n    metadata = scenario.get(\"metadata\")\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    if isinstance(region, Mapping):\n        latitude = math.radians(float(region.get(\"latitude_deg\", 0.0)))\n        longitude = math.radians(float(region.get(\"longitude_deg\", 0.0)))\n        return latitude, longitude\n\n    return math.radians(35.6892), math.radians(51.3890)\n\n\ndef _initial_states(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    drag_coefficient: float,\n    *,\n    epoch_offset_s: float = 0.0,\n) -> list[SpacecraftState]:\n    \"\"\"Construct initial Cartesian states for the formation.\"\"\"\n\n    elements_at_start = leader_elements\n    if epoch_offset_s:\n        mean_motion = leader_elements.mean_motion()\n        propagated_mean_anomaly = _wrap_angle(\n            leader_elements.mean_anomaly + mean_motion * epoch_offset_s\n        )\n        elements_at_start = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis,\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination,\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=propagated_mean_anomaly,\n        )\n\n    position, velocity = _classical_to_cartesian(elements_at_start)\n    rotation = rotation_matrix_rtn_to_eci(position, velocity)\n    states: list[SpacecraftState] = []\n\n    for entry in formation:\n        identifier = str(entry.get(\"identifier\", \"spacecraft\"))\n        radial_offset = float(entry.get(\"radial_offset_km\", 0.0)) * 1_000.0\n        along_track_offset = float(entry.get(\"along_track_offset_km\", 0.0)) * 1_000.0\n        cross_track_offset = float(entry.get(\"cross_track_offset_km\", 0.0)) * 1_000.0\n\n        offset_rtn = np.array([radial_offset, along_track_offset, cross_track_offset], dtype=float)\n        position_shift = rotation @ offset_rtn\n\n        adjusted_position = position + position_shift\n        adjusted_velocity = velocity.copy()\n\n        states.append(\n            SpacecraftState(\n                identifier=identifier,\n                position_m=adjusted_position.astype(float),\n                velocity_mps=adjusted_velocity.astype(float),\n                drag_coefficient=float(entry.get(\"drag_coefficient\", drag_coefficient)),\n                area_m2=4.0,\n                mass_kg=120.0,\n            )\n        )\n\n    return states\n\n\ndef _classical_to_cartesian(elements: OrbitalElements) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return Cartesian state vectors for *elements* without SciPy dependency.\"\"\"\n\n    # This inline implementation mirrors :func:`constellation.orbit.classical_to_cartesian`\n    # but operates fully in metres to avoid repeated conversions.\n    a = elements.semi_major_axis\n    e = elements.eccentricity\n    i = elements.inclination\n    raan = elements.raan\n    argp = elements.arg_perigee\n    m_anomaly = elements.mean_anomaly\n\n    true_anomaly = _mean_to_true(m_anomaly, e)\n    semi_latus = a * (1.0 - e * e)\n    radius = semi_latus / (1.0 + e * math.cos(true_anomaly))\n\n    position_pf = np.array(\n        [radius * math.cos(true_anomaly), radius * math.sin(true_anomaly), 0.0],\n        dtype=float,\n    )\n    velocity_pf = np.array(\n        [\n            -math.sqrt(MU_EARTH / semi_latus) * math.sin(true_anomaly),\n            math.sqrt(MU_EARTH / semi_latus) * (e + math.cos(true_anomaly)),\n            0.0,\n        ],\n        dtype=float,\n    )\n\n    cos_raan = math.cos(raan)\n    sin_raan = math.sin(raan)\n    cos_argp = math.cos(argp)\n    sin_argp = math.sin(argp)\n    cos_i = math.cos(i)\n    sin_i = math.sin(i)\n\n    rotation = np.array(\n        [\n            [\n                cos_raan * cos_argp - sin_raan * sin_argp * cos_i,\n                -cos_raan * sin_argp - sin_raan * cos_argp * cos_i,\n                sin_raan * sin_i,\n            ],\n            [\n                sin_raan * cos_argp + cos_raan * sin_argp * cos_i,\n                -sin_raan * sin_argp + cos_raan * cos_argp * cos_i,\n                -cos_raan * sin_i,\n            ],\n            [sin_argp * sin_i, cos_argp * sin_i, cos_i],\n        ],\n        dtype=float,\n    )\n\n    position = rotation @ position_pf\n    velocity = rotation @ velocity_pf\n    return position, velocity\n\n\ndef _mean_to_true(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert mean anomaly to true anomaly using Newton iterations.\"\"\"\n\n    if eccentricity < 1.0e-12:\n        return _wrap_angle(mean_anomaly)\n\n    eccentric = mean_anomaly\n    for _ in range(30):\n        residual = eccentric - eccentricity * math.sin(eccentric) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric)\n        step = -residual / derivative\n        eccentric += step\n        if abs(step) < 1.0e-12:\n            break\n\n    sine_half = math.sqrt(1.0 + eccentricity) * math.sin(0.5 * eccentric)\n    cosine_half = math.sqrt(1.0 - eccentricity) * math.cos(0.5 * eccentric)\n    return _wrap_angle(2.0 * math.atan2(sine_half, cosine_half))\n\n\ndef _wrap_angle(angle: float) -> float:\n    \"\"\"Normalise *angle* to the ``[0, 2π)`` interval.\"\"\"\n\n    wrapped = math.fmod(angle, 2.0 * math.pi)\n    return wrapped + 2.0 * math.pi if wrapped < 0.0 else wrapped\n\n\ndef _propagate_deterministic(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n) -> tuple[\n    MutableMapping[str, list[tuple[datetime, dict[str, float]]]],\n    MutableMapping[str, object],\n]:\n    \"\"\"Propagate the deterministic trajectory and capture time histories.\"\"\"\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n    states = _initial_states(\n        leader_elements,\n        formation,\n        settings.drag_coefficient,\n        epoch_offset_s=epoch_offset,\n    )\n    epoch = settings.start_time\n    dt = settings.time_step_s\n\n    times: list[datetime] = []\n    cross_track_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    altitude_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    orbital_elements_series: dict[str, dict[str, list[float]]] = {\n        state.identifier: {field: [] for field in CLASSICAL_ELEMENT_FIELDS} for state in states\n    }\n    metrics = _initial_metric_structure(states, target_lat, target_lon)\n    plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n    relative_stats = {\"max_abs\": 0.0, \"min_abs\": math.inf, \"time_of_min\": None, \"series\": []}\n\n    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    for _ in tqdm(range(n_steps + 1), desc=\"Deterministic propagation\"):\n        times.append(epoch)\n        for state in states:\n            elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n            per_satellite = orbital_elements_series[state.identifier]\n            per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n            per_satellite[\"eccentricity\"].append(elements.eccentricity)\n            per_satellite[\"inclination_deg\"].append(\n                _normalise_angle_degrees(elements.inclination)\n            )\n            per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n            per_satellite[\"argument_of_perigee_deg\"].append(\n                _normalise_angle_degrees(elements.arg_perigee)\n            )\n            per_satellite[\"mean_anomaly_deg\"].append(\n                _normalise_angle_degrees(elements.mean_anomaly)\n            )\n        _record_metrics(\n            states,\n            epoch,\n            target_lat,\n            target_lon,\n            metrics,\n            cross_track_series,\n            altitude_series,\n            relative_stats,\n        )\n\n        if epoch >= settings.stop_time:\n            break\n\n        states = _rk4_step(states, dt, settings)\n        epoch += timedelta(seconds=dt)\n\n    overall_min_abs = 0.0\n    for entry in metrics[\"vehicles\"]:\n        min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n        if math.isfinite(min_abs):\n            overall_min_abs = max(overall_min_abs, min_abs)\n    metrics[\"overall_min_abs_cross_track_km\"] = float(overall_min_abs)\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    evaluation_timestamp = evaluation_time\n    before_index = after_index = 0\n    fraction = 0.0\n    if times:\n        before_index = max(\n            (idx for idx, stamp in enumerate(times) if stamp <= evaluation_time),\n            default=0,\n        )\n        after_index = min(\n            (idx for idx, stamp in enumerate(times) if stamp >= evaluation_time),\n            default=len(times) - 1,\n        )\n        if after_index < before_index:\n            after_index = before_index\n        before_time = times[before_index]\n        after_time = times[after_index]\n        if after_time > before_time:\n            elapsed = (evaluation_time - before_time).total_seconds()\n            span = (after_time - before_time).total_seconds()\n            fraction = max(0.0, min(elapsed / span, 1.0))\n        else:\n            fraction = 0.0\n\n    evaluation_values: dict[str, float] = {}\n    evaluation_abs: dict[str, float] = {}\n    for identifier, series_values in cross_track_series.items():\n        value_before = float(\"nan\")\n        value_after = float(\"nan\")\n        if series_values:\n            clamped_before = min(before_index, len(series_values) - 1)\n            clamped_after = min(after_index, len(series_values) - 1)\n            value_before = float(series_values[clamped_before])\n            value_after = float(series_values[clamped_after])\n            if math.isfinite(value_before) and math.isfinite(value_after):\n                value = value_before + fraction * (value_after - value_before)\n            elif math.isfinite(value_before):\n                value = value_before\n            elif math.isfinite(value_after):\n                value = value_after\n            else:\n                value = float(\"nan\")\n        else:\n            value = float(\"nan\")\n        evaluation_values[identifier] = value\n        evaluation_abs[identifier] = abs(value) if math.isfinite(value) else math.inf\n\n    centroid_value = fmean(evaluation_values.values()) if evaluation_values else math.nan\n    centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n    worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in evaluation_abs.values() if math.isfinite(value)\n    ]\n    waiver_pass_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    primary_pass_fraction = 1.0 if primary_compliant else 0.0\n\n    for entry in metrics[\"vehicles\"]:\n        identifier = str(entry.get(\"identifier\", \"\"))\n        value = evaluation_values.get(identifier, float(\"nan\"))\n        abs_value = evaluation_abs.get(identifier, math.inf)\n        entry[\"cross_track_at_evaluation_km\"] = (\n            float(value) if math.isfinite(value) else None\n        )\n        entry[\"abs_cross_track_at_evaluation_km\"] = (\n            float(abs_value) if math.isfinite(abs_value) else None\n        )\n        entry[\"compliant\"] = bool(\n            math.isfinite(abs_value) and abs_value <= waiver_limit\n        )\n\n    evaluation_timestamp_iso = evaluation_timestamp.isoformat().replace(\"+00:00\", \"Z\")\n    centroid_value_out = float(centroid_value) if math.isfinite(centroid_value) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    metrics[\"evaluation\"] = {\n        \"time_utc\": evaluation_timestamp_iso,\n        \"vehicles\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_values.items()\n        },\n        \"vehicle_abs\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_abs.items()\n        },\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": float(primary_limit),\n        \"waiver_limit_km\": float(waiver_limit),\n        \"primary_pass_fraction\": float(primary_pass_fraction),\n        \"waiver_pass_fraction\": float(waiver_pass_fraction),\n    }\n    metrics[\"centroid_cross_track_km_at_evaluation\"] = centroid_value_out\n    metrics[\"centroid_abs_cross_track_km_at_evaluation\"] = centroid_abs_out\n    metrics[\"overall_abs_cross_track_km_at_evaluation\"] = worst_abs_out\n    metrics[\"primary_compliant\"] = bool(primary_compliant)\n    metrics[\"waiver_compliant\"] = bool(waiver_compliant)\n    metrics[\"primary_limit_km\"] = float(primary_limit)\n    metrics[\"waiver_limit_km\"] = float(waiver_limit)\n    metrics[\"primary_pass_fraction\"] = float(primary_pass_fraction)\n    metrics[\"waiver_pass_fraction\"] = float(waiver_pass_fraction)\n\n    relative_summary = {\n        \"max_abs_km\": float(relative_stats[\"max_abs\"]),\n        \"min_abs_km\": float(relative_stats[\"min_abs\"] if math.isfinite(relative_stats[\"min_abs\"]) else 0.0),\n        \"time_of_min\": relative_stats[\"time_of_min\"],\n        \"series\": list(relative_stats[\"series\"]),\n    }\n\n    altitude_min = min(min(series) for series in altitude_series.values()) if altitude_series else 0.0\n    altitude_max = max(max(series) for series in altitude_series.values()) if altitude_series else 0.0\n\n    deterministic_metrics: MutableMapping[str, object] = {\n        \"orbital_period_s\": float(orbital_period),\n        \"two_body_period_s\": float(orbital_period),\n        \"cross_track\": metrics,\n        \"altitude_range_m\": {\n            \"min\": float(altitude_min),\n            \"max\": float(altitude_max),\n        },\n    }\n    centroid_summary = metrics.get(\"centroid\", {})\n    if isinstance(centroid_summary, Mapping):\n        deterministic_metrics[\"centroid_cross_track\"] = {\n            \"min_cross_track_km\": float(centroid_summary.get(\"min_cross_track_km\", math.inf)),\n            \"min_abs_cross_track_km\": float(\n                centroid_summary.get(\"min_abs_cross_track_km\", math.inf)\n            ),\n            \"time_of_min_abs_cross_track\": centroid_summary.get(\n                \"time_of_min_abs_cross_track\"\n            ),\n            \"vehicle_cross_track_km\": centroid_summary.get(\n                \"vehicle_cross_track_km_at_min\", {}\n            ),\n            \"vehicle_abs_cross_track_km\": centroid_summary.get(\n                \"vehicle_abs_cross_track_km_at_min\", {}\n            ),\n            \"worst_vehicle_abs_cross_track_km\": float(\n                centroid_summary.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n            ),\n            \"evaluation_cross_track_km\": centroid_value_out,\n            \"evaluation_abs_cross_track_km\": centroid_abs_out,\n            \"worst_vehicle_abs_cross_track_km_at_evaluation\": worst_abs_out,\n            \"evaluation_time_utc\": evaluation_timestamp_iso,\n            \"primary_limit_km\": float(primary_limit),\n            \"waiver_limit_km\": float(waiver_limit),\n            \"primary_compliant\": bool(primary_compliant),\n            \"waiver_compliant\": bool(waiver_compliant),\n        }\n    plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n    plane_limit = settings.plane_intersection_limit_km\n    if plane_limit is not None:\n        plane_metrics[\"limit_km\"] = float(plane_limit)\n        if math.isfinite(plane_distance):\n            plane_metrics[\"compliant\"] = bool(plane_distance <= plane_limit)\n        else:\n            plane_metrics[\"compliant\"] = False\n    else:\n        plane_metrics[\"compliant\"] = None\n    deterministic_metrics[\"plane_intersection\"] = plane_metrics\n    deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    deterministic_metrics[\"cross_track_limits_km\"] = {\n        \"primary\": float(primary_limit),\n        \"waiver\": float(waiver_limit),\n    }\n    deterministic_metrics[\"cross_track_evaluation\"] = metrics[\"evaluation\"]\n    deterministic_metrics[\"orbital_elements\"] = {\n        \"time_series\": {\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n            \"artefact_key\": \"deterministic_orbital_elements_csv\",\n        }\n    }\n\n    series = {\n        \"samples\": [\n            (\n                timestamp,\n                {identifier: cross_track_series[identifier][index] for identifier in cross_track_series},\n            )\n            for index, timestamp in enumerate(times)\n        ],\n        \"altitudes\": {\n            identifier: altitude_series[identifier] for identifier in altitude_series\n        },\n        \"orbital_elements\": {\n            \"times\": list(times),\n            \"per_satellite\": {\n                identifier: {\n                    field: list(values)\n                    for field, values in per_satellite.items()\n                }\n                for identifier, per_satellite in orbital_elements_series.items()\n            },\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n        },\n    }\n\n    return series, deterministic_metrics\n\n\ndef _initial_metric_structure(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Construct the dictionary used to record cross-track statistics.\"\"\"\n\n    vehicle_metrics: list[MutableMapping[str, object]] = []\n    for state in states:\n        vehicle_metrics.append(\n            {\n                \"identifier\": state.identifier,\n                \"target_latitude_deg\": math.degrees(target_lat),\n                \"target_longitude_deg\": math.degrees(target_lon),\n                \"max_cross_track_km\": -math.inf,\n                \"min_cross_track_km\": math.inf,\n                \"max_abs_cross_track_km\": -math.inf,\n                \"min_abs_cross_track_km\": math.inf,\n                \"time_of_max_abs_cross_track\": None,\n                \"time_of_min_abs_cross_track\": None,\n                \"pass_count\": 0,\n                \"compliant\": False,\n            }\n        )\n\n    return {\n        \"vehicles\": vehicle_metrics,\n        \"overall_max_abs_cross_track_km\": -math.inf,\n        \"overall_min_abs_cross_track_km\": math.inf,\n        \"centroid\": {\n            \"min_cross_track_km\": math.inf,\n            \"min_abs_cross_track_km\": math.inf,\n            \"time_of_min_abs_cross_track\": None,\n            \"vehicle_cross_track_km_at_min\": {},\n            \"vehicle_abs_cross_track_km_at_min\": {},\n            \"worst_vehicle_abs_cross_track_km\": math.inf,\n        },\n    }\n\n\ndef _normalise_angle_degrees(angle_rad: float) -> float:\n    \"\"\"Convert radians to wrapped degrees in the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _plane_intersection_metrics(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Compute the plane intersection proximity to the target.\"\"\"\n\n    plane_normals: dict[str, list[np.ndarray]] = {}\n    for state in states:\n        plane = PLANE_ASSIGNMENTS.get(state.identifier, \"Plane A\")\n        h_vec = np.cross(state.position_m, state.velocity_mps)\n        norm = np.linalg.norm(h_vec)\n        if norm == 0.0:\n            continue\n        plane_normals.setdefault(plane, []).append(h_vec / norm)\n\n    averaged_normals: dict[str, np.ndarray] = {}\n    for plane, vectors in plane_normals.items():\n        if not vectors:\n            continue\n        stacked = np.vstack(vectors)\n        mean_vec = np.mean(stacked, axis=0)\n        norm = np.linalg.norm(mean_vec)\n        if norm == 0.0:\n            continue\n        averaged_normals[plane] = mean_vec / norm\n\n    plane_a = averaged_normals.get(\"Plane A\")\n    plane_b = averaged_normals.get(\"Plane B\")\n    if plane_a is None or plane_b is None:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    line_direction = np.cross(plane_a, plane_b)\n    norm_line = np.linalg.norm(line_direction)\n    if norm_line == 0.0:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    unit_direction = line_direction / norm_line\n    candidates = [unit_direction, -unit_direction]\n    candidate_metrics: list[MutableMapping[str, float]] = []\n    min_distance = math.inf\n\n    for candidate in candidates:\n        surface_point = candidate * EARTH_EQUATORIAL_RADIUS_M\n        lat, lon, _ = geodetic_coordinates(surface_point)\n        distance = haversine_distance(lat, lon, target_lat, target_lon)\n        min_distance = min(min_distance, distance)\n        candidate_metrics.append(\n            {\n                \"latitude_deg\": math.degrees(lat),\n                \"longitude_deg\": math.degrees(lon),\n                \"distance_to_target_km\": distance / 1_000.0,\n            }\n        )\n\n    return {\n        \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n        \"intersection_candidates\": candidate_metrics,\n        \"target_distance_km\": min_distance / 1_000.0,\n    }\n\n\ndef _record_metrics(\n    states: Sequence[SpacecraftState],\n    epoch: datetime,\n    target_lat: float,\n    target_lon: float,\n    metrics: MutableMapping[str, object],\n    cross_track_series: MutableMapping[str, list[float]],\n    altitude_series: MutableMapping[str, list[float]],\n    relative_stats: MutableMapping[str, object],\n) -> None:\n    \"\"\"Update cross-track statistics for the current epoch.\"\"\"\n\n    vehicle_entries = metrics[\"vehicles\"]\n    centroid_metrics = metrics.get(\"centroid\", {})\n    epoch_values: dict[str, float] = {}\n\n    for index, state in enumerate(states):\n        position_ecef = inertial_to_ecef(state.position_m, epoch)\n        latitude, longitude, altitude = geodetic_coordinates(position_ecef)\n        cross_track_distance_m = haversine_distance(latitude, longitude, target_lat, target_lon)\n        sign = 1.0 if latitude >= target_lat else -1.0\n        cross_track_km = sign * (cross_track_distance_m / 1_000.0)\n\n        cross_track_series[state.identifier].append(float(cross_track_km))\n        altitude_series[state.identifier].append(float(altitude))\n        epoch_values[state.identifier] = float(cross_track_km)\n\n        entry = vehicle_entries[index]\n        entry[\"max_cross_track_km\"] = max(entry[\"max_cross_track_km\"], cross_track_km)\n        entry[\"min_cross_track_km\"] = min(entry[\"min_cross_track_km\"], cross_track_km)\n\n        abs_value = abs(cross_track_km)\n        if abs_value > entry[\"max_abs_cross_track_km\"]:\n            entry[\"max_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_max_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        if abs_value < entry[\"min_abs_cross_track_km\"]:\n            entry[\"min_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_min_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n\n        previous_series = cross_track_series[state.identifier]\n        if len(previous_series) >= 2 and previous_series[-2] == 0.0:\n            entry[\"pass_count\"] += 1\n        elif len(previous_series) >= 2 and previous_series[-2] * cross_track_km < 0.0:\n            entry[\"pass_count\"] += 1\n\n        metrics[\"overall_max_abs_cross_track_km\"] = max(\n            metrics[\"overall_max_abs_cross_track_km\"], abs_value\n        )\n        metrics[\"overall_min_abs_cross_track_km\"] = min(\n            metrics[\"overall_min_abs_cross_track_km\"], abs_value\n        )\n\n    if epoch_values and isinstance(centroid_metrics, MutableMapping):\n        centroid_value = fmean(epoch_values.values())\n        abs_centroid = abs(centroid_value)\n        if abs_centroid < centroid_metrics.get(\"min_abs_cross_track_km\", math.inf):\n            centroid_metrics[\"min_cross_track_km\"] = float(centroid_value)\n            centroid_metrics[\"min_abs_cross_track_km\"] = float(abs_centroid)\n            centroid_metrics[\"time_of_min_abs_cross_track\"] = (\n                epoch.isoformat().replace(\"+00:00\", \"Z\")\n            )\n            centroid_metrics[\"vehicle_cross_track_km_at_min\"] = {\n                identifier: float(value) for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"vehicle_abs_cross_track_km_at_min\"] = {\n                identifier: float(abs(value))\n                for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"worst_vehicle_abs_cross_track_km\"] = float(\n                max(abs(value) for value in epoch_values.values())\n            )\n\n    leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n    plane_b_state = next(\n        (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n        None,\n    )\n    if leader_state is not None and plane_b_state is not None:\n        normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n        norm_normal = np.linalg.norm(normal_vec)\n        if norm_normal > 0.0:\n            normal_unit = normal_vec / norm_normal\n            relative_vec = plane_b_state.position_m - leader_state.position_m\n            cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n            abs_val = abs(cross_track_km)\n            if abs_val > relative_stats[\"max_abs\"]:\n                relative_stats[\"max_abs\"] = abs_val\n            if abs_val < relative_stats[\"min_abs\"]:\n                relative_stats[\"min_abs\"] = abs_val\n                relative_stats[\"time_of_min\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n            relative_stats[\"series\"].append(\n                (epoch.isoformat().replace(\"+00:00\", \"Z\"), cross_track_km)\n            )\n\n\ndef _rk4_step(states: Sequence[SpacecraftState], dt: float, settings: PropagatorSettings) -> list[SpacecraftState]:\n    \"\"\"Advance the spacecraft states by *dt* seconds using RK4 integration.\"\"\"\n\n    next_states: list[SpacecraftState] = []\n    for state in states:\n        x0 = state.position_m\n        v0 = state.velocity_mps\n\n        a1 = _acceleration(x0, v0, state, settings)\n        k1_pos = v0\n        k1_vel = a1\n\n        a2 = _acceleration(x0 + 0.5 * dt * k1_pos, v0 + 0.5 * dt * k1_vel, state, settings)\n        k2_pos = v0 + 0.5 * dt * k1_vel\n        k2_vel = a2\n\n        a3 = _acceleration(x0 + 0.5 * dt * k2_pos, v0 + 0.5 * dt * k2_vel, state, settings)\n        k3_pos = v0 + 0.5 * dt * k2_vel\n        k3_vel = a3\n\n        a4 = _acceleration(x0 + dt * k3_pos, v0 + dt * k3_vel, state, settings)\n        k4_pos = v0 + dt * k3_vel\n        k4_vel = a4\n\n        new_position = x0 + (dt / 6.0) * (k1_pos + 2.0 * k2_pos + 2.0 * k3_pos + k4_pos)\n        new_velocity = v0 + (dt / 6.0) * (k1_vel + 2.0 * k2_vel + 2.0 * k3_vel + k4_vel)\n\n        state.position_m = new_position\n        state.velocity_mps = new_velocity\n        next_states.append(state)\n\n    return next_states\n\n\ndef _acceleration(\n    position: np.ndarray,\n    velocity: np.ndarray,\n    state: SpacecraftState,\n    settings: PropagatorSettings,\n) -> np.ndarray:\n    r\"\"\"Return acceleration including central gravity, \\(J_2\\), and drag.\"\"\"\n\n    r_vec = position\n    v_vec = velocity\n    r_norm = np.linalg.norm(r_vec)\n\n    if r_norm == 0.0:\n        raise ValueError(\"State vector has zero magnitude; cannot propagate.\")\n\n    mu = MU_EARTH\n    central = -mu * r_vec / (r_norm**3)\n\n    z2 = r_vec[2] ** 2\n    r2 = r_norm**2\n    factor = 1.5 * J2_COEFFICIENT * mu * (EARTH_EQUATORIAL_RADIUS_M**2) / (r_norm**5)\n    coef_xy = factor * (5.0 * z2 / r2 - 1.0)\n    coef_z = factor * (5.0 * z2 / r2 - 3.0)\n    accel_j2 = np.array([coef_xy * r_vec[0], coef_xy * r_vec[1], coef_z * r_vec[2]], dtype=float)\n\n    v_rel = v_vec - np.cross(OMEGA_EARTH_VECTOR, r_vec)\n    speed_rel = np.linalg.norm(v_rel)\n    altitude = r_norm - EARTH_EQUATORIAL_RADIUS_M\n    density = _atmospheric_density(altitude, settings.solar_flux_index)\n    drag = np.zeros(3, dtype=float)\n\n    if speed_rel > 0.0 and density > 0.0:\n        ballistic = state.area_m2 / state.mass_kg\n        drag = (\n            -0.5\n            * density\n            * state.drag_coefficient\n            * ballistic\n            * speed_rel\n            * v_rel\n        )\n\n    return central + accel_j2 + drag\n\n\ndef _atmospheric_density(altitude_m: float, solar_index: float) -> float:\n    \"\"\"Return a simple exponential atmospheric density profile.\"\"\"\n\n    base_density = 3.614e-11  # kg/m^3 at 400 km reference\n    scale_height = 55_000.0 * math.sqrt(max(solar_index, 1.0) / SOLAR_FLUX_BASE)\n    density = base_density * math.exp(-(altitude_m - 400_000.0) / scale_height)\n    return max(density, 0.0)\n\n\ndef _run_monte_carlo(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n    monte_carlo: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Execute Monte Carlo dispersions and aggregate cross-track statistics.\"\"\"\n\n    if not monte_carlo.get(\"enabled\", True):\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n\n    runs = int(monte_carlo.get(\"runs\", 0))\n    if runs <= 0:\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n    runs = max(runs, 500)\n\n    dispersions = monte_carlo.get(\"dispersions\", {})\n    sigma_a = float(dispersions.get(\"semi_major_axis_sigma_m\", 0.0))\n    sigma_i = math.radians(float(dispersions.get(\"inclination_sigma_deg\", 0.0)))\n    sigma_cd = float(dispersions.get(\"drag_coefficient_sigma\", 0.0))\n\n    seed = int(monte_carlo.get(\"seed\", 42))\n    rng = np.random.default_rng(seed)\n\n    vehicle_ids = [entry.get(\"identifier\", \"spacecraft\") for entry in formation]\n    run_metrics_max: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_min: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_eval: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    intersection_distances: list[float] = []\n    relative_max_list: list[float] = []\n    relative_min_list: list[float] = []\n    centroid_abs_list: list[float] = []\n    worst_abs_list: list[float] = []\n\n    primary_success_count = 0\n    waiver_success_count = 0\n    relative_success_count = 0\n    plane_success_count = 0\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    plane_limit = settings.plane_intersection_limit_km\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n\n    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):\n        dispersed_elements = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis + rng.normal(0.0, sigma_a),\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination + rng.normal(0.0, sigma_i),\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=leader_elements.mean_anomaly,\n        )\n\n        drag_scale = 1.0 + rng.normal(0.0, sigma_cd)\n        states = _initial_states(\n            dispersed_elements,\n            formation,\n            settings.drag_coefficient * drag_scale,\n            epoch_offset_s=epoch_offset,\n        )\n        plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n        plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n\n        epoch = settings.start_time\n        dt = settings.time_step_s\n        vehicle_metrics = _initial_metric_structure(states, target_lat, target_lon)\n        previous_signs = {state.identifier: 0.0 for state in states}\n        relative_run_stats = {\"max\": 0.0, \"min\": math.inf}\n        evaluation_snapshot: dict[str, float] = {}\n        evaluation_best = math.inf\n\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            current_values: dict[str, float] = {}\n            for idx, state in enumerate(states):\n                position_ecef = inertial_to_ecef(state.position_m, epoch)\n                latitude, longitude, _ = geodetic_coordinates(position_ecef)\n                distance = haversine_distance(latitude, longitude, target_lat, target_lon)\n                sign = 1.0 if latitude >= target_lat else -1.0\n                cross_track_km = sign * (distance / 1_000.0)\n\n                vehicle_entry = vehicle_metrics[\"vehicles\"][idx]\n                vehicle_entry[\"max_cross_track_km\"] = max(\n                    vehicle_entry[\"max_cross_track_km\"], cross_track_km\n                )\n                vehicle_entry[\"min_cross_track_km\"] = min(\n                    vehicle_entry[\"min_cross_track_km\"], cross_track_km\n                )\n                abs_value = abs(cross_track_km)\n                if abs_value > vehicle_entry[\"max_abs_cross_track_km\"]:\n                    vehicle_entry[\"max_abs_cross_track_km\"] = abs_value\n                if abs_value < vehicle_entry[\"min_abs_cross_track_km\"]:\n                    vehicle_entry[\"min_abs_cross_track_km\"] = abs_value\n\n                current_values[str(state.identifier)] = float(cross_track_km)\n\n                identifier = str(state.identifier)\n                previous_sign = previous_signs[identifier]\n                if previous_sign != 0.0 and previous_sign * cross_track_km < 0.0:\n                    vehicle_entry[\"pass_count\"] += 1\n                previous_signs[identifier] = cross_track_km\n\n            diff = abs((epoch - evaluation_time).total_seconds())\n            if diff < evaluation_best:\n                evaluation_best = diff\n                evaluation_snapshot = dict(current_values)\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n\n            leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n            plane_b_state = next(\n                (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n                None,\n            )\n            if leader_state is not None and plane_b_state is not None:\n                normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n                norm_normal = np.linalg.norm(normal_vec)\n                if norm_normal > 0.0:\n                    normal_unit = normal_vec / norm_normal\n                    relative_vec = plane_b_state.position_m - leader_state.position_m\n                    cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n                    abs_val = abs(cross_track_km)\n                    if abs_val > relative_run_stats[\"max\"]:\n                        relative_run_stats[\"max\"] = abs_val\n                    if abs_val < relative_run_stats[\"min\"]:\n                        relative_run_stats[\"min\"] = abs_val\n\n        evaluation_abs = {\n            identifier: abs(value) if math.isfinite(value) else math.inf\n            for identifier, value in evaluation_snapshot.items()\n        }\n        centroid_value = (\n            fmean(evaluation_snapshot.values()) if evaluation_snapshot else math.nan\n        )\n        centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n        worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n        primary_compliant = (\n            math.isfinite(centroid_abs)\n            and centroid_abs <= primary_limit\n            and math.isfinite(worst_abs)\n            and worst_abs <= waiver_limit\n        )\n        waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n\n        if math.isfinite(centroid_abs):\n            centroid_abs_list.append(centroid_abs)\n        if math.isfinite(worst_abs):\n            worst_abs_list.append(worst_abs)\n\n        for entry in vehicle_metrics[\"vehicles\"]:\n            identifier = str(entry[\"identifier\"])\n            max_abs = float(entry[\"max_abs_cross_track_km\"])\n            min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n            value = evaluation_snapshot.get(identifier, float(\"nan\"))\n            abs_value = evaluation_abs.get(identifier, math.inf)\n            entry[\"cross_track_at_evaluation_km\"] = (\n                float(value) if math.isfinite(value) else None\n            )\n            entry[\"abs_cross_track_at_evaluation_km\"] = (\n                float(abs_value) if math.isfinite(abs_value) else None\n            )\n            entry[\"compliant\"] = bool(\n                math.isfinite(abs_value) and abs_value <= waiver_limit\n            )\n            run_metrics_max[identifier].append(max_abs)\n            if math.isfinite(min_abs):\n                run_metrics_min[identifier].append(min_abs)\n            if math.isfinite(abs_value):\n                run_metrics_eval[identifier].append(abs_value)\n\n        plane_success = True\n        if math.isfinite(plane_distance):\n            if plane_limit is not None:\n                intersection_distances.append(plane_distance)\n                plane_success = plane_distance <= plane_limit\n        elif plane_limit is not None:\n            plane_success = False\n\n        if plane_success:\n            plane_success_count += 1\n\n        if math.isfinite(relative_run_stats[\"max\"]):\n            relative_max_list.append(relative_run_stats[\"max\"])\n        if math.isfinite(relative_run_stats[\"min\"]):\n            relative_min_list.append(relative_run_stats[\"min\"])\n\n        if primary_compliant:\n            primary_success_count += 1\n        if waiver_compliant:\n            waiver_success_count += 1\n        if relative_run_stats[\"max\"] <= waiver_limit:\n            relative_success_count += 1\n\n    aggregated: MutableMapping[str, object] = {\n        \"runs\": runs,\n        \"seed\": seed,\n        \"max_abs_cross_track_km\": {},\n        \"min_abs_cross_track_km\": {},\n        \"evaluation_abs_cross_track_km\": {},\n        \"relative_cross_track_km\": {},\n        \"compliance\": {\n            \"primary_fraction\": float(primary_success_count / runs) if runs else 0.0,\n            \"waiver_fraction\": float(waiver_success_count / runs) if runs else 0.0,\n            \"relative_fraction\": float(relative_success_count / runs) if runs else 0.0,\n        },\n        \"fleet_compliance_probability\": float(primary_success_count / runs)\n        if runs\n        else 0.0,\n        \"absolute_cross_track_compliance_probability\": float(\n            waiver_success_count / runs\n        )\n        if runs\n        else 0.0,\n        \"evaluation_time_utc\": evaluation_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"cross_track_limits_km\": {\n            \"primary\": float(primary_limit),\n            \"waiver\": float(waiver_limit),\n        },\n    }\n\n    if plane_limit is not None:\n        aggregated[\"plane_intersection_distance_km\"] = {}\n\n    if plane_limit is not None:\n        aggregated[\"compliance\"][\"plane_fraction\"] = float(\n            plane_success_count / runs\n        ) if runs else 0.0\n\n    for identifier, values in run_metrics_max.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"max_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_min.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"min_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_eval.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"evaluation_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if plane_limit is not None and intersection_distances:\n        array = np.asarray(intersection_distances, dtype=float)\n        aggregated[\"plane_intersection_distance_km\"][\"fleet\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if relative_max_list:\n        array = np.asarray(relative_max_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_max\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n    if relative_min_list:\n        array = np.asarray(relative_min_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_min\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if centroid_abs_list:\n        array = np.asarray(centroid_abs_list, dtype=float)\n        centroid_stats = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n        aggregated[\"centroid_abs_cross_track_km\"] = centroid_stats\n        aggregated[\"centroid_abs_cross_track_km_mean\"] = centroid_stats[\"mean\"]\n        aggregated[\"centroid_abs_cross_track_km_p95\"] = centroid_stats[\"p95\"]\n    if worst_abs_list:\n        array = np.asarray(worst_abs_list, dtype=float)\n        aggregated[\"worst_vehicle_abs_cross_track_km\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    return aggregated\n\n\ndef _write_outputs(\n    output_directory: Path,\n    series: Mapping[str, object],\n    deterministic_metrics: Mapping[str, object],\n    monte_carlo_metrics: Mapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, str]:\n    \"\"\"Persist deterministic and Monte Carlo outputs to the run directory.\"\"\"\n\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    deterministic_path = output_directory / \"deterministic_cross_track.csv\"\n    relative_path = output_directory / \"relative_cross_track.csv\"\n    summary_path = output_directory / \"deterministic_summary.json\"\n    orbital_path = output_directory / \"deterministic_orbital_elements.csv\"\n    monte_carlo_path = output_directory / \"monte_carlo_cross_track.csv\"\n    monte_carlo_summary_path = output_directory / \"monte_carlo_summary.json\"\n    settings_path = output_directory / \"solver_settings.json\"\n\n    _write_cross_track_csv(deterministic_path, series)\n    orbital_series = series.get(\"orbital_elements\") if isinstance(series, Mapping) else {}\n    _write_orbital_elements_csv(orbital_path, orbital_series)\n\n    relative_summary = deterministic_metrics.get(\"relative_cross_track\")\n    if isinstance(relative_summary, Mapping):\n        _write_relative_cross_track_csv(relative_path, relative_summary)\n        relative_summary = dict(relative_summary)\n        relative_summary.pop(\"series\", None)\n        deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    else:\n        _write_relative_cross_track_csv(relative_path, {})\n\n    summary_payload = {\n        \"metrics\": deterministic_metrics,\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": settings.time_step_s,\n            \"drag_coefficient\": settings.drag_coefficient,\n            \"ballistic_coefficient_m2_per_kg\": settings.ballistic_coefficient_m2_per_kg,\n            \"solar_flux_index\": settings.solar_flux_index,\n        },\n    }\n    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding=\"utf-8\")\n\n    _write_monte_carlo_csv(monte_carlo_path, monte_carlo_metrics)\n    monte_carlo_summary_path.write_text(json.dumps(monte_carlo_metrics, indent=2), encoding=\"utf-8\")\n\n    settings_payload = {\n        \"propagator\": {\n            \"integrator\": \"fixed_step_rk4\",\n            \"time_step_s\": settings.time_step_s,\n            \"force_models\": {\n                \"central_body\": True,\n                \"j2\": True,\n                \"atmospheric_drag\": {\n                    \"model\": \"exponential\",\n                    \"solar_flux_index\": settings.solar_flux_index,\n                },\n            },\n        },\n        \"monte_carlo\": {\n            \"runs\": monte_carlo_config.get(\"runs\", 0),\n            \"seed\": monte_carlo_config.get(\"seed\", 0),\n            \"dispersions\": monte_carlo_config.get(\"dispersions\", {}),\n        },\n        \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n    settings_path.write_text(json.dumps(settings_payload, indent=2), encoding=\"utf-8\")\n\n    orbital_metrics = deterministic_metrics.get(\"orbital_elements\")\n    if isinstance(orbital_metrics, Mapping):\n        time_series_meta = orbital_metrics.get(\"time_series\")\n        if isinstance(time_series_meta, Mapping):\n            time_series_meta[\"artefact\"] = str(orbital_path)\n\n    return {\n        \"deterministic_cross_track_csv\": str(deterministic_path),\n        \"deterministic_orbital_elements_csv\": str(orbital_path),\n        \"deterministic_summary_json\": str(summary_path),\n        \"monte_carlo_cross_track_csv\": str(monte_carlo_path),\n        \"relative_cross_track_csv\": str(relative_path),\n        \"monte_carlo_summary_json\": str(monte_carlo_summary_path),\n        \"solver_settings_json\": str(settings_path),\n    }\n\n\ndef _write_cross_track_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Write deterministic cross-track histories to *path*.\"\"\"\n\n    samples = series.get(\"samples\", [])\n    if not samples:\n        path.write_text(\"time,\\n\", encoding=\"utf-8\")\n        return\n\n    vehicle_ids = sorted(next(iter(samples))[1])\n    header = [\"time_iso\"] + vehicle_ids\n    lines = [\",\".join(header)]\n\n    for timestamp, cross_track in samples:\n        row = [timestamp.isoformat().replace(\"+00:00\", \"Z\")]\n        for identifier in vehicle_ids:\n            row.append(f\"{cross_track.get(identifier, 0.0):.6f}\")\n        lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_orbital_elements_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Persist orbital-element histories to *path*.\"\"\"\n\n    header = \"time_iso,satellite_id,\" + \",\".join(CLASSICAL_ELEMENT_FIELDS)\n    if not isinstance(series, Mapping) or not series:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    times = list(series.get(\"times\", []))\n    per_satellite = series.get(\"per_satellite\")\n    if not isinstance(per_satellite, Mapping) or not times:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [header]\n    for index, timestamp in enumerate(times):\n        if isinstance(timestamp, datetime):\n            stamp = timestamp.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        else:\n            stamp = str(timestamp)\n        for identifier in sorted(per_satellite):\n            elements = per_satellite.get(identifier)\n            if not isinstance(elements, Mapping):\n                continue\n            row = [stamp, str(identifier)]\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                value = float(\"nan\")\n                if isinstance(values, Sequence) and index < len(values):\n                    try:\n                        value = float(values[index])  # type: ignore[arg-type]\n                    except (TypeError, ValueError):\n                        value = float(\"nan\")\n                row.append(f\"{value:.9g}\" if math.isfinite(value) else \"\")\n            lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_monte_carlo_csv(path: Path, metrics: Mapping[str, object]) -> None:\n    \"\"\"Write Monte Carlo aggregate metrics to *path*.\"\"\"\n\n    vehicle_stats = metrics.get(\"max_abs_cross_track_km\", {})\n    min_stats_all = metrics.get(\"min_abs_cross_track_km\", {})\n    plane_stats = metrics.get(\"plane_intersection_distance_km\", {})\n    relative_stats = metrics.get(\"relative_cross_track_km\", {})\n    if (\n        not vehicle_stats\n        and not min_stats_all\n        and not plane_stats\n        and not relative_stats\n    ):\n        path.write_text(\n            \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n            \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n            \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n            \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n            \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n            \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\\n\",\n            encoding=\"utf-8\",\n        )\n        return\n\n    header = (\n        \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n        \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n        \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n        \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n        \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n        \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\"\n    )\n    lines = [header]\n\n    identifiers = sorted(\n        set(\n            list(vehicle_stats.keys())\n            + list(min_stats_all.keys())\n            + list(plane_stats.keys())\n            + list(relative_stats.keys())\n        )\n    )\n\n    for identifier in identifiers:\n        stats = vehicle_stats.get(identifier, {})\n        min_stats = min_stats_all.get(identifier, {})\n        plane_entry = plane_stats.get(identifier, {})\n        relative_entry = relative_stats.get(identifier, {})\n        line = \",\".join(\n            [\n                str(identifier),\n                f\"{min_stats.get('mean', 0.0):.6f}\",\n                f\"{min_stats.get('std', 0.0):.6f}\",\n                f\"{min_stats.get('p95', 0.0):.6f}\",\n                f\"{min_stats.get('min', 0.0):.6f}\",\n                f\"{min_stats.get('max', 0.0):.6f}\",\n                f\"{stats.get('mean', 0.0):.6f}\",\n                f\"{stats.get('std', 0.0):.6f}\",\n                f\"{stats.get('p95', 0.0):.6f}\",\n                f\"{stats.get('min', 0.0):.6f}\",\n                f\"{stats.get('max', 0.0):.6f}\",\n                f\"{plane_entry.get('mean', 0.0):.6f}\",\n                f\"{plane_entry.get('std', 0.0):.6f}\",\n                f\"{plane_entry.get('p95', 0.0):.6f}\",\n                f\"{plane_entry.get('min', 0.0):.6f}\",\n                f\"{plane_entry.get('max', 0.0):.6f}\",\n                f\"{relative_entry.get('mean', 0.0):.6f}\",\n                f\"{relative_entry.get('std', 0.0):.6f}\",\n                f\"{relative_entry.get('p95', 0.0):.6f}\",\n                f\"{relative_entry.get('min', 0.0):.6f}\",\n                f\"{relative_entry.get('max', 0.0):.6f}\",\n            ]\n        )\n        lines.append(line)\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_relative_cross_track_csv(path: Path, summary: Mapping[str, object]) -> None:\n    \"\"\"Write relative cross-track series to *path*.\"\"\"\n\n    series = summary.get(\"series\") if isinstance(summary, Mapping) else None\n    if not series:\n        path.write_text(\"time_iso,relative_cross_track_km\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [\"time_iso,relative_cross_track_km\"]\n    for time_iso, value in series:\n        lines.append(f\"{time_iso},{float(value):.6f}\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _parse_time(value: object) -> datetime | None:\n    \"\"\"Parse an ISO-8601 timestamp into an aware :class:`datetime`.\"\"\"\n\n    if isinstance(value, str):\n        try:\n            if value.endswith(\"Z\"):\n                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n            parsed = datetime.fromisoformat(value)\n            if parsed.tzinfo is None:\n                return parsed.replace(tzinfo=timezone.utc)\n            return parsed.astimezone(timezone.utc)\n        except ValueError:\n            return None\n    return None\n\n\n__all__ = [\n    \"PropagatorSettings\",\n    \"propagate_constellation\",\n    \"scenario_cross_track_limits\",\n    \"scenario_access_window\",\n]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "I'll add a progress bar to `_propagate_deterministic` and `_run_monte_carlo`. Instead of replacing the `while` loop in `_propagate_deterministic`, I'll create a `tqdm` object before the loop and update it within each iteration; it's cleaner. The `for` loop in `_run_monte_carlo` will be a simple `tqdm` wrap. I'll start with `_propagate_deterministic`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            times.append(epoch)\n            for state in states:\n                elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                per_satellite = orbital_elements_series[state.identifier]\n                per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                per_satellite[\"inclination_deg\"].append(\n                    _normalise_angle_degrees(elements.inclination)\n                )\n                per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                per_satellite[\"argument_of_perigee_deg\"].append(\n                    _normalise_angle_degrees(elements.arg_perigee)\n                )\n                per_satellite[\"mean_anomaly_deg\"].append(\n                    _normalise_angle_degrees(elements.mean_anomaly)\n                )\n            _record_metrics(\n                states,\n                epoch,\n                target_lat,\n                target_lon,\n                metrics,\n                cross_track_series,\n                altitude_series,\n                relative_stats,\n            )\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n            pbar.update(1)",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "old_string": "    while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n        times.append(epoch)\n        for state in states:\n            elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n            per_satellite = orbital_elements_series[state.identifier]\n            per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n            per_satellite[\"eccentricity\"].append(elements.eccentricity)\n            per_satellite[\"inclination_deg\"].append(\n                _normalise_angle_degrees(elements.inclination)\n            )\n            per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n            per_satellite[\"argument_of_perigee_deg\"].append(\n                _normalise_angle_degrees(elements.arg_perigee)\n            )\n            per_satellite[\"mean_anomaly_deg\"].append(\n                _normalise_angle_degrees(elements.mean_anomaly)\n            )\n        _record_metrics(\n            states,\n            epoch,\n            target_lat,\n            target_lon,\n            metrics,\n            cross_track_series,\n            altitude_series,\n            relative_stats,\n        )\n\n        if epoch >= settings.stop_time:\n            break\n\n        states = _rk4_step(states, dt, settings)\n        epoch += timedelta(seconds=dt)",
            "instruction": "Add a tqdm progress bar to the deterministic propagation loop using a with statement."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793406932-1db2570fba1b7",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjMB4/H/XiWqdHij04rkWj/sMDmWlCS63srcW3KqX5X70oZbBmTUBpE5/+6SaDdV4jeJT/QKVgHj8f9eUtzB/NBvmI1iv6WzuClHPsp/igtMoUZAj6+ZVEaKal5gzmcjg2RqygFvM41T72Kxv9iPNJnF9jXW94+vqqeKAWg2E15ochx10fDnsVoUVgrV",
        "text": "Okay, adding a progress bar to `_run_monte_carlo` now."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop.",
            "new_string": "    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "old_string": "    for _ in range(runs):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793458859-9fbbbded2d443",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the target line `    for _ in range(runs):` was not found. Upon inspecting the file, it appears the intended change (adding the `tqdm` progress bar) has already been applied. The line `    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):` is already present in the file. Therefore, no further changes are required.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    for _ in range(runs):). Original old_string was (    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiUB4/H/XkIvsnPRbTHrNzDX/QyCtRD+vHqOCGj6PlAXWAfn7Ee/CmoB4/H/XgedfzxjSrch+rlV9YsAlru8/yLaK/l3ABOYvnobpOpZC6+WMDl/ABS6Mnb0KRWtkDbpN8Bdy+wxnaPgS27wcSQNE0iMAzgP8HQ1dmOhD4Y7A/C1Mf8og6d01kdHbS40joN2rffbClsB4/H/Xi+L2j4LTnccTZkXEXNWJutvlxMHuDRRTUX+aXnB8XkjXOxPXKVG61N/9wGkAT20FY6r4gXs7k6vpSbazcYwY1QCuUr/kou+wp3sh84Xd/QZ92qKZxj8Cp0BAePx/15U5c21+0/CP6PodO6JBj6hoboGyahARedOQs1jRjrcYvOI96gP2zEZGoKPYdne4mTzYH8FZCcWO7Bf70ocXKa/BfYCsHz/5e+XemdWSsDurSJMJknUxq7TwVQUZNvf31bWwi2pSZr5nZWdaskVwu+fKQnRM6hE1GD5AxPMc+cMfTLjd7SOuvPRwzbdl28jkDyDuIIfIKLrpwqHAQHj8f9eFXQw6Zd+B3KXy03uWk/HK2c9lKg+zcScocnylYkZ78H9FaQyTGEyx5tZp4CjOsvr6jhCG9xCR5X1NNZKYdevLvMq+LVnmtckoqwZcjd2yanJzesaAlaaLjMpu3w7ofjpBXcY7L8J11TakccdjMbvQuXa2Ffej9Z4rimTpQqzGco6GQrCAQHj8f9e9f0RHX4xCa6T6xhCP5Mn+zIHRdj0Z+7l6qvMmYXcTjz47O1F1cc7qWzPWR391xQ1w0mvrRZKg6VbY83oD5evY8Szeb5sUAmX1wAw+v0nNtQaxKlhr7/WP/sfMqw0uOurAC5vhsX3Q0WgxMe/M8ee/7iR5ppRxapCwuzk/EYGwfIdS3hQWap4pZC/MIpwkgG/EuG7Lk2gsNafz9vlUPAVx3Otu4CtKPTzqoqgd0wnEwVYkr3R5+js6PiQyHZVCrQBAePx/17cbSJZdHyg2kpjIU52ZJSdga68BMQH2z9QWC5uD1EF/+sTwyBhS0fSimNhhcTdlr38tJL5dNZtq55Oijy0J1QHC1WwPFKvaJwcw0h5SonUVM5mx1WsmkG1ZN3tZ6tkj2WrdBkLPe07xHkABPa+z63zIGSh7cH5ZGzx7Mq803R5FIi/0Y7EH3NznAO+p7oApi6MNO685iwskIKmx6QW13azzrOZoNPvILkWrC178XdNCqUBAePx/17h4lFhcXYqV2mCqjOrAIP5CNGrB0uz9718u7jUQgGDDLbuw7mh4gwnt5veiYB8B1F5mXx+gxc1e1Rp+OroPgcmp8jqNeqaweeY3rngi8CMQ7VMsUF8I9uroBApf0HIe9DCZwab+1+azKeIutGu9MZeV2d+Zi9Ir4Cyy0zOckWPsiu+cVgxfOj1YyRGFAK5+XKmp2F+DerKH2POvwsLTVDF",
        "text": "The `replace` tool is failing. I'll try again, but this time I'll use a larger `old_string` context to ensure uniqueness. I'll replace the existing `for` loop with a `tqdm` version, including the preceding `LOGGER.info` call for better targeting. I'm hoping this will resolve the issue."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "old_string": "    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793469806-88cb90d1735fa",
          "name": "replace",
          "response": {
            "error": "Failed to generate JSON content: Resource has been exhausted (e.g. check quota)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XiUbRwNq6JYOE12ba4aEThLfPvd6sA5t5EfbS/RCgnUKegHj8f9eauBlfW+ctf/R8N7iEUUIUhQg/3n4Ld0v43Za689Wfz7HMN3JQqP3kqiL1eTt9aZQZpFRff5qOpbY67Z+xcsvcNQsvwZ390D5gV0nwuQH5r97YHdxQ1waO86MLlxpIT0oZZMkayt8bdsRnTr/zW0bGwuBLjNdCl8B4/H/XqYWsnP6z+roUCKIFMwlEuf9yJVky5bb1hTopqjgFCxrN/S6gPzeXHgX9/cSSYEQAoUBZZ3bn1h0OO41i9sKthkS1UWfA6KntbTyK4odpV7ZM9ipX1eYrq7GKA==",
        "text": "Tool failed, resource error. Retrying same command."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop.",
            "old_string": "    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793487758-ec9f820bbc2d1",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the `replace` content was already present in the file. The `from tqdm import tqdm` line and the modified `for` loop were already applied. Therefore, no changes are required.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):). Original old_string was (    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiUB4/H/XhRfeB/lJjgooKBXm4wam1vHNitvTTmTEDHwVtlFEPqRCmwB4/H/XloKTAIPWC/HM0iIJY+v77+u6dkcXxW2oQTGPmuBLvviEt/i94c6mWrn5nO4qUaVjQvVKlytvTw3UOMAHpOYZSLimya/n7lGMhP9Pwg9Cgao6/Pvol83o6h/fkntHtP3HLTPLv1u7S4KbwHj8f9eTLZ3/iqzN4RT+wjxjYHlOW0asqUfnJBUiTmcEWZSdwhG3wctjcNPZVifTIIdOa+X++wvr15Ezx9KJt6GVpeHPAbHNlpY278MJFLFfcP1LffFK1lBYMnmUqTQNLhOikACdLvzjD8P3RgUrwqFAQHj8f9e+eei40lAJWuZwCViVu7eZEcZKhw4E3zeyFi8gNTK41UdXHNPqBj9e1JohQPuIgnzDkOUNA7kNps8i04qtOqrV+drHDq9zbiiHwnrlAFzfZ5xuzyDTS9yIlhOV3oeUazUixi71jBMO1PBFsNIVX7o6BP35yb58MIMGMk/arbjmlI=",
        "text": "The `replace` tool is failing me; it's not seeing the changes I know are there. I'll re-read `sim/scripts/perturbation_analysis.py` to confirm its content."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762793524859-b6e7126b955d6",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"High-fidelity constellation propagation with \\(J_2\\) and drag perturbations.\n\nThe helper functions defined here extend the lightweight scenario pipeline by\nintroducing a deterministic force model and a Monte Carlo dispersion analysis\ntailored to the Tehran daily pass configuration.  The propagated states capture\ncross-track offsets for every spacecraft in the formation, enabling direct\nverification of the ±30 km primary alignment tolerance (and the ±70 km waiver\nceiling) that governs MR-2 and SRD-P-001.  Outputs are serialisable to JSON and\ncomma-separated value (CSV) formats so they remain compatible with Systems Tool\nKit (STK 11.2) validation workflows as required by the repository guidelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Mapping, MutableMapping, Sequence\n\nimport os\n\nimport numpy as np\nfrom statistics import fmean\n\nfrom src.constellation.frames import rotation_matrix_rtn_to_eci\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    EARTH_ROTATION_RATE,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\n\n\nJ2_COEFFICIENT = 1.08262668e-3\nSOLAR_FLUX_BASE = 150.0\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\n\nDEFAULT_FORMATION = (\n    {\n        \"identifier\": \"FSAT-LDR\",\n        \"role\": \"leader\",\n        \"radial_offset_km\": 0.0,\n        \"along_track_offset_km\": 0.0,\n        \"cross_track_offset_km\": 0.0,\n    },\n    {\n        \"identifier\": \"FSAT-DP1\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": -0.05,\n        \"along_track_offset_km\": 20.0,\n        \"cross_track_offset_km\": 0.2,\n    },\n    {\n        \"identifier\": \"FSAT-DP2\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": 0.05,\n        \"along_track_offset_km\": -20.0,\n        \"cross_track_offset_km\": -0.2,\n    },\n)\n\nPLANE_ASSIGNMENTS = {\n    \"FSAT-LDR\": \"Plane A\",\n    \"FSAT-DP1\": \"Plane A\",\n    \"FSAT-DP2\": \"Plane B\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nCLASSICAL_ELEMENT_UNITS = {\n    \"semi_major_axis_km\": \"km\",\n    \"eccentricity\": \"\",\n    \"inclination_deg\": \"deg\",\n    \"raan_deg\": \"deg\",\n    \"argument_of_perigee_deg\": \"deg\",\n    \"mean_anomaly_deg\": \"deg\",\n}\n\n\nDEFAULT_MONTE_CARLO = {\n    \"enabled\": True,\n    \"runs\": 1000,\n    \"seed\": 4242,\n    \"dispersions\": {\n        \"semi_major_axis_sigma_m\": 5.0,\n        \"inclination_sigma_deg\": 0.01,\n        \"drag_coefficient_sigma\": 0.05,\n    },\n}\n\n\n@dataclass\nclass PropagatorSettings:\n    \"\"\"Container describing the deterministic propagation configuration.\"\"\"\n\n    start_time: datetime\n    epoch_time: datetime\n    stop_time: datetime\n    time_step_s: float\n    drag_coefficient: float\n    ballistic_coefficient_m2_per_kg: float\n    solar_flux_index: float\n    evaluation_time: datetime | None = None\n    primary_cross_track_limit_km: float = 30.0\n    waiver_cross_track_limit_km: float = 70.0\n    plane_intersection_limit_km: float | None = None\n\n\n@dataclass\nclass SpacecraftState:\n    \"\"\"Cartesian state and drag properties for a spacecraft.\"\"\"\n\n    identifier: str\n    position_m: np.ndarray\n    velocity_mps: np.ndarray\n    drag_coefficient: float\n    area_m2: float\n    mass_kg: float\n\n\ndef propagate_constellation(\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Path | None = None,\n    formation: Sequence[Mapping[str, float | str]] = DEFAULT_FORMATION,\n    settings: PropagatorSettings | None = None,\n    monte_carlo: Mapping[str, object] | None = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation including \\(J_2\\) and atmospheric drag.\"\"\"\n\n    resolved_settings = settings or _default_settings(scenario)\n    monte_carlo_config = _normalise_monte_carlo(monte_carlo)\n\n    target_lat, target_lon = _target_coordinates(scenario)\n    leader_elements = _scenario_orbital_elements(scenario)\n\n    deterministic_series, deterministic_metrics = _propagate_deterministic(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n    )\n\n    monte_carlo_metrics = _run_monte_carlo(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n        monte_carlo_config,\n    )\n\n    artefact_paths: MutableMapping[str, str] = {}\n    if output_directory:\n        artefact_paths = _write_outputs(\n            Path(output_directory),\n            deterministic_series,\n            deterministic_metrics,\n            monte_carlo_metrics,\n            resolved_settings,\n            monte_carlo_config,\n        )\n\n    summary = _build_summary(\n        deterministic_metrics,\n        monte_carlo_metrics,\n        resolved_settings,\n        monte_carlo_config,\n    )\n\n    return summary, artefact_paths\n\n\ndef scenario_cross_track_limits(\n    scenario: Mapping[str, object]\n) -> tuple[float, float, float | None]:\n    \"\"\"Return the primary, waiver, and optional plane-intersection limits.\"\"\"\n\n    primary_limit = float(os.getenv(\"FSAT_PRIMARY_CROSS_TRACK_LIMIT_KM\", \"30.0\"))\n    waiver_limit = float(os.getenv(\"FSAT_WAIVER_CROSS_TRACK_LIMIT_KM\", \"70.0\"))\n    plane_limit_env = os.getenv(\"FSAT_PLANE_INTERSECTION_LIMIT_KM\")\n    plane_limit: float | None = (\n        float(plane_limit_env) if plane_limit_env is not None else None\n    )\n\n    limits = scenario.get(\"cross_track_limits\")\n    if isinstance(limits, Mapping):\n        if \"primary_km\" in limits:\n            try:\n                primary_limit = float(limits[\"primary_km\"])\n            except (TypeError, ValueError):\n                pass\n        if \"waiver_km\" in limits:\n            try:\n                waiver_limit = float(limits[\"waiver_km\"])\n            except (TypeError, ValueError):\n                pass\n        plane_candidate = limits.get(\"plane_intersection_limit_km\")\n        if plane_candidate is not None:\n            try:\n                plane_limit = float(plane_candidate)\n            except (TypeError, ValueError):\n                pass\n    else:\n        requirements = scenario.get(\"alignment_requirements\")\n        if isinstance(requirements, Mapping):\n            primary_limit = float(\n                requirements.get(\"primary_cross_track_limit_km\", primary_limit)\n            )\n            waiver_limit = float(\n                requirements.get(\"waiver_cross_track_limit_km\", waiver_limit)\n            )\n            plane_candidate = requirements.get(\"plane_intersection_limit_km\", plane_limit)\n            if plane_candidate is not None:\n                plane_limit = float(plane_candidate)\n\n    return primary_limit, waiver_limit, plane_limit\n\n\ndef scenario_access_window(\n    scenario: Mapping[str, object]\n) -> tuple[datetime | None, datetime | None, datetime | None]:\n    \"\"\"Return start, end, and midpoint timestamps for the primary access window.\"\"\"\n\n    window = scenario.get(\"access_window\")\n    start = _parse_time(window.get(\"start_utc\")) if isinstance(window, Mapping) else None\n    end = _parse_time(window.get(\"end_utc\")) if isinstance(window, Mapping) else None\n    midpoint = (\n        _parse_time(window.get(\"midpoint_utc\")) if isinstance(window, Mapping) else None\n    )\n\n    timing = (\n        scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    )\n    windows = timing.get(\"daily_access_windows\")\n    if start is None or end is None:\n        if isinstance(windows, Sequence) and windows:\n            primary = windows[0] if isinstance(windows[0], Mapping) else {}\n            if start is None:\n                start = _parse_time(primary.get(\"start_utc\"))\n            if end is None:\n                end = _parse_time(primary.get(\"end_utc\"))\n            if midpoint is None:\n                midpoint = _parse_time(primary.get(\"midpoint_utc\"))\n\n    if midpoint is None and start and end:\n        midpoint = start + (end - start) / 2\n    elif midpoint is None and start:\n        midpoint = start + timedelta(seconds=45.0)\n\n    return start, end, midpoint\n\n\ndef _build_summary(\n    deterministic_metrics: MutableMapping[str, object],\n    monte_carlo_metrics: MutableMapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Combine deterministic and Monte Carlo outputs into a summary map.\"\"\"\n\n    orbital_period = deterministic_metrics.get(\"orbital_period_s\", 0.0)\n    two_body_period = deterministic_metrics.get(\"two_body_period_s\", orbital_period)\n    period_delta = float(orbital_period) - float(two_body_period)\n\n    return {\n        \"model\": \"high_fidelity_j2_drag\",\n        \"orbital_period_s\": float(orbital_period),\n        \"period_delta_s\": float(period_delta),\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": float(settings.time_step_s),\n            \"drag_coefficient\": float(settings.drag_coefficient),\n            \"ballistic_coefficient_m2_per_kg\": float(settings.ballistic_coefficient_m2_per_kg),\n            \"solar_flux_index\": float(settings.solar_flux_index),\n            \"monte_carlo_runs\": int(monte_carlo_config.get(\"runs\", 0)),\n        },\n        \"cross_track\": deterministic_metrics.get(\"cross_track\", {}),\n        \"altitude_range_m\": deterministic_metrics.get(\"altitude_range_m\", {}),\n        \"plane_intersection\": deterministic_metrics.get(\"plane_intersection\", {}),\n        \"monte_carlo\": monte_carlo_metrics,\n    }\n\n\ndef _default_settings(scenario: Mapping[str, object]) -> PropagatorSettings:\n    \"\"\"Return baseline propagation settings derived from the scenario.\"\"\"\n\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    window_start, window_end, window_midpoint = scenario_access_window(scenario)\n    orbital_elements = scenario.get(\"orbital_elements\", {})\n    reference_start = window_start or window_midpoint\n    if reference_start is None:\n        reference_start = _parse_time(\n            orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n        )\n    if reference_start is None:\n        reference_start = datetime(2026, 3, 21, 0, 0, 0, tzinfo=timezone.utc)\n\n    epoch_time = _parse_time(\n        orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n    ) or reference_start\n\n    planning = timing.get(\"planning_horizon\", {}) if isinstance(timing.get(\"planning_horizon\"), Mapping) else {}\n    planning_stop = _parse_time(planning.get(\"stop_utc\"))\n\n    propagation = timing.get(\"propagation\") if isinstance(timing.get(\"propagation\"), Mapping) else {}\n    margin_s = 300.0\n    try:\n        margin_candidate = float(propagation.get(\"margin_s\", margin_s))\n        margin_s = max(margin_candidate, 120.0)\n    except (TypeError, ValueError):\n        margin_s = max(margin_s, 120.0)\n    margin = timedelta(seconds=margin_s)\n\n    start_override = _parse_time(propagation.get(\"start_utc\")) if propagation else None\n    end_override = _parse_time(propagation.get(\"end_utc\")) if propagation else None\n    time_step = 10.0\n    try:\n        time_step = float(propagation.get(\"time_step_s\", time_step))\n    except (TypeError, ValueError):\n        time_step = 10.0\n\n    analysis_start = start_override or (reference_start - margin)\n    if window_start and analysis_start > window_start - margin:\n        analysis_start = window_start - margin\n\n    candidate_stop = end_override\n    if candidate_stop is None:\n        if window_end:\n            candidate_stop = window_end + margin\n        elif window_midpoint:\n            candidate_stop = window_midpoint + margin\n        else:\n            candidate_stop = analysis_start + timedelta(minutes=10)\n\n    if planning_stop:\n        candidate_stop = min(candidate_stop, planning_stop)\n\n    if candidate_stop <= analysis_start:\n        candidate_stop = analysis_start + timedelta(minutes=10)\n\n    evaluation_time = window_midpoint or reference_start\n\n    primary_limit, waiver_limit, plane_limit = scenario_cross_track_limits(scenario)\n\n    return PropagatorSettings(\n        start_time=analysis_start,\n        epoch_time=epoch_time,\n        stop_time=candidate_stop,\n        time_step_s=time_step,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit,\n        waiver_cross_track_limit_km=waiver_limit,\n        plane_intersection_limit_km=plane_limit,\n    )\n\n\ndef _normalise_monte_carlo(monte_carlo: Mapping[str, object] | None) -> MutableMapping[str, object]:\n    \"\"\"Return a mutable Monte Carlo configuration with defaults applied.\"\"\"\n\n    if monte_carlo is None:\n        config = dict(DEFAULT_MONTE_CARLO)\n    else:\n        config = dict(DEFAULT_MONTE_CARLO)\n        config.update({key: monte_carlo.get(key, value) for key, value in DEFAULT_MONTE_CARLO.items()})\n        for key, value in monte_carlo.items():\n            if key == \"dispersions\" and isinstance(value, Mapping):\n                default_disp = dict(DEFAULT_MONTE_CARLO.get(\"dispersions\", {}))\n                default_disp.update({k: float(v) for k, v in value.items()})\n                config[\"dispersions\"] = default_disp\n            else:\n                config[key] = value\n\n    dispersions = config.get(\"dispersions\", {})\n    if not isinstance(dispersions, Mapping):\n        dispersions = DEFAULT_MONTE_CARLO[\"dispersions\"]\n    config[\"dispersions\"] = dict(dispersions)\n\n    try:\n        runs = int(config.get(\"runs\", DEFAULT_MONTE_CARLO[\"runs\"]))\n    except (TypeError, ValueError):\n        runs = DEFAULT_MONTE_CARLO[\"runs\"]\n    config[\"runs\"] = max(runs, 500)\n\n    return config\n\n\ndef _resolve_evaluation_time(settings: PropagatorSettings) -> datetime:\n    \"\"\"Return a clamped evaluation time within the propagation span.\"\"\"\n\n    evaluation = settings.evaluation_time or settings.start_time\n    if evaluation < settings.start_time:\n        evaluation = settings.start_time\n    if evaluation > settings.stop_time:\n        evaluation = settings.stop_time\n    return evaluation\n\n\ndef _scenario_orbital_elements(scenario: Mapping[str, object]) -> OrbitalElements:\n    \"\"\"Extract classical elements from the scenario mapping.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, Mapping):\n        fallback = _fallback_orbital_elements(scenario)\n        if fallback is None:\n            raise ValueError(\"Scenario is missing the 'orbital_elements' section.\")\n        return fallback\n\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, Mapping):\n        raise ValueError(\"Scenario orbital elements must include the 'classical' block.\")\n\n    semi_major_axis_m = float(classical.get(\"semi_major_axis_km\", 0.0)) * 1_000.0\n    eccentricity = float(classical.get(\"eccentricity\", 0.0))\n    inclination = math.radians(float(classical.get(\"inclination_deg\", 0.0)))\n    raan = math.radians(float(classical.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(classical.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(classical.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis_m,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _fallback_orbital_elements(\n    scenario: Mapping[str, object]\n) -> OrbitalElements | None:\n    spacecraft = scenario.get(\"spacecraft\")\n    if not isinstance(spacecraft, Sequence) or not spacecraft:\n        return None\n    leader = spacecraft[0]\n    if not isinstance(leader, Mapping):\n        return None\n    initial_state = leader.get(\"initial_state\")\n    if not isinstance(initial_state, Mapping):\n        return None\n\n    semi_major_axis = float(initial_state.get(\"semi_major_axis_m\", 0.0))\n    if semi_major_axis <= 0.0:\n        return None\n    eccentricity = float(initial_state.get(\"eccentricity\", 0.0))\n    inclination = float(initial_state.get(\"inclination_rad\", 0.0))\n    raan = math.radians(float(initial_state.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(initial_state.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(initial_state.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _target_coordinates(scenario: Mapping[str, object]) -> tuple[float, float]:\n    \"\"\"Return the target latitude and longitude in radians.\"\"\"\n\n    metadata = scenario.get(\"metadata\")\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    if isinstance(region, Mapping):\n        latitude = math.radians(float(region.get(\"latitude_deg\", 0.0)))\n        longitude = math.radians(float(region.get(\"longitude_deg\", 0.0)))\n        return latitude, longitude\n\n    return math.radians(35.6892), math.radians(51.3890)\n\n\ndef _initial_states(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    drag_coefficient: float,\n    *,\n    epoch_offset_s: float = 0.0,\n) -> list[SpacecraftState]:\n    \"\"\"Construct initial Cartesian states for the formation.\"\"\"\n\n    elements_at_start = leader_elements\n    if epoch_offset_s:\n        mean_motion = leader_elements.mean_motion()\n        propagated_mean_anomaly = _wrap_angle(\n            leader_elements.mean_anomaly + mean_motion * epoch_offset_s\n        )\n        elements_at_start = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis,\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination,\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=propagated_mean_anomaly,\n        )\n\n    position, velocity = _classical_to_cartesian(elements_at_start)\n    rotation = rotation_matrix_rtn_to_eci(position, velocity)\n    states: list[SpacecraftState] = []\n\n    for entry in formation:\n        identifier = str(entry.get(\"identifier\", \"spacecraft\"))\n        radial_offset = float(entry.get(\"radial_offset_km\", 0.0)) * 1_000.0\n        along_track_offset = float(entry.get(\"along_track_offset_km\", 0.0)) * 1_000.0\n        cross_track_offset = float(entry.get(\"cross_track_offset_km\", 0.0)) * 1_000.0\n\n        offset_rtn = np.array([radial_offset, along_track_offset, cross_track_offset], dtype=float)\n        position_shift = rotation @ offset_rtn\n\n        adjusted_position = position + position_shift\n        adjusted_velocity = velocity.copy()\n\n        states.append(\n            SpacecraftState(\n                identifier=identifier,\n                position_m=adjusted_position.astype(float),\n                velocity_mps=adjusted_velocity.astype(float),\n                drag_coefficient=float(entry.get(\"drag_coefficient\", drag_coefficient)),\n                area_m2=4.0,\n                mass_kg=120.0,\n            )\n        )\n\n    return states\n\n\ndef _classical_to_cartesian(elements: OrbitalElements) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return Cartesian state vectors for *elements* without SciPy dependency.\"\"\"\n\n    # This inline implementation mirrors :func:`constellation.orbit.classical_to_cartesian`\n    # but operates fully in metres to avoid repeated conversions.\n    a = elements.semi_major_axis\n    e = elements.eccentricity\n    i = elements.inclination\n    raan = elements.raan\n    argp = elements.arg_perigee\n    m_anomaly = elements.mean_anomaly\n\n    true_anomaly = _mean_to_true(m_anomaly, e)\n    semi_latus = a * (1.0 - e * e)\n    radius = semi_latus / (1.0 + e * math.cos(true_anomaly))\n\n    position_pf = np.array(\n        [radius * math.cos(true_anomaly), radius * math.sin(true_anomaly), 0.0],\n        dtype=float,\n    )\n    velocity_pf = np.array(\n        [\n            -math.sqrt(MU_EARTH / semi_latus) * math.sin(true_anomaly),\n            math.sqrt(MU_EARTH / semi_latus) * (e + math.cos(true_anomaly)),\n            0.0,\n        ],\n        dtype=float,\n    )\n\n    cos_raan = math.cos(raan)\n    sin_raan = math.sin(raan)\n    cos_argp = math.cos(argp)\n    sin_argp = math.sin(argp)\n    cos_i = math.cos(i)\n    sin_i = math.sin(i)\n\n    rotation = np.array(\n        [\n            [\n                cos_raan * cos_argp - sin_raan * sin_argp * cos_i,\n                -cos_raan * sin_argp - sin_raan * cos_argp * cos_i,\n                sin_raan * sin_i,\n            ],\n            [\n                sin_raan * cos_argp + cos_raan * sin_argp * cos_i,\n                -sin_raan * sin_argp + cos_raan * cos_argp * cos_i,\n                -cos_raan * sin_i,\n            ],\n            [sin_argp * sin_i, cos_argp * sin_i, cos_i],\n        ],\n        dtype=float,\n    )\n\n    position = rotation @ position_pf\n    velocity = rotation @ velocity_pf\n    return position, velocity\n\n\ndef _mean_to_true(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert mean anomaly to true anomaly using Newton iterations.\"\"\"\n\n    if eccentricity < 1.0e-12:\n        return _wrap_angle(mean_anomaly)\n\n    eccentric = mean_anomaly\n    for _ in range(30):\n        residual = eccentric - eccentricity * math.sin(eccentric) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric)\n        step = -residual / derivative\n        eccentric += step\n        if abs(step) < 1.0e-12:\n            break\n\n    sine_half = math.sqrt(1.0 + eccentricity) * math.sin(0.5 * eccentric)\n    cosine_half = math.sqrt(1.0 - eccentricity) * math.cos(0.5 * eccentric)\n    return _wrap_angle(2.0 * math.atan2(sine_half, cosine_half))\n\n\ndef _wrap_angle(angle: float) -> float:\n    \"\"\"Normalise *angle* to the ``[0, 2π)`` interval.\"\"\"\n\n    wrapped = math.fmod(angle, 2.0 * math.pi)\n    return wrapped + 2.0 * math.pi if wrapped < 0.0 else wrapped\n\n\ndef _propagate_deterministic(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n) -> tuple[\n    MutableMapping[str, list[tuple[datetime, dict[str, float]]]],\n    MutableMapping[str, object],\n]:\n    \"\"\"Propagate the deterministic trajectory and capture time histories.\"\"\"\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n    states = _initial_states(\n        leader_elements,\n        formation,\n        settings.drag_coefficient,\n        epoch_offset_s=epoch_offset,\n    )\n    epoch = settings.start_time\n    dt = settings.time_step_s\n\n    times: list[datetime] = []\n    cross_track_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    altitude_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    orbital_elements_series: dict[str, dict[str, list[float]]] = {\n        state.identifier: {field: [] for field in CLASSICAL_ELEMENT_FIELDS} for state in states\n    }\n    metrics = _initial_metric_structure(states, target_lat, target_lon)\n    plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n    relative_stats = {\"max_abs\": 0.0, \"min_abs\": math.inf, \"time_of_min\": None, \"series\": []}\n\n    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            times.append(epoch)\n            for state in states:\n                elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                per_satellite = orbital_elements_series[state.identifier]\n                per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                per_satellite[\"inclination_deg\"].append(\n                    _normalise_angle_degrees(elements.inclination)\n                )\n                per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                per_satellite[\"argument_of_perigee_deg\"].append(\n                    _normalise_angle_degrees(elements.arg_perigee)\n                )\n                per_satellite[\"mean_anomaly_deg\"].append(\n                    _normalise_angle_degrees(elements.mean_anomaly)\n                )\n            _record_metrics(\n                states,\n                epoch,\n                target_lat,\n                target_lon,\n                metrics,\n                cross_track_series,\n                altitude_series,\n                relative_stats,\n            )\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n            pbar.update(1)\n\n    overall_min_abs = 0.0\n    for entry in metrics[\"vehicles\"]:\n        min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n        if math.isfinite(min_abs):\n            overall_min_abs = max(overall_min_abs, min_abs)\n    metrics[\"overall_min_abs_cross_track_km\"] = float(overall_min_abs)\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    evaluation_timestamp = evaluation_time\n    before_index = after_index = 0\n    fraction = 0.0\n    if times:\n        before_index = max(\n            (idx for idx, stamp in enumerate(times) if stamp <= evaluation_time),\n            default=0,\n        )\n        after_index = min(\n            (idx for idx, stamp in enumerate(times) if stamp >= evaluation_time),\n            default=len(times) - 1,\n        )\n        if after_index < before_index:\n            after_index = before_index\n        before_time = times[before_index]\n        after_time = times[after_index]\n        if after_time > before_time:\n            elapsed = (evaluation_time - before_time).total_seconds()\n            span = (after_time - before_time).total_seconds()\n            fraction = max(0.0, min(elapsed / span, 1.0))\n        else:\n            fraction = 0.0\n\n    evaluation_values: dict[str, float] = {}\n    evaluation_abs: dict[str, float] = {}\n    for identifier, series_values in cross_track_series.items():\n        value_before = float(\"nan\")\n        value_after = float(\"nan\")\n        if series_values:\n            clamped_before = min(before_index, len(series_values) - 1)\n            clamped_after = min(after_index, len(series_values) - 1)\n            value_before = float(series_values[clamped_before])\n            value_after = float(series_values[clamped_after])\n            if math.isfinite(value_before) and math.isfinite(value_after):\n                value = value_before + fraction * (value_after - value_before)\n            elif math.isfinite(value_before):\n                value = value_before\n            elif math.isfinite(value_after):\n                value = value_after\n            else:\n                value = float(\"nan\")\n        else:\n            value = float(\"nan\")\n        evaluation_values[identifier] = value\n        evaluation_abs[identifier] = abs(value) if math.isfinite(value) else math.inf\n\n    centroid_value = fmean(evaluation_values.values()) if evaluation_values else math.nan\n    centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n    worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in evaluation_abs.values() if math.isfinite(value)\n    ]\n    waiver_pass_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    primary_pass_fraction = 1.0 if primary_compliant else 0.0\n\n    for entry in metrics[\"vehicles\"]:\n        identifier = str(entry.get(\"identifier\", \"\"))\n        value = evaluation_values.get(identifier, float(\"nan\"))\n        abs_value = evaluation_abs.get(identifier, math.inf)\n        entry[\"cross_track_at_evaluation_km\"] = (\n            float(value) if math.isfinite(value) else None\n        )\n        entry[\"abs_cross_track_at_evaluation_km\"] = (\n            float(abs_value) if math.isfinite(abs_value) else None\n        )\n        entry[\"compliant\"] = bool(\n            math.isfinite(abs_value) and abs_value <= waiver_limit\n        )\n\n    evaluation_timestamp_iso = evaluation_timestamp.isoformat().replace(\"+00:00\", \"Z\")\n    centroid_value_out = float(centroid_value) if math.isfinite(centroid_value) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    metrics[\"evaluation\"] = {\n        \"time_utc\": evaluation_timestamp_iso,\n        \"vehicles\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_values.items()\n        },\n        \"vehicle_abs\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_abs.items()\n        },\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": float(primary_limit),\n        \"waiver_limit_km\": float(waiver_limit),\n        \"primary_pass_fraction\": float(primary_pass_fraction),\n        \"waiver_pass_fraction\": float(waiver_pass_fraction),\n    }\n    metrics[\"centroid_cross_track_km_at_evaluation\"] = centroid_value_out\n    metrics[\"centroid_abs_cross_track_km_at_evaluation\"] = centroid_abs_out\n    metrics[\"overall_abs_cross_track_km_at_evaluation\"] = worst_abs_out\n    metrics[\"primary_compliant\"] = bool(primary_compliant)\n    metrics[\"waiver_compliant\"] = bool(waiver_compliant)\n    metrics[\"primary_limit_km\"] = float(primary_limit)\n    metrics[\"waiver_limit_km\"] = float(waiver_limit)\n    metrics[\"primary_pass_fraction\"] = float(primary_pass_fraction)\n    metrics[\"waiver_pass_fraction\"] = float(waiver_pass_fraction)\n\n    relative_summary = {\n        \"max_abs_km\": float(relative_stats[\"max_abs\"]),\n        \"min_abs_km\": float(relative_stats[\"min_abs\"] if math.isfinite(relative_stats[\"min_abs\"]) else 0.0),\n        \"time_of_min\": relative_stats[\"time_of_min\"],\n        \"series\": list(relative_stats[\"series\"]),\n    }\n\n    altitude_min = min(min(series) for series in altitude_series.values()) if altitude_series else 0.0\n    altitude_max = max(max(series) for series in altitude_series.values()) if altitude_series else 0.0\n\n    deterministic_metrics: MutableMapping[str, object] = {\n        \"orbital_period_s\": float(orbital_period),\n        \"two_body_period_s\": float(orbital_period),\n        \"cross_track\": metrics,\n        \"altitude_range_m\": {\n            \"min\": float(altitude_min),\n            \"max\": float(altitude_max),\n        },\n    }\n    centroid_summary = metrics.get(\"centroid\", {})\n    if isinstance(centroid_summary, Mapping):\n        deterministic_metrics[\"centroid_cross_track\"] = {\n            \"min_cross_track_km\": float(centroid_summary.get(\"min_cross_track_km\", math.inf)),\n            \"min_abs_cross_track_km\": float(\n                centroid_summary.get(\"min_abs_cross_track_km\", math.inf)\n            ),\n            \"time_of_min_abs_cross_track\": centroid_summary.get(\n                \"time_of_min_abs_cross_track\"\n            ),\n            \"vehicle_cross_track_km\": centroid_summary.get(\n                \"vehicle_cross_track_km_at_min\", {}\n            ),\n            \"vehicle_abs_cross_track_km\": centroid_summary.get(\n                \"vehicle_abs_cross_track_km_at_min\", {}\n            ),\n            \"worst_vehicle_abs_cross_track_km\": float(\n                centroid_summary.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n            ),\n            \"evaluation_cross_track_km\": centroid_value_out,\n            \"evaluation_abs_cross_track_km\": centroid_abs_out,\n            \"worst_vehicle_abs_cross_track_km_at_evaluation\": worst_abs_out,\n            \"evaluation_time_utc\": evaluation_timestamp_iso,\n            \"primary_limit_km\": float(primary_limit),\n            \"waiver_limit_km\": float(waiver_limit),\n            \"primary_compliant\": bool(primary_compliant),\n            \"waiver_compliant\": bool(waiver_compliant),\n        }\n    plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n    plane_limit = settings.plane_intersection_limit_km\n    if plane_limit is not None:\n        plane_metrics[\"limit_km\"] = float(plane_limit)\n        if math.isfinite(plane_distance):\n            plane_metrics[\"compliant\"] = bool(plane_distance <= plane_limit)\n        else:\n            plane_metrics[\"compliant\"] = False\n    else:\n        plane_metrics[\"compliant\"] = None\n    deterministic_metrics[\"plane_intersection\"] = plane_metrics\n    deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    deterministic_metrics[\"cross_track_limits_km\"] = {\n        \"primary\": float(primary_limit),\n        \"waiver\": float(waiver_limit),\n    }\n    deterministic_metrics[\"cross_track_evaluation\"] = metrics[\"evaluation\"]\n    deterministic_metrics[\"orbital_elements\"] = {\n        \"time_series\": {\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n            \"artefact_key\": \"deterministic_orbital_elements_csv\",\n        }\n    }\n\n    series = {\n        \"samples\": [\n            (\n                timestamp,\n                {identifier: cross_track_series[identifier][index] for identifier in cross_track_series},\n            )\n            for index, timestamp in enumerate(times)\n        ],\n        \"altitudes\": {\n            identifier: altitude_series[identifier] for identifier in altitude_series\n        },\n        \"orbital_elements\": {\n            \"times\": list(times),\n            \"per_satellite\": {\n                identifier: {\n                    field: list(values)\n                    for field, values in per_satellite.items()\n                }\n                for identifier, per_satellite in orbital_elements_series.items()\n            },\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n        },\n    }\n\n    return series, deterministic_metrics\n\n\ndef _initial_metric_structure(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Construct the dictionary used to record cross-track statistics.\"\"\"\n\n    vehicle_metrics: list[MutableMapping[str, object]] = []\n    for state in states:\n        vehicle_metrics.append(\n            {\n                \"identifier\": state.identifier,\n                \"target_latitude_deg\": math.degrees(target_lat),\n                \"target_longitude_deg\": math.degrees(target_lon),\n                \"max_cross_track_km\": -math.inf,\n                \"min_cross_track_km\": math.inf,\n                \"max_abs_cross_track_km\": -math.inf,\n                \"min_abs_cross_track_km\": math.inf,\n                \"time_of_max_abs_cross_track\": None,\n                \"time_of_min_abs_cross_track\": None,\n                \"pass_count\": 0,\n                \"compliant\": False,\n            }\n        )\n\n    return {\n        \"vehicles\": vehicle_metrics,\n        \"overall_max_abs_cross_track_km\": -math.inf,\n        \"overall_min_abs_cross_track_km\": math.inf,\n        \"centroid\": {\n            \"min_cross_track_km\": math.inf,\n            \"min_abs_cross_track_km\": math.inf,\n            \"time_of_min_abs_cross_track\": None,\n            \"vehicle_cross_track_km_at_min\": {},\n            \"vehicle_abs_cross_track_km_at_min\": {},\n            \"worst_vehicle_abs_cross_track_km\": math.inf,\n        },\n    }\n\n\ndef _normalise_angle_degrees(angle_rad: float) -> float:\n    \"\"\"Convert radians to wrapped degrees in the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _plane_intersection_metrics(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Compute the plane intersection proximity to the target.\"\"\"\n\n    plane_normals: dict[str, list[np.ndarray]] = {}\n    for state in states:\n        plane = PLANE_ASSIGNMENTS.get(state.identifier, \"Plane A\")\n        h_vec = np.cross(state.position_m, state.velocity_mps)\n        norm = np.linalg.norm(h_vec)\n        if norm == 0.0:\n            continue\n        plane_normals.setdefault(plane, []).append(h_vec / norm)\n\n    averaged_normals: dict[str, np.ndarray] = {}\n    for plane, vectors in plane_normals.items():\n        if not vectors:\n            continue\n        stacked = np.vstack(vectors)\n        mean_vec = np.mean(stacked, axis=0)\n        norm = np.linalg.norm(mean_vec)\n        if norm == 0.0:\n            continue\n        averaged_normals[plane] = mean_vec / norm\n\n    plane_a = averaged_normals.get(\"Plane A\")\n    plane_b = averaged_normals.get(\"Plane B\")\n    if plane_a is None or plane_b is None:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    line_direction = np.cross(plane_a, plane_b)\n    norm_line = np.linalg.norm(line_direction)\n    if norm_line == 0.0:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    unit_direction = line_direction / norm_line\n    candidates = [unit_direction, -unit_direction]\n    candidate_metrics: list[MutableMapping[str, float]] = []\n    min_distance = math.inf\n\n    for candidate in candidates:\n        surface_point = candidate * EARTH_EQUATORIAL_RADIUS_M\n        lat, lon, _ = geodetic_coordinates(surface_point)\n        distance = haversine_distance(lat, lon, target_lat, target_lon)\n        min_distance = min(min_distance, distance)\n        candidate_metrics.append(\n            {\n                \"latitude_deg\": math.degrees(lat),\n                \"longitude_deg\": math.degrees(lon),\n                \"distance_to_target_km\": distance / 1_000.0,\n            }\n        )\n\n    return {\n        \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n        \"intersection_candidates\": candidate_metrics,\n        \"target_distance_km\": min_distance / 1_000.0,\n    }\n\n\ndef _record_metrics(\n    states: Sequence[SpacecraftState],\n    epoch: datetime,\n    target_lat: float,\n    target_lon: float,\n    metrics: MutableMapping[str, object],\n    cross_track_series: MutableMapping[str, list[float]],\n    altitude_series: MutableMapping[str, list[float]],\n    relative_stats: MutableMapping[str, object],\n) -> None:\n    \"\"\"Update cross-track statistics for the current epoch.\"\"\"\n\n    vehicle_entries = metrics[\"vehicles\"]\n    centroid_metrics = metrics.get(\"centroid\", {})\n    epoch_values: dict[str, float] = {}\n\n    for index, state in enumerate(states):\n        position_ecef = inertial_to_ecef(state.position_m, epoch)\n        latitude, longitude, altitude = geodetic_coordinates(position_ecef)\n        cross_track_distance_m = haversine_distance(latitude, longitude, target_lat, target_lon)\n        sign = 1.0 if latitude >= target_lat else -1.0\n        cross_track_km = sign * (cross_track_distance_m / 1_000.0)\n\n        cross_track_series[state.identifier].append(float(cross_track_km))\n        altitude_series[state.identifier].append(float(altitude))\n        epoch_values[state.identifier] = float(cross_track_km)\n\n        entry = vehicle_entries[index]\n        entry[\"max_cross_track_km\"] = max(entry[\"max_cross_track_km\"], cross_track_km)\n        entry[\"min_cross_track_km\"] = min(entry[\"min_cross_track_km\"], cross_track_km)\n\n        abs_value = abs(cross_track_km)\n        if abs_value > entry[\"max_abs_cross_track_km\"]:\n            entry[\"max_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_max_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        if abs_value < entry[\"min_abs_cross_track_km\"]:\n            entry[\"min_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_min_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n\n        previous_series = cross_track_series[state.identifier]\n        if len(previous_series) >= 2 and previous_series[-2] == 0.0:\n            entry[\"pass_count\"] += 1\n        elif len(previous_series) >= 2 and previous_series[-2] * cross_track_km < 0.0:\n            entry[\"pass_count\"] += 1\n\n        metrics[\"overall_max_abs_cross_track_km\"] = max(\n            metrics[\"overall_max_abs_cross_track_km\"], abs_value\n        )\n        metrics[\"overall_min_abs_cross_track_km\"] = min(\n            metrics[\"overall_min_abs_cross_track_km\"], abs_value\n        )\n\n    if epoch_values and isinstance(centroid_metrics, MutableMapping):\n        centroid_value = fmean(epoch_values.values())\n        abs_centroid = abs(centroid_value)\n        if abs_centroid < centroid_metrics.get(\"min_abs_cross_track_km\", math.inf):\n            centroid_metrics[\"min_cross_track_km\"] = float(centroid_value)\n            centroid_metrics[\"min_abs_cross_track_km\"] = float(abs_centroid)\n            centroid_metrics[\"time_of_min_abs_cross_track\"] = (\n                epoch.isoformat().replace(\"+00:00\", \"Z\")\n            )\n            centroid_metrics[\"vehicle_cross_track_km_at_min\"] = {\n                identifier: float(value) for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"vehicle_abs_cross_track_km_at_min\"] = {\n                identifier: float(abs(value))\n                for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"worst_vehicle_abs_cross_track_km\"] = float(\n                max(abs(value) for value in epoch_values.values())\n            )\n\n    leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n    plane_b_state = next(\n        (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n        None,\n    )\n    if leader_state is not None and plane_b_state is not None:\n        normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n        norm_normal = np.linalg.norm(normal_vec)\n        if norm_normal > 0.0:\n            normal_unit = normal_vec / norm_normal\n            relative_vec = plane_b_state.position_m - leader_state.position_m\n            cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n            abs_val = abs(cross_track_km)\n            if abs_val > relative_stats[\"max_abs\"]:\n                relative_stats[\"max_abs\"] = abs_val\n            if abs_val < relative_stats[\"min_abs\"]:\n                relative_stats[\"min_abs\"] = abs_val\n                relative_stats[\"time_of_min\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n            relative_stats[\"series\"].append(\n                (epoch.isoformat().replace(\"+00:00\", \"Z\"), cross_track_km)\n            )\n\n\ndef _rk4_step(states: Sequence[SpacecraftState], dt: float, settings: PropagatorSettings) -> list[SpacecraftState]:\n    \"\"\"Advance the spacecraft states by *dt* seconds using RK4 integration.\"\"\"\n\n    next_states: list[SpacecraftState] = []\n    for state in states:\n        x0 = state.position_m\n        v0 = state.velocity_mps\n\n        a1 = _acceleration(x0, v0, state, settings)\n        k1_pos = v0\n        k1_vel = a1\n\n        a2 = _acceleration(x0 + 0.5 * dt * k1_pos, v0 + 0.5 * dt * k1_vel, state, settings)\n        k2_pos = v0 + 0.5 * dt * k1_vel\n        k2_vel = a2\n\n        a3 = _acceleration(x0 + 0.5 * dt * k2_pos, v0 + 0.5 * dt * k2_vel, state, settings)\n        k3_pos = v0 + 0.5 * dt * k2_vel\n        k3_vel = a3\n\n        a4 = _acceleration(x0 + dt * k3_pos, v0 + dt * k3_vel, state, settings)\n        k4_pos = v0 + dt * k3_vel\n        k4_vel = a4\n\n        new_position = x0 + (dt / 6.0) * (k1_pos + 2.0 * k2_pos + 2.0 * k3_pos + k4_pos)\n        new_velocity = v0 + (dt / 6.0) * (k1_vel + 2.0 * k2_vel + 2.0 * k3_vel + k4_vel)\n\n        state.position_m = new_position\n        state.velocity_mps = new_velocity\n        next_states.append(state)\n\n    return next_states\n\n\ndef _acceleration(\n    position: np.ndarray,\n    velocity: np.ndarray,\n    state: SpacecraftState,\n    settings: PropagatorSettings,\n) -> np.ndarray:\n    r\"\"\"Return acceleration including central gravity, \\(J_2\\), and drag.\"\"\"\n\n    r_vec = position\n    v_vec = velocity\n    r_norm = np.linalg.norm(r_vec)\n\n    if r_norm == 0.0:\n        raise ValueError(\"State vector has zero magnitude; cannot propagate.\")\n\n    mu = MU_EARTH\n    central = -mu * r_vec / (r_norm**3)\n\n    z2 = r_vec[2] ** 2\n    r2 = r_norm**2\n    factor = 1.5 * J2_COEFFICIENT * mu * (EARTH_EQUATORIAL_RADIUS_M**2) / (r_norm**5)\n    coef_xy = factor * (5.0 * z2 / r2 - 1.0)\n    coef_z = factor * (5.0 * z2 / r2 - 3.0)\n    accel_j2 = np.array([coef_xy * r_vec[0], coef_xy * r_vec[1], coef_z * r_vec[2]], dtype=float)\n\n    v_rel = v_vec - np.cross(OMEGA_EARTH_VECTOR, r_vec)\n    speed_rel = np.linalg.norm(v_rel)\n    altitude = r_norm - EARTH_EQUATORIAL_RADIUS_M\n    density = _atmospheric_density(altitude, settings.solar_flux_index)\n    drag = np.zeros(3, dtype=float)\n\n    if speed_rel > 0.0 and density > 0.0:\n        ballistic = state.area_m2 / state.mass_kg\n        drag = (\n            -0.5\n            * density\n            * state.drag_coefficient\n            * ballistic\n            * speed_rel\n            * v_rel\n        )\n\n    return central + accel_j2 + drag\n\n\ndef _atmospheric_density(altitude_m: float, solar_index: float) -> float:\n    \"\"\"Return a simple exponential atmospheric density profile.\"\"\"\n\n    base_density = 3.614e-11  # kg/m^3 at 400 km reference\n    scale_height = 55_000.0 * math.sqrt(max(solar_index, 1.0) / SOLAR_FLUX_BASE)\n    density = base_density * math.exp(-(altitude_m - 400_000.0) / scale_height)\n    return max(density, 0.0)\n\n\ndef _run_monte_carlo(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n    monte_carlo: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Execute Monte Carlo dispersions and aggregate cross-track statistics.\"\"\"\n\n    if not monte_carlo.get(\"enabled\", True):\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n\n    runs = int(monte_carlo.get(\"runs\", 0))\n    if runs <= 0:\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n    runs = max(runs, 500)\n\n    dispersions = monte_carlo.get(\"dispersions\", {})\n    sigma_a = float(dispersions.get(\"semi_major_axis_sigma_m\", 0.0))\n    sigma_i = math.radians(float(dispersions.get(\"inclination_sigma_deg\", 0.0)))\n    sigma_cd = float(dispersions.get(\"drag_coefficient_sigma\", 0.0))\n\n    seed = int(monte_carlo.get(\"seed\", 42))\n    rng = np.random.default_rng(seed)\n\n    vehicle_ids = [entry.get(\"identifier\", \"spacecraft\") for entry in formation]\n    run_metrics_max: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_min: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_eval: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    intersection_distances: list[float] = []\n    relative_max_list: list[float] = []\n    relative_min_list: list[float] = []\n    centroid_abs_list: list[float] = []\n    worst_abs_list: list[float] = []\n\n    primary_success_count = 0\n    waiver_success_count = 0\n    relative_success_count = 0\n    plane_success_count = 0\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    plane_limit = settings.plane_intersection_limit_km\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n\n    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):\n        dispersed_elements = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis + rng.normal(0.0, sigma_a),\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination + rng.normal(0.0, sigma_i),\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=leader_elements.mean_anomaly,\n        )\n\n        drag_scale = 1.0 + rng.normal(0.0, sigma_cd)\n        states = _initial_states(\n            dispersed_elements,\n            formation,\n            settings.drag_coefficient * drag_scale,\n            epoch_offset_s=epoch_offset,\n        )\n        plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n        plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n\n        epoch = settings.start_time\n        dt = settings.time_step_s\n        vehicle_metrics = _initial_metric_structure(states, target_lat, target_lon)\n        previous_signs = {state.identifier: 0.0 for state in states}\n        relative_run_stats = {\"max\": 0.0, \"min\": math.inf}\n        evaluation_snapshot: dict[str, float] = {}\n        evaluation_best = math.inf\n\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            current_values: dict[str, float] = {}\n            for idx, state in enumerate(states):\n                position_ecef = inertial_to_ecef(state.position_m, epoch)\n                latitude, longitude, _ = geodetic_coordinates(position_ecef)\n                distance = haversine_distance(latitude, longitude, target_lat, target_lon)\n                sign = 1.0 if latitude >= target_lat else -1.0\n                cross_track_km = sign * (distance / 1_000.0)\n\n                vehicle_entry = vehicle_metrics[\"vehicles\"][idx]\n                vehicle_entry[\"max_cross_track_km\"] = max(\n                    vehicle_entry[\"max_cross_track_km\"], cross_track_km\n                )\n                vehicle_entry[\"min_cross_track_km\"] = min(\n                    vehicle_entry[\"min_cross_track_km\"], cross_track_km\n                )\n                abs_value = abs(cross_track_km)\n                if abs_value > vehicle_entry[\"max_abs_cross_track_km\"]:\n                    vehicle_entry[\"max_abs_cross_track_km\"] = abs_value\n                if abs_value < vehicle_entry[\"min_abs_cross_track_km\"]:\n                    vehicle_entry[\"min_abs_cross_track_km\"] = abs_value\n\n                current_values[str(state.identifier)] = float(cross_track_km)\n\n                identifier = str(state.identifier)\n                previous_sign = previous_signs[identifier]\n                if previous_sign != 0.0 and previous_sign * cross_track_km < 0.0:\n                    vehicle_entry[\"pass_count\"] += 1\n                previous_signs[identifier] = cross_track_km\n\n            diff = abs((epoch - evaluation_time).total_seconds())\n            if diff < evaluation_best:\n                evaluation_best = diff\n                evaluation_snapshot = dict(current_values)\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n\n            leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n            plane_b_state = next(\n                (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n                None,\n            )\n            if leader_state is not None and plane_b_state is not None:\n                normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n                norm_normal = np.linalg.norm(normal_vec)\n                if norm_normal > 0.0:\n                    normal_unit = normal_vec / norm_normal\n                    relative_vec = plane_b_state.position_m - leader_state.position_m\n                    cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n                    abs_val = abs(cross_track_km)\n                    if abs_val > relative_run_stats[\"max\"]:\n                        relative_run_stats[\"max\"] = abs_val\n                    if abs_val < relative_run_stats[\"min\"]:\n                        relative_run_stats[\"min\"] = abs_val\n\n        evaluation_abs = {\n            identifier: abs(value) if math.isfinite(value) else math.inf\n            for identifier, value in evaluation_snapshot.items()\n        }\n        centroid_value = (\n            fmean(evaluation_snapshot.values()) if evaluation_snapshot else math.nan\n        )\n        centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n        worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n        primary_compliant = (\n            math.isfinite(centroid_abs)\n            and centroid_abs <= primary_limit\n            and math.isfinite(worst_abs)\n            and worst_abs <= waiver_limit\n        )\n        waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n\n        if math.isfinite(centroid_abs):\n            centroid_abs_list.append(centroid_abs)\n        if math.isfinite(worst_abs):\n            worst_abs_list.append(worst_abs)\n\n        for entry in vehicle_metrics[\"vehicles\"]:\n            identifier = str(entry[\"identifier\"])\n            max_abs = float(entry[\"max_abs_cross_track_km\"])\n            min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n            value = evaluation_snapshot.get(identifier, float(\"nan\"))\n            abs_value = evaluation_abs.get(identifier, math.inf)\n            entry[\"cross_track_at_evaluation_km\"] = (\n                float(value) if math.isfinite(value) else None\n            )\n            entry[\"abs_cross_track_at_evaluation_km\"] = (\n                float(abs_value) if math.isfinite(abs_value) else None\n            )\n            entry[\"compliant\"] = bool(\n                math.isfinite(abs_value) and abs_value <= waiver_limit\n            )\n            run_metrics_max[identifier].append(max_abs)\n            if math.isfinite(min_abs):\n                run_metrics_min[identifier].append(min_abs)\n            if math.isfinite(abs_value):\n                run_metrics_eval[identifier].append(abs_value)\n\n        plane_success = True\n        if math.isfinite(plane_distance):\n            if plane_limit is not None:\n                intersection_distances.append(plane_distance)\n                plane_success = plane_distance <= plane_limit\n        elif plane_limit is not None:\n            plane_success = False\n\n        if plane_success:\n            plane_success_count += 1\n\n        if math.isfinite(relative_run_stats[\"max\"]):\n            relative_max_list.append(relative_run_stats[\"max\"])\n        if math.isfinite(relative_run_stats[\"min\"]):\n            relative_min_list.append(relative_run_stats[\"min\"])\n\n        if primary_compliant:\n            primary_success_count += 1\n        if waiver_compliant:\n            waiver_success_count += 1\n        if relative_run_stats[\"max\"] <= waiver_limit:\n            relative_success_count += 1\n\n    aggregated: MutableMapping[str, object] = {\n        \"runs\": runs,\n        \"seed\": seed,\n        \"max_abs_cross_track_km\": {},\n        \"min_abs_cross_track_km\": {},\n        \"evaluation_abs_cross_track_km\": {},\n        \"relative_cross_track_km\": {},\n        \"compliance\": {\n            \"primary_fraction\": float(primary_success_count / runs) if runs else 0.0,\n            \"waiver_fraction\": float(waiver_success_count / runs) if runs else 0.0,\n            \"relative_fraction\": float(relative_success_count / runs) if runs else 0.0,\n        },\n        \"fleet_compliance_probability\": float(primary_success_count / runs)\n        if runs\n        else 0.0,\n        \"absolute_cross_track_compliance_probability\": float(\n            waiver_success_count / runs\n        )\n        if runs\n        else 0.0,\n        \"evaluation_time_utc\": evaluation_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"cross_track_limits_km\": {\n            \"primary\": float(primary_limit),\n            \"waiver\": float(waiver_limit),\n        },\n    }\n\n    if plane_limit is not None:\n        aggregated[\"plane_intersection_distance_km\"] = {}\n\n    if plane_limit is not None:\n        aggregated[\"compliance\"][\"plane_fraction\"] = float(\n            plane_success_count / runs\n        ) if runs else 0.0\n\n    for identifier, values in run_metrics_max.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"max_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_min.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"min_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_eval.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"evaluation_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if plane_limit is not None and intersection_distances:\n        array = np.asarray(intersection_distances, dtype=float)\n        aggregated[\"plane_intersection_distance_km\"][\"fleet\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if relative_max_list:\n        array = np.asarray(relative_max_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_max\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n    if relative_min_list:\n        array = np.asarray(relative_min_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_min\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if centroid_abs_list:\n        array = np.asarray(centroid_abs_list, dtype=float)\n        centroid_stats = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n        aggregated[\"centroid_abs_cross_track_km\"] = centroid_stats\n        aggregated[\"centroid_abs_cross_track_km_mean\"] = centroid_stats[\"mean\"]\n        aggregated[\"centroid_abs_cross_track_km_p95\"] = centroid_stats[\"p95\"]\n    if worst_abs_list:\n        array = np.asarray(worst_abs_list, dtype=float)\n        aggregated[\"worst_vehicle_abs_cross_track_km\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    return aggregated\n\n\ndef _write_outputs(\n    output_directory: Path,\n    series: Mapping[str, object],\n    deterministic_metrics: Mapping[str, object],\n    monte_carlo_metrics: Mapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, str]:\n    \"\"\"Persist deterministic and Monte Carlo outputs to the run directory.\"\"\"\n\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    deterministic_path = output_directory / \"deterministic_cross_track.csv\"\n    relative_path = output_directory / \"relative_cross_track.csv\"\n    summary_path = output_directory / \"deterministic_summary.json\"\n    orbital_path = output_directory / \"deterministic_orbital_elements.csv\"\n    monte_carlo_path = output_directory / \"monte_carlo_cross_track.csv\"\n    monte_carlo_summary_path = output_directory / \"monte_carlo_summary.json\"\n    settings_path = output_directory / \"solver_settings.json\"\n\n    _write_cross_track_csv(deterministic_path, series)\n    orbital_series = series.get(\"orbital_elements\") if isinstance(series, Mapping) else {}\n    _write_orbital_elements_csv(orbital_path, orbital_series)\n\n    relative_summary = deterministic_metrics.get(\"relative_cross_track\")\n    if isinstance(relative_summary, Mapping):\n        _write_relative_cross_track_csv(relative_path, relative_summary)\n        relative_summary = dict(relative_summary)\n        relative_summary.pop(\"series\", None)\n        deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    else:\n        _write_relative_cross_track_csv(relative_path, {})\n\n    summary_payload = {\n        \"metrics\": deterministic_metrics,\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": settings.time_step_s,\n            \"drag_coefficient\": settings.drag_coefficient,\n            \"ballistic_coefficient_m2_per_kg\": settings.ballistic_coefficient_m2_per_kg,\n            \"solar_flux_index\": settings.solar_flux_index,\n        },\n    }\n    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding=\"utf-8\")\n\n    _write_monte_carlo_csv(monte_carlo_path, monte_carlo_metrics)\n    monte_carlo_summary_path.write_text(json.dumps(monte_carlo_metrics, indent=2), encoding=\"utf-8\")\n\n    settings_payload = {\n        \"propagator\": {\n            \"integrator\": \"fixed_step_rk4\",\n            \"time_step_s\": settings.time_step_s,\n            \"force_models\": {\n                \"central_body\": True,\n                \"j2\": True,\n                \"atmospheric_drag\": {\n                    \"model\": \"exponential\",\n                    \"solar_flux_index\": settings.solar_flux_index,\n                },\n            },\n        },\n        \"monte_carlo\": {\n            \"runs\": monte_carlo_config.get(\"runs\", 0),\n            \"seed\": monte_carlo_config.get(\"seed\", 0),\n            \"dispersions\": monte_carlo_config.get(\"dispersions\", {}),\n        },\n        \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n    settings_path.write_text(json.dumps(settings_payload, indent=2), encoding=\"utf-8\")\n\n    orbital_metrics = deterministic_metrics.get(\"orbital_elements\")\n    if isinstance(orbital_metrics, Mapping):\n        time_series_meta = orbital_metrics.get(\"time_series\")\n        if isinstance(time_series_meta, Mapping):\n            time_series_meta[\"artefact\"] = str(orbital_path)\n\n    return {\n        \"deterministic_cross_track_csv\": str(deterministic_path),\n        \"deterministic_orbital_elements_csv\": str(orbital_path),\n        \"deterministic_summary_json\": str(summary_path),\n        \"monte_carlo_cross_track_csv\": str(monte_carlo_path),\n        \"relative_cross_track_csv\": str(relative_path),\n        \"monte_carlo_summary_json\": str(monte_carlo_summary_path),\n        \"solver_settings_json\": str(settings_path),\n    }\n\n\ndef _write_cross_track_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Write deterministic cross-track histories to *path*.\"\"\"\n\n    samples = series.get(\"samples\", [])\n    if not samples:\n        path.write_text(\"time,\\n\", encoding=\"utf-8\")\n        return\n\n    vehicle_ids = sorted(next(iter(samples))[1])\n    header = [\"time_iso\"] + vehicle_ids\n    lines = [\",\".join(header)]\n\n    for timestamp, cross_track in samples:\n        row = [timestamp.isoformat().replace(\"+00:00\", \"Z\")]\n        for identifier in vehicle_ids:\n            row.append(f\"{cross_track.get(identifier, 0.0):.6f}\")\n        lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_orbital_elements_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Persist orbital-element histories to *path*.\"\"\"\n\n    header = \"time_iso,satellite_id,\" + \",\".join(CLASSICAL_ELEMENT_FIELDS)\n    if not isinstance(series, Mapping) or not series:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    times = list(series.get(\"times\", []))\n    per_satellite = series.get(\"per_satellite\")\n    if not isinstance(per_satellite, Mapping) or not times:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [header]\n    for index, timestamp in enumerate(times):\n        if isinstance(timestamp, datetime):\n            stamp = timestamp.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        else:\n            stamp = str(timestamp)\n        for identifier in sorted(per_satellite):\n            elements = per_satellite.get(identifier)\n            if not isinstance(elements, Mapping):\n                continue\n            row = [stamp, str(identifier)]\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                value = float(\"nan\")\n                if isinstance(values, Sequence) and index < len(values):\n                    try:\n                        value = float(values[index])  # type: ignore[arg-type]\n                    except (TypeError, ValueError):\n                        value = float(\"nan\")\n                row.append(f\"{value:.9g}\" if math.isfinite(value) else \"\")\n            lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_monte_carlo_csv(path: Path, metrics: Mapping[str, object]) -> None:\n    \"\"\"Write Monte Carlo aggregate metrics to *path*.\"\"\"\n\n    vehicle_stats = metrics.get(\"max_abs_cross_track_km\", {})\n    min_stats_all = metrics.get(\"min_abs_cross_track_km\", {})\n    plane_stats = metrics.get(\"plane_intersection_distance_km\", {})\n    relative_stats = metrics.get(\"relative_cross_track_km\", {})\n    if (\n        not vehicle_stats\n        and not min_stats_all\n        and not plane_stats\n        and not relative_stats\n    ):\n        path.write_text(\n            \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n            \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n            \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n            \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n            \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n            \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\\n\",\n            encoding=\"utf-8\",\n        )\n        return\n\n    header = (\n        \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n        \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n        \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n        \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n        \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n        \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\"\n    )\n    lines = [header]\n\n    identifiers = sorted(\n        set(\n            list(vehicle_stats.keys())\n            + list(min_stats_all.keys())\n            + list(plane_stats.keys())\n            + list(relative_stats.keys())\n        )\n    )\n\n    for identifier in identifiers:\n        stats = vehicle_stats.get(identifier, {})\n        min_stats = min_stats_all.get(identifier, {})\n        plane_entry = plane_stats.get(identifier, {})\n        relative_entry = relative_stats.get(identifier, {})\n        line = \",\".join(\n            [\n                str(identifier),\n                f\"{min_stats.get('mean', 0.0):.6f}\",\n                f\"{min_stats.get('std', 0.0):.6f}\",\n                f\"{min_stats.get('p95', 0.0):.6f}\",\n                f\"{min_stats.get('min', 0.0):.6f}\",\n                f\"{min_stats.get('max', 0.0):.6f}\",\n                f\"{stats.get('mean', 0.0):.6f}\",\n                f\"{stats.get('std', 0.0):.6f}\",\n                f\"{stats.get('p95', 0.0):.6f}\",\n                f\"{stats.get('min', 0.0):.6f}\",\n                f\"{stats.get('max', 0.0):.6f}\",\n                f\"{plane_entry.get('mean', 0.0):.6f}\",\n                f\"{plane_entry.get('std', 0.0):.6f}\",\n                f\"{plane_entry.get('p95', 0.0):.6f}\",\n                f\"{plane_entry.get('min', 0.0):.6f}\",\n                f\"{plane_entry.get('max', 0.0):.6f}\",\n                f\"{relative_entry.get('mean', 0.0):.6f}\",\n                f\"{relative_entry.get('std', 0.0):.6f}\",\n                f\"{relative_entry.get('p95', 0.0):.6f}\",\n                f\"{relative_entry.get('min', 0.0):.6f}\",\n                f\"{relative_entry.get('max', 0.0):.6f}\",\n            ]\n        )\n        lines.append(line)\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_relative_cross_track_csv(path: Path, summary: Mapping[str, object]) -> None:\n    \"\"\"Write relative cross-track series to *path*.\"\"\"\n\n    series = summary.get(\"series\") if isinstance(summary, Mapping) else None\n    if not series:\n        path.write_text(\"time_iso,relative_cross_track_km\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [\"time_iso,relative_cross_track_km\"]\n    for time_iso, value in series:\n        lines.append(f\"{time_iso},{float(value):.6f}\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _parse_time(value: object) -> datetime | None:\n    \"\"\"Parse an ISO-8601 timestamp into an aware :class:`datetime`.\"\"\"\n\n    if isinstance(value, str):\n        try:\n            if value.endswith(\"Z\"):\n                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n            parsed = datetime.fromisoformat(value)\n            if parsed.tzinfo is None:\n                return parsed.replace(tzinfo=timezone.utc)\n            return parsed.astimezone(timezone.utc)\n        except ValueError:\n            return None\n    return None\n\n\n__all__ = [\n    \"PropagatorSettings\",\n    \"propagate_constellation\",\n    \"scenario_cross_track_limits\",\n    \"scenario_access_window\",\n]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "Okay, I'll retry this with two `replace` calls. First, I'll tackle `_propagate_deterministic` by replacing its `while` loop with a `tqdm`-wrapped version. Then, I'll move on to `_run_monte_carlo` and wrap its `for` loop with `tqdm` as well."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the deterministic propagation loop using a with statement.",
            "old_string": "    while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n        times.append(epoch)\n        for state in states:\n            elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n            per_satellite = orbital_elements_series[state.identifier]\n            per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n            per_satellite[\"eccentricity\"].append(elements.eccentricity)\n            per_satellite[\"inclination_deg\"].append(\n                _normalise_angle_degrees(elements.inclination)\n            )\n            per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n            per_satellite[\"argument_of_perigee_deg\"].append(\n                _normalise_angle_degrees(elements.arg_perigee)\n            )\n            per_satellite[\"mean_anomaly_deg\"].append(\n                _normalise_angle_degrees(elements.mean_anomaly)\n            )\n        _record_metrics(\n            states,\n            epoch,\n            target_lat,\n            target_lon,\n            metrics,\n            cross_track_series,\n            altitude_series,\n            relative_stats,\n        )\n\n        if epoch >= settings.stop_time:\n            break\n\n        states = _rk4_step(states, dt, settings)\n        epoch += timedelta(seconds=dt)",
            "new_string": "    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            times.append(epoch)\n            for state in states:\n                elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                per_satellite = orbital_elements_series[state.identifier]\n                per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                per_satellite[\"inclination_deg\"].append(\n                    _normalise_angle_degrees(elements.inclination)\n                )\n                per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                per_satellite[\"argument_of_perigee_deg\"].append(\n                    _normalise_angle_degrees(elements.arg_perigee)\n                )\n                per_satellite[\"mean_anomaly_deg\"].append(\n                    _normalise_angle_degrees(elements.mean_anomaly)\n                )\n            _record_metrics(\n                states,\n                epoch,\n                target_lat,\n                target_lon,\n                metrics,\n                cross_track_series,\n                altitude_series,\n                relative_stats,\n            )\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n            pbar.update(1)"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793545785-656dea76dcb9b",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CikB4/H/Xg5UvASXIXD9lzhZqkvCIM0d4k3LFgmN8H0dbwnwlC2VYHf2WwpyAePx/15Ju+lnGr0I8QE1W/7ueQgT7OzNdzFDRUZv9yORq7IIziXlulasbpq9BIhQtt4mBdAHZfHp1utTTpsTYNzDH0J/J94lfWS/W+RVaWfbPoiSO3eII08kZmaZuJYWumBYbMcLeiQe4rVJjOjz7xIGCl4B4/H/XhhXaSYQ6e1WuXYO3hCP09F+371E2MW2O8J3PBW0FrXbU4DPXYyjwiqDHEr9L9g9vzph+BcJ/eLCnJBytFzmUYKKz0qxxu4EJTPLf9sY32VaQP/xjZvEcARQ",
        "text": "Okay, `_propagate_deterministic` has its progress bar. Now adding one to `_run_monte_carlo`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "old_string": "    for _ in range(runs):",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793550223-5894606e914df",
          "name": "replace",
          "response": {
            "error": "Failed to generate JSON content: Resource has been exhausted (e.g. check quota)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XgzO/8Wv/1/EKaqUjKlpdzRnLdOG5eBFrIM1XTad/wwKbQHj8f9ebSssMp+PpF3YQC1yFoaFsHzDvnrUvR4BP6KPeOoIrwaUFyVyp7Y15fzkz8U2l/dPZ5fJ7PbaqC2qKxs9qQkUnmIWTzZVzGecVwcY/8yNQ/sxm4zjELkLvj8XF3o201riBm+Mb4RyiqMKaQHj8f9e6uyIEYjgDc56XOhqjVU9KPLKLT/Oa2KAr1fukLBFAcMYPNCO0XZzLv5/3r4bCq0MQqmuXrfzetEB9TswUyxTPVHuMP3Ql3bA+1oca7SHvjAi5xLQ/u39Oo62X2Xs77ObRUQqgw==",
        "text": "`replace` failed; resource error. Retrying same command. Transient issue, maybe."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop.",
            "new_string": "    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):",
            "old_string": "    for _ in range(runs):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793554852-baf79dd1a59f6",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the target code block, including the `from tqdm import tqdm` import and the `for` loop wrapped with `tqdm`, is already present in the file. No changes are required as the desired modification has already been applied.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    for _ in range(runs):). Original old_string was (    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XsQZajILS8NFLKSQPIwF3SOUegusovGTAKRep59ziQwKbQHj8f9e3pCuyxb+4iJDHRzfOnK++UqNkpnYUWE6aRvXskYRkJ8qXRN0gqfyGvvuMN0/DSHKi3CfIuraUdHxWEib+M+m6fG7eGZZW0p9zq+y9JC2HKQLyNAWL+Yx2kh6k+VQOrlvuwBChxcflvUKawHj8f9e4WYwMN8qPrzWaaa4iQvYJJMSpxy/qUQlcMJWni1eh8TUvccmhSTFPcgEimtR9tX+8IaQkZqyASSfeU4FvYSwydwoLCKsaTTbnLGTO3xBVFjaKncM8lDXKhEUkGcGgjaifhurZ6g8CrwBAePx/17q5SCCxZJx/QP2aekVOW2MZiweEVXJYzVBT+BNysWfx+2oOXGZE55XWRhgYs2UJTyLQeOKP9xJpqufLtICkm0uDDCRqwLO1L4nqL151IOTJrdjk1KRN+BHTRYYk+vD2GlGu1ke38UdywFf0Zv4/9+GZGyUq6Yusk1rSWestlw63YHqybuYNK7DNvkxotEbSVkinDT04sZ8e3nYeNc1Vxn28aGK0EmkHXvp9t7s4InkUAZFce3DERMKZAHj8f9e+yNJvlPz6l3LOZA4caom6yLYt9RUY/7ZOv7BcqGKgkBglloTOnllQ6/MoQATnzIw46wZ58oycKkPR4MlEHrU7B9W7ZqmxBudoSwGDrHmkEeYpfhkhA+ihng3xxpEBFg=",
        "text": "The `replace` tool is unreliable. I'll check the file again. If the change is there, I'll run the simulation. If not, I'll have to tell the user I can't add the progress bar."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762793589859-c23a6d41f7d6c",
          "name": "read_file",
          "response": {
            "output": "r\"\"\"High-fidelity constellation propagation with \\(J_2\\) and drag perturbations.\n\nThe helper functions defined here extend the lightweight scenario pipeline by\nintroducing a deterministic force model and a Monte Carlo dispersion analysis\ntailored to the Tehran daily pass configuration.  The propagated states capture\ncross-track offsets for every spacecraft in the formation, enabling direct\nverification of the ±30 km primary alignment tolerance (and the ±70 km waiver\nceiling) that governs MR-2 and SRD-P-001.  Outputs are serialisable to JSON and\ncomma-separated value (CSV) formats so they remain compatible with Systems Tool\nKit (STK 11.2) validation workflows as required by the repository guidelines.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta, timezone\nfrom pathlib import Path\nfrom typing import Mapping, MutableMapping, Sequence\n\nimport os\n\nimport numpy as np\nfrom statistics import fmean\n\nfrom src.constellation.frames import rotation_matrix_rtn_to_eci\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    EARTH_ROTATION_RATE,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\n\n\nJ2_COEFFICIENT = 1.08262668e-3\nSOLAR_FLUX_BASE = 150.0\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\nOMEGA_EARTH_VECTOR = np.array([0.0, 0.0, EARTH_ROTATION_RATE], dtype=float)\n\nDEFAULT_FORMATION = (\n    {\n        \"identifier\": \"FSAT-LDR\",\n        \"role\": \"leader\",\n        \"radial_offset_km\": 0.0,\n        \"along_track_offset_km\": 0.0,\n        \"cross_track_offset_km\": 0.0,\n    },\n    {\n        \"identifier\": \"FSAT-DP1\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": -0.05,\n        \"along_track_offset_km\": 20.0,\n        \"cross_track_offset_km\": 0.2,\n    },\n    {\n        \"identifier\": \"FSAT-DP2\",\n        \"role\": \"deputy\",\n        \"radial_offset_km\": 0.05,\n        \"along_track_offset_km\": -20.0,\n        \"cross_track_offset_km\": -0.2,\n    },\n)\n\nPLANE_ASSIGNMENTS = {\n    \"FSAT-LDR\": \"Plane A\",\n    \"FSAT-DP1\": \"Plane A\",\n    \"FSAT-DP2\": \"Plane B\",\n}\n\nLOGGER = logging.getLogger(__name__)\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nCLASSICAL_ELEMENT_UNITS = {\n    \"semi_major_axis_km\": \"km\",\n    \"eccentricity\": \"\",\n    \"inclination_deg\": \"deg\",\n    \"raan_deg\": \"deg\",\n    \"argument_of_perigee_deg\": \"deg\",\n    \"mean_anomaly_deg\": \"deg\",\n}\n\n\nDEFAULT_MONTE_CARLO = {\n    \"enabled\": True,\n    \"runs\": 1000,\n    \"seed\": 4242,\n    \"dispersions\": {\n        \"semi_major_axis_sigma_m\": 5.0,\n        \"inclination_sigma_deg\": 0.01,\n        \"drag_coefficient_sigma\": 0.05,\n    },\n}\n\n\n@dataclass\nclass PropagatorSettings:\n    \"\"\"Container describing the deterministic propagation configuration.\"\"\"\n\n    start_time: datetime\n    epoch_time: datetime\n    stop_time: datetime\n    time_step_s: float\n    drag_coefficient: float\n    ballistic_coefficient_m2_per_kg: float\n    solar_flux_index: float\n    evaluation_time: datetime | None = None\n    primary_cross_track_limit_km: float = 30.0\n    waiver_cross_track_limit_km: float = 70.0\n    plane_intersection_limit_km: float | None = None\n\n\n@dataclass\nclass SpacecraftState:\n    \"\"\"Cartesian state and drag properties for a spacecraft.\"\"\"\n\n    identifier: str\n    position_m: np.ndarray\n    velocity_mps: np.ndarray\n    drag_coefficient: float\n    area_m2: float\n    mass_kg: float\n\n\ndef propagate_constellation(\n    scenario: Mapping[str, object],\n    *,\n    output_directory: Path | None = None,\n    formation: Sequence[Mapping[str, float | str]] = DEFAULT_FORMATION,\n    settings: PropagatorSettings | None = None,\n    monte_carlo: Mapping[str, object] | None = None,\n) -> tuple[\n    MutableMapping[str, object],\n    MutableMapping[str, str],\n]:\n    r\"\"\"Propagate the constellation including \\(J_2\\) and atmospheric drag.\"\"\"\n\n    resolved_settings = settings or _default_settings(scenario)\n    monte_carlo_config = _normalise_monte_carlo(monte_carlo)\n\n    target_lat, target_lon = _target_coordinates(scenario)\n    leader_elements = _scenario_orbital_elements(scenario)\n\n    deterministic_series, deterministic_metrics = _propagate_deterministic(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n    )\n\n    monte_carlo_metrics = _run_monte_carlo(\n        leader_elements,\n        formation,\n        resolved_settings,\n        target_lat,\n        target_lon,\n        monte_carlo_config,\n    )\n\n    artefact_paths: MutableMapping[str, str] = {}\n    if output_directory:\n        artefact_paths = _write_outputs(\n            Path(output_directory),\n            deterministic_series,\n            deterministic_metrics,\n            monte_carlo_metrics,\n            resolved_settings,\n            monte_carlo_config,\n        )\n\n    summary = _build_summary(\n        deterministic_metrics,\n        monte_carlo_metrics,\n        resolved_settings,\n        monte_carlo_config,\n    )\n\n    return summary, artefact_paths\n\n\ndef scenario_cross_track_limits(\n    scenario: Mapping[str, object]\n) -> tuple[float, float, float | None]:\n    \"\"\"Return the primary, waiver, and optional plane-intersection limits.\"\"\"\n\n    primary_limit = float(os.getenv(\"FSAT_PRIMARY_CROSS_TRACK_LIMIT_KM\", \"30.0\"))\n    waiver_limit = float(os.getenv(\"FSAT_WAIVER_CROSS_TRACK_LIMIT_KM\", \"70.0\"))\n    plane_limit_env = os.getenv(\"FSAT_PLANE_INTERSECTION_LIMIT_KM\")\n    plane_limit: float | None = (\n        float(plane_limit_env) if plane_limit_env is not None else None\n    )\n\n    limits = scenario.get(\"cross_track_limits\")\n    if isinstance(limits, Mapping):\n        if \"primary_km\" in limits:\n            try:\n                primary_limit = float(limits[\"primary_km\"])\n            except (TypeError, ValueError):\n                pass\n        if \"waiver_km\" in limits:\n            try:\n                waiver_limit = float(limits[\"waiver_km\"])\n            except (TypeError, ValueError):\n                pass\n        plane_candidate = limits.get(\"plane_intersection_limit_km\")\n        if plane_candidate is not None:\n            try:\n                plane_limit = float(plane_candidate)\n            except (TypeError, ValueError):\n                pass\n    else:\n        requirements = scenario.get(\"alignment_requirements\")\n        if isinstance(requirements, Mapping):\n            primary_limit = float(\n                requirements.get(\"primary_cross_track_limit_km\", primary_limit)\n            )\n            waiver_limit = float(\n                requirements.get(\"waiver_cross_track_limit_km\", waiver_limit)\n            )\n            plane_candidate = requirements.get(\"plane_intersection_limit_km\", plane_limit)\n            if plane_candidate is not None:\n                plane_limit = float(plane_candidate)\n\n    return primary_limit, waiver_limit, plane_limit\n\n\ndef scenario_access_window(\n    scenario: Mapping[str, object]\n) -> tuple[datetime | None, datetime | None, datetime | None]:\n    \"\"\"Return start, end, and midpoint timestamps for the primary access window.\"\"\"\n\n    window = scenario.get(\"access_window\")\n    start = _parse_time(window.get(\"start_utc\")) if isinstance(window, Mapping) else None\n    end = _parse_time(window.get(\"end_utc\")) if isinstance(window, Mapping) else None\n    midpoint = (\n        _parse_time(window.get(\"midpoint_utc\")) if isinstance(window, Mapping) else None\n    )\n\n    timing = (\n        scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    )\n    windows = timing.get(\"daily_access_windows\")\n    if start is None or end is None:\n        if isinstance(windows, Sequence) and windows:\n            primary = windows[0] if isinstance(windows[0], Mapping) else {}\n            if start is None:\n                start = _parse_time(primary.get(\"start_utc\"))\n            if end is None:\n                end = _parse_time(primary.get(\"end_utc\"))\n            if midpoint is None:\n                midpoint = _parse_time(primary.get(\"midpoint_utc\"))\n\n    if midpoint is None and start and end:\n        midpoint = start + (end - start) / 2\n    elif midpoint is None and start:\n        midpoint = start + timedelta(seconds=45.0)\n\n    return start, end, midpoint\n\n\ndef _build_summary(\n    deterministic_metrics: MutableMapping[str, object],\n    monte_carlo_metrics: MutableMapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Combine deterministic and Monte Carlo outputs into a summary map.\"\"\"\n\n    orbital_period = deterministic_metrics.get(\"orbital_period_s\", 0.0)\n    two_body_period = deterministic_metrics.get(\"two_body_period_s\", orbital_period)\n    period_delta = float(orbital_period) - float(two_body_period)\n\n    return {\n        \"model\": \"high_fidelity_j2_drag\",\n        \"orbital_period_s\": float(orbital_period),\n        \"period_delta_s\": float(period_delta),\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": float(settings.time_step_s),\n            \"drag_coefficient\": float(settings.drag_coefficient),\n            \"ballistic_coefficient_m2_per_kg\": float(settings.ballistic_coefficient_m2_per_kg),\n            \"solar_flux_index\": float(settings.solar_flux_index),\n            \"monte_carlo_runs\": int(monte_carlo_config.get(\"runs\", 0)),\n        },\n        \"cross_track\": deterministic_metrics.get(\"cross_track\", {}),\n        \"altitude_range_m\": deterministic_metrics.get(\"altitude_range_m\", {}),\n        \"plane_intersection\": deterministic_metrics.get(\"plane_intersection\", {}),\n        \"monte_carlo\": monte_carlo_metrics,\n    }\n\n\ndef _default_settings(scenario: Mapping[str, object]) -> PropagatorSettings:\n    \"\"\"Return baseline propagation settings derived from the scenario.\"\"\"\n\n    timing = scenario.get(\"timing\", {}) if isinstance(scenario.get(\"timing\"), Mapping) else {}\n    window_start, window_end, window_midpoint = scenario_access_window(scenario)\n    orbital_elements = scenario.get(\"orbital_elements\", {})\n    reference_start = window_start or window_midpoint\n    if reference_start is None:\n        reference_start = _parse_time(\n            orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n        )\n    if reference_start is None:\n        reference_start = datetime(2026, 3, 21, 0, 0, 0, tzinfo=timezone.utc)\n\n    epoch_time = _parse_time(\n        orbital_elements.get(\"epoch_utc\") if isinstance(orbital_elements, Mapping) else None\n    ) or reference_start\n\n    planning = timing.get(\"planning_horizon\", {}) if isinstance(timing.get(\"planning_horizon\"), Mapping) else {}\n    planning_stop = _parse_time(planning.get(\"stop_utc\"))\n\n    propagation = timing.get(\"propagation\") if isinstance(timing.get(\"propagation\"), Mapping) else {}\n    margin_s = 300.0\n    try:\n        margin_candidate = float(propagation.get(\"margin_s\", margin_s))\n        margin_s = max(margin_candidate, 120.0)\n    except (TypeError, ValueError):\n        margin_s = max(margin_s, 120.0)\n    margin = timedelta(seconds=margin_s)\n\n    start_override = _parse_time(propagation.get(\"start_utc\")) if propagation else None\n    end_override = _parse_time(propagation.get(\"end_utc\")) if propagation else None\n    time_step = 10.0\n    try:\n        time_step = float(propagation.get(\"time_step_s\", time_step))\n    except (TypeError, ValueError):\n        time_step = 10.0\n\n    analysis_start = start_override or (reference_start - margin)\n    if window_start and analysis_start > window_start - margin:\n        analysis_start = window_start - margin\n\n    candidate_stop = end_override\n    if candidate_stop is None:\n        if window_end:\n            candidate_stop = window_end + margin\n        elif window_midpoint:\n            candidate_stop = window_midpoint + margin\n        else:\n            candidate_stop = analysis_start + timedelta(minutes=10)\n\n    if planning_stop:\n        candidate_stop = min(candidate_stop, planning_stop)\n\n    if candidate_stop <= analysis_start:\n        candidate_stop = analysis_start + timedelta(minutes=10)\n\n    evaluation_time = window_midpoint or reference_start\n\n    primary_limit, waiver_limit, plane_limit = scenario_cross_track_limits(scenario)\n\n    return PropagatorSettings(\n        start_time=analysis_start,\n        epoch_time=epoch_time,\n        stop_time=candidate_stop,\n        time_step_s=time_step,\n        drag_coefficient=2.2,\n        ballistic_coefficient_m2_per_kg=0.025,\n        solar_flux_index=SOLAR_FLUX_BASE,\n        evaluation_time=evaluation_time,\n        primary_cross_track_limit_km=primary_limit,\n        waiver_cross_track_limit_km=waiver_limit,\n        plane_intersection_limit_km=plane_limit,\n    )\n\n\ndef _normalise_monte_carlo(monte_carlo: Mapping[str, object] | None) -> MutableMapping[str, object]:\n    \"\"\"Return a mutable Monte Carlo configuration with defaults applied.\"\"\"\n\n    if monte_carlo is None:\n        config = dict(DEFAULT_MONTE_CARLO)\n    else:\n        config = dict(DEFAULT_MONTE_CARLO)\n        config.update({key: monte_carlo.get(key, value) for key, value in DEFAULT_MONTE_CARLO.items()})\n        for key, value in monte_carlo.items():\n            if key == \"dispersions\" and isinstance(value, Mapping):\n                default_disp = dict(DEFAULT_MONTE_CARLO.get(\"dispersions\", {}))\n                default_disp.update({k: float(v) for k, v in value.items()})\n                config[\"dispersions\"] = default_disp\n            else:\n                config[key] = value\n\n    dispersions = config.get(\"dispersions\", {})\n    if not isinstance(dispersions, Mapping):\n        dispersions = DEFAULT_MONTE_CARLO[\"dispersions\"]\n    config[\"dispersions\"] = dict(dispersions)\n\n    try:\n        runs = int(config.get(\"runs\", DEFAULT_MONTE_CARLO[\"runs\"]))\n    except (TypeError, ValueError):\n        runs = DEFAULT_MONTE_CARLO[\"runs\"]\n    config[\"runs\"] = max(runs, 500)\n\n    return config\n\n\ndef _resolve_evaluation_time(settings: PropagatorSettings) -> datetime:\n    \"\"\"Return a clamped evaluation time within the propagation span.\"\"\"\n\n    evaluation = settings.evaluation_time or settings.start_time\n    if evaluation < settings.start_time:\n        evaluation = settings.start_time\n    if evaluation > settings.stop_time:\n        evaluation = settings.stop_time\n    return evaluation\n\n\ndef _scenario_orbital_elements(scenario: Mapping[str, object]) -> OrbitalElements:\n    \"\"\"Extract classical elements from the scenario mapping.\"\"\"\n\n    orbital = scenario.get(\"orbital_elements\")\n    if not isinstance(orbital, Mapping):\n        fallback = _fallback_orbital_elements(scenario)\n        if fallback is None:\n            raise ValueError(\"Scenario is missing the 'orbital_elements' section.\")\n        return fallback\n\n    classical = orbital.get(\"classical\")\n    if not isinstance(classical, Mapping):\n        raise ValueError(\"Scenario orbital elements must include the 'classical' block.\")\n\n    semi_major_axis_m = float(classical.get(\"semi_major_axis_km\", 0.0)) * 1_000.0\n    eccentricity = float(classical.get(\"eccentricity\", 0.0))\n    inclination = math.radians(float(classical.get(\"inclination_deg\", 0.0)))\n    raan = math.radians(float(classical.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(classical.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(classical.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis_m,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _fallback_orbital_elements(\n    scenario: Mapping[str, object]\n) -> OrbitalElements | None:\n    spacecraft = scenario.get(\"spacecraft\")\n    if not isinstance(spacecraft, Sequence) or not spacecraft:\n        return None\n    leader = spacecraft[0]\n    if not isinstance(leader, Mapping):\n        return None\n    initial_state = leader.get(\"initial_state\")\n    if not isinstance(initial_state, Mapping):\n        return None\n\n    semi_major_axis = float(initial_state.get(\"semi_major_axis_m\", 0.0))\n    if semi_major_axis <= 0.0:\n        return None\n    eccentricity = float(initial_state.get(\"eccentricity\", 0.0))\n    inclination = float(initial_state.get(\"inclination_rad\", 0.0))\n    raan = math.radians(float(initial_state.get(\"raan_deg\", 0.0)))\n    arg_perigee = math.radians(float(initial_state.get(\"argument_of_perigee_deg\", 0.0)))\n    mean_anomaly = math.radians(float(initial_state.get(\"mean_anomaly_deg\", 0.0)))\n\n    return OrbitalElements(\n        semi_major_axis=semi_major_axis,\n        eccentricity=eccentricity,\n        inclination=inclination,\n        raan=raan,\n        arg_perigee=arg_perigee,\n        mean_anomaly=mean_anomaly,\n    )\n\n\ndef _target_coordinates(scenario: Mapping[str, object]) -> tuple[float, float]:\n    \"\"\"Return the target latitude and longitude in radians.\"\"\"\n\n    metadata = scenario.get(\"metadata\")\n    region = metadata.get(\"region\") if isinstance(metadata, Mapping) else None\n    if isinstance(region, Mapping):\n        latitude = math.radians(float(region.get(\"latitude_deg\", 0.0)))\n        longitude = math.radians(float(region.get(\"longitude_deg\", 0.0)))\n        return latitude, longitude\n\n    return math.radians(35.6892), math.radians(51.3890)\n\n\ndef _initial_states(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    drag_coefficient: float,\n    *,\n    epoch_offset_s: float = 0.0,\n) -> list[SpacecraftState]:\n    \"\"\"Construct initial Cartesian states for the formation.\"\"\"\n\n    elements_at_start = leader_elements\n    if epoch_offset_s:\n        mean_motion = leader_elements.mean_motion()\n        propagated_mean_anomaly = _wrap_angle(\n            leader_elements.mean_anomaly + mean_motion * epoch_offset_s\n        )\n        elements_at_start = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis,\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination,\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=propagated_mean_anomaly,\n        )\n\n    position, velocity = _classical_to_cartesian(elements_at_start)\n    rotation = rotation_matrix_rtn_to_eci(position, velocity)\n    states: list[SpacecraftState] = []\n\n    for entry in formation:\n        identifier = str(entry.get(\"identifier\", \"spacecraft\"))\n        radial_offset = float(entry.get(\"radial_offset_km\", 0.0)) * 1_000.0\n        along_track_offset = float(entry.get(\"along_track_offset_km\", 0.0)) * 1_000.0\n        cross_track_offset = float(entry.get(\"cross_track_offset_km\", 0.0)) * 1_000.0\n\n        offset_rtn = np.array([radial_offset, along_track_offset, cross_track_offset], dtype=float)\n        position_shift = rotation @ offset_rtn\n\n        adjusted_position = position + position_shift\n        adjusted_velocity = velocity.copy()\n\n        states.append(\n            SpacecraftState(\n                identifier=identifier,\n                position_m=adjusted_position.astype(float),\n                velocity_mps=adjusted_velocity.astype(float),\n                drag_coefficient=float(entry.get(\"drag_coefficient\", drag_coefficient)),\n                area_m2=4.0,\n                mass_kg=120.0,\n            )\n        )\n\n    return states\n\n\ndef _classical_to_cartesian(elements: OrbitalElements) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return Cartesian state vectors for *elements* without SciPy dependency.\"\"\"\n\n    # This inline implementation mirrors :func:`constellation.orbit.classical_to_cartesian`\n    # but operates fully in metres to avoid repeated conversions.\n    a = elements.semi_major_axis\n    e = elements.eccentricity\n    i = elements.inclination\n    raan = elements.raan\n    argp = elements.arg_perigee\n    m_anomaly = elements.mean_anomaly\n\n    true_anomaly = _mean_to_true(m_anomaly, e)\n    semi_latus = a * (1.0 - e * e)\n    radius = semi_latus / (1.0 + e * math.cos(true_anomaly))\n\n    position_pf = np.array(\n        [radius * math.cos(true_anomaly), radius * math.sin(true_anomaly), 0.0],\n        dtype=float,\n    )\n    velocity_pf = np.array(\n        [\n            -math.sqrt(MU_EARTH / semi_latus) * math.sin(true_anomaly),\n            math.sqrt(MU_EARTH / semi_latus) * (e + math.cos(true_anomaly)),\n            0.0,\n        ],\n        dtype=float,\n    )\n\n    cos_raan = math.cos(raan)\n    sin_raan = math.sin(raan)\n    cos_argp = math.cos(argp)\n    sin_argp = math.sin(argp)\n    cos_i = math.cos(i)\n    sin_i = math.sin(i)\n\n    rotation = np.array(\n        [\n            [\n                cos_raan * cos_argp - sin_raan * sin_argp * cos_i,\n                -cos_raan * sin_argp - sin_raan * cos_argp * cos_i,\n                sin_raan * sin_i,\n            ],\n            [\n                sin_raan * cos_argp + cos_raan * sin_argp * cos_i,\n                -sin_raan * sin_argp + cos_raan * cos_argp * cos_i,\n                -cos_raan * sin_i,\n            ],\n            [sin_argp * sin_i, cos_argp * sin_i, cos_i],\n        ],\n        dtype=float,\n    )\n\n    position = rotation @ position_pf\n    velocity = rotation @ velocity_pf\n    return position, velocity\n\n\ndef _mean_to_true(mean_anomaly: float, eccentricity: float) -> float:\n    \"\"\"Convert mean anomaly to true anomaly using Newton iterations.\"\"\"\n\n    if eccentricity < 1.0e-12:\n        return _wrap_angle(mean_anomaly)\n\n    eccentric = mean_anomaly\n    for _ in range(30):\n        residual = eccentric - eccentricity * math.sin(eccentric) - mean_anomaly\n        derivative = 1.0 - eccentricity * math.cos(eccentric)\n        step = -residual / derivative\n        eccentric += step\n        if abs(step) < 1.0e-12:\n            break\n\n    sine_half = math.sqrt(1.0 + eccentricity) * math.sin(0.5 * eccentric)\n    cosine_half = math.sqrt(1.0 - eccentricity) * math.cos(0.5 * eccentric)\n    return _wrap_angle(2.0 * math.atan2(sine_half, cosine_half))\n\n\ndef _wrap_angle(angle: float) -> float:\n    \"\"\"Normalise *angle* to the ``[0, 2π)`` interval.\"\"\"\n\n    wrapped = math.fmod(angle, 2.0 * math.pi)\n    return wrapped + 2.0 * math.pi if wrapped < 0.0 else wrapped\n\n\ndef _propagate_deterministic(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n) -> tuple[\n    MutableMapping[str, list[tuple[datetime, dict[str, float]]]],\n    MutableMapping[str, object],\n]:\n    \"\"\"Propagate the deterministic trajectory and capture time histories.\"\"\"\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n    states = _initial_states(\n        leader_elements,\n        formation,\n        settings.drag_coefficient,\n        epoch_offset_s=epoch_offset,\n    )\n    epoch = settings.start_time\n    dt = settings.time_step_s\n\n    times: list[datetime] = []\n    cross_track_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    altitude_series: dict[str, list[float]] = {state.identifier: [] for state in states}\n    orbital_elements_series: dict[str, dict[str, list[float]]] = {\n        state.identifier: {field: [] for field in CLASSICAL_ELEMENT_FIELDS} for state in states\n    }\n    metrics = _initial_metric_structure(states, target_lat, target_lon)\n    plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n    relative_stats = {\"max_abs\": 0.0, \"min_abs\": math.inf, \"time_of_min\": None, \"series\": []}\n\n    mean_motion = leader_elements.mean_motion()\n    orbital_period = 2.0 * math.pi / mean_motion if mean_motion else 0.0\n\n    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n            from tqdm import tqdm\n            n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n            with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n                while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n                    times.append(epoch)\n                    for state in states:\n                        elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                        per_satellite = orbital_elements_series[state.identifier]\n                        per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                        per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                        per_satellite[\"inclination_deg\"].append(\n                            _normalise_angle_degrees(elements.inclination)\n                        )\n                        per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                        per_satellite[\"argument_of_perigee_deg\"].append(\n                            _normalise_angle_degrees(elements.arg_perigee)\n                        )\n                        per_satellite[\"mean_anomaly_deg\"].append(\n                            _normalise_angle_degrees(elements.mean_anomaly)\n                        )\n                    _record_metrics(\n                        states,\n                        epoch,\n                        target_lat,\n                        target_lon,\n                        metrics,\n                        cross_track_series,\n                        altitude_series,\n                        relative_stats,\n                    )\n        \n                    if epoch >= settings.stop_time:\n                        break\n        \n                    states = _rk4_step(states, dt, settings)\n                    epoch += timedelta(seconds=dt)\n                    pbar.update(1)            pbar.update(1)\n\n    overall_min_abs = 0.0\n    for entry in metrics[\"vehicles\"]:\n        min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n        if math.isfinite(min_abs):\n            overall_min_abs = max(overall_min_abs, min_abs)\n    metrics[\"overall_min_abs_cross_track_km\"] = float(overall_min_abs)\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    evaluation_timestamp = evaluation_time\n    before_index = after_index = 0\n    fraction = 0.0\n    if times:\n        before_index = max(\n            (idx for idx, stamp in enumerate(times) if stamp <= evaluation_time),\n            default=0,\n        )\n        after_index = min(\n            (idx for idx, stamp in enumerate(times) if stamp >= evaluation_time),\n            default=len(times) - 1,\n        )\n        if after_index < before_index:\n            after_index = before_index\n        before_time = times[before_index]\n        after_time = times[after_index]\n        if after_time > before_time:\n            elapsed = (evaluation_time - before_time).total_seconds()\n            span = (after_time - before_time).total_seconds()\n            fraction = max(0.0, min(elapsed / span, 1.0))\n        else:\n            fraction = 0.0\n\n    evaluation_values: dict[str, float] = {}\n    evaluation_abs: dict[str, float] = {}\n    for identifier, series_values in cross_track_series.items():\n        value_before = float(\"nan\")\n        value_after = float(\"nan\")\n        if series_values:\n            clamped_before = min(before_index, len(series_values) - 1)\n            clamped_after = min(after_index, len(series_values) - 1)\n            value_before = float(series_values[clamped_before])\n            value_after = float(series_values[clamped_after])\n            if math.isfinite(value_before) and math.isfinite(value_after):\n                value = value_before + fraction * (value_after - value_before)\n            elif math.isfinite(value_before):\n                value = value_before\n            elif math.isfinite(value_after):\n                value = value_after\n            else:\n                value = float(\"nan\")\n        else:\n            value = float(\"nan\")\n        evaluation_values[identifier] = value\n        evaluation_abs[identifier] = abs(value) if math.isfinite(value) else math.inf\n\n    centroid_value = fmean(evaluation_values.values()) if evaluation_values else math.nan\n    centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n    worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n\n    primary_compliant = (\n        math.isfinite(centroid_abs)\n        and centroid_abs <= primary_limit\n        and math.isfinite(worst_abs)\n        and worst_abs <= waiver_limit\n    )\n    waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n    finite_vehicle_abs = [\n        value for value in evaluation_abs.values() if math.isfinite(value)\n    ]\n    waiver_pass_fraction = (\n        sum(1 for value in finite_vehicle_abs if value <= waiver_limit) / len(finite_vehicle_abs)\n        if finite_vehicle_abs\n        else 0.0\n    )\n    primary_pass_fraction = 1.0 if primary_compliant else 0.0\n\n    for entry in metrics[\"vehicles\"]:\n        identifier = str(entry.get(\"identifier\", \"\"))\n        value = evaluation_values.get(identifier, float(\"nan\"))\n        abs_value = evaluation_abs.get(identifier, math.inf)\n        entry[\"cross_track_at_evaluation_km\"] = (\n            float(value) if math.isfinite(value) else None\n        )\n        entry[\"abs_cross_track_at_evaluation_km\"] = (\n            float(abs_value) if math.isfinite(abs_value) else None\n        )\n        entry[\"compliant\"] = bool(\n            math.isfinite(abs_value) and abs_value <= waiver_limit\n        )\n\n    evaluation_timestamp_iso = evaluation_timestamp.isoformat().replace(\"+00:00\", \"Z\")\n    centroid_value_out = float(centroid_value) if math.isfinite(centroid_value) else None\n    centroid_abs_out = float(centroid_abs) if math.isfinite(centroid_abs) else None\n    worst_abs_out = float(worst_abs) if math.isfinite(worst_abs) else None\n\n    metrics[\"evaluation\"] = {\n        \"time_utc\": evaluation_timestamp_iso,\n        \"vehicles\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_values.items()\n        },\n        \"vehicle_abs\": {\n            identifier: (float(value) if math.isfinite(value) else None)\n            for identifier, value in evaluation_abs.items()\n        },\n        \"centroid_cross_track_km\": centroid_value_out,\n        \"centroid_abs_cross_track_km\": centroid_abs_out,\n        \"worst_vehicle_abs_cross_track_km\": worst_abs_out,\n        \"primary_compliant\": bool(primary_compliant),\n        \"waiver_compliant\": bool(waiver_compliant),\n        \"primary_limit_km\": float(primary_limit),\n        \"waiver_limit_km\": float(waiver_limit),\n        \"primary_pass_fraction\": float(primary_pass_fraction),\n        \"waiver_pass_fraction\": float(waiver_pass_fraction),\n    }\n    metrics[\"centroid_cross_track_km_at_evaluation\"] = centroid_value_out\n    metrics[\"centroid_abs_cross_track_km_at_evaluation\"] = centroid_abs_out\n    metrics[\"overall_abs_cross_track_km_at_evaluation\"] = worst_abs_out\n    metrics[\"primary_compliant\"] = bool(primary_compliant)\n    metrics[\"waiver_compliant\"] = bool(waiver_compliant)\n    metrics[\"primary_limit_km\"] = float(primary_limit)\n    metrics[\"waiver_limit_km\"] = float(waiver_limit)\n    metrics[\"primary_pass_fraction\"] = float(primary_pass_fraction)\n    metrics[\"waiver_pass_fraction\"] = float(waiver_pass_fraction)\n\n    relative_summary = {\n        \"max_abs_km\": float(relative_stats[\"max_abs\"]),\n        \"min_abs_km\": float(relative_stats[\"min_abs\"] if math.isfinite(relative_stats[\"min_abs\"]) else 0.0),\n        \"time_of_min\": relative_stats[\"time_of_min\"],\n        \"series\": list(relative_stats[\"series\"]),\n    }\n\n    altitude_min = min(min(series) for series in altitude_series.values()) if altitude_series else 0.0\n    altitude_max = max(max(series) for series in altitude_series.values()) if altitude_series else 0.0\n\n    deterministic_metrics: MutableMapping[str, object] = {\n        \"orbital_period_s\": float(orbital_period),\n        \"two_body_period_s\": float(orbital_period),\n        \"cross_track\": metrics,\n        \"altitude_range_m\": {\n            \"min\": float(altitude_min),\n            \"max\": float(altitude_max),\n        },\n    }\n    centroid_summary = metrics.get(\"centroid\", {})\n    if isinstance(centroid_summary, Mapping):\n        deterministic_metrics[\"centroid_cross_track\"] = {\n            \"min_cross_track_km\": float(centroid_summary.get(\"min_cross_track_km\", math.inf)),\n            \"min_abs_cross_track_km\": float(\n                centroid_summary.get(\"min_abs_cross_track_km\", math.inf)\n            ),\n            \"time_of_min_abs_cross_track\": centroid_summary.get(\n                \"time_of_min_abs_cross_track\"\n            ),\n            \"vehicle_cross_track_km\": centroid_summary.get(\n                \"vehicle_cross_track_km_at_min\", {}\n            ),\n            \"vehicle_abs_cross_track_km\": centroid_summary.get(\n                \"vehicle_abs_cross_track_km_at_min\", {}\n            ),\n            \"worst_vehicle_abs_cross_track_km\": float(\n                centroid_summary.get(\"worst_vehicle_abs_cross_track_km\", math.inf)\n            ),\n            \"evaluation_cross_track_km\": centroid_value_out,\n            \"evaluation_abs_cross_track_km\": centroid_abs_out,\n            \"worst_vehicle_abs_cross_track_km_at_evaluation\": worst_abs_out,\n            \"evaluation_time_utc\": evaluation_timestamp_iso,\n            \"primary_limit_km\": float(primary_limit),\n            \"waiver_limit_km\": float(waiver_limit),\n            \"primary_compliant\": bool(primary_compliant),\n            \"waiver_compliant\": bool(waiver_compliant),\n        }\n    plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n    plane_limit = settings.plane_intersection_limit_km\n    if plane_limit is not None:\n        plane_metrics[\"limit_km\"] = float(plane_limit)\n        if math.isfinite(plane_distance):\n            plane_metrics[\"compliant\"] = bool(plane_distance <= plane_limit)\n        else:\n            plane_metrics[\"compliant\"] = False\n    else:\n        plane_metrics[\"compliant\"] = None\n    deterministic_metrics[\"plane_intersection\"] = plane_metrics\n    deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    deterministic_metrics[\"cross_track_limits_km\"] = {\n        \"primary\": float(primary_limit),\n        \"waiver\": float(waiver_limit),\n    }\n    deterministic_metrics[\"cross_track_evaluation\"] = metrics[\"evaluation\"]\n    deterministic_metrics[\"orbital_elements\"] = {\n        \"time_series\": {\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n            \"artefact_key\": \"deterministic_orbital_elements_csv\",\n        }\n    }\n\n    series = {\n        \"samples\": [\n            (\n                timestamp,\n                {identifier: cross_track_series[identifier][index] for identifier in cross_track_series},\n            )\n            for index, timestamp in enumerate(times)\n        ],\n        \"altitudes\": {\n            identifier: altitude_series[identifier] for identifier in altitude_series\n        },\n        \"orbital_elements\": {\n            \"times\": list(times),\n            \"per_satellite\": {\n                identifier: {\n                    field: list(values)\n                    for field, values in per_satellite.items()\n                }\n                for identifier, per_satellite in orbital_elements_series.items()\n            },\n            \"fields\": CLASSICAL_ELEMENT_FIELDS,\n            \"units\": CLASSICAL_ELEMENT_UNITS,\n        },\n    }\n\n    return series, deterministic_metrics\n\n\ndef _initial_metric_structure(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Construct the dictionary used to record cross-track statistics.\"\"\"\n\n    vehicle_metrics: list[MutableMapping[str, object]] = []\n    for state in states:\n        vehicle_metrics.append(\n            {\n                \"identifier\": state.identifier,\n                \"target_latitude_deg\": math.degrees(target_lat),\n                \"target_longitude_deg\": math.degrees(target_lon),\n                \"max_cross_track_km\": -math.inf,\n                \"min_cross_track_km\": math.inf,\n                \"max_abs_cross_track_km\": -math.inf,\n                \"min_abs_cross_track_km\": math.inf,\n                \"time_of_max_abs_cross_track\": None,\n                \"time_of_min_abs_cross_track\": None,\n                \"pass_count\": 0,\n                \"compliant\": False,\n            }\n        )\n\n    return {\n        \"vehicles\": vehicle_metrics,\n        \"overall_max_abs_cross_track_km\": -math.inf,\n        \"overall_min_abs_cross_track_km\": math.inf,\n        \"centroid\": {\n            \"min_cross_track_km\": math.inf,\n            \"min_abs_cross_track_km\": math.inf,\n            \"time_of_min_abs_cross_track\": None,\n            \"vehicle_cross_track_km_at_min\": {},\n            \"vehicle_abs_cross_track_km_at_min\": {},\n            \"worst_vehicle_abs_cross_track_km\": math.inf,\n        },\n    }\n\n\ndef _normalise_angle_degrees(angle_rad: float) -> float:\n    \"\"\"Convert radians to wrapped degrees in the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _plane_intersection_metrics(\n    states: Sequence[SpacecraftState],\n    target_lat: float,\n    target_lon: float,\n) -> MutableMapping[str, object]:\n    \"\"\"Compute the plane intersection proximity to the target.\"\"\"\n\n    plane_normals: dict[str, list[np.ndarray]] = {}\n    for state in states:\n        plane = PLANE_ASSIGNMENTS.get(state.identifier, \"Plane A\")\n        h_vec = np.cross(state.position_m, state.velocity_mps)\n        norm = np.linalg.norm(h_vec)\n        if norm == 0.0:\n            continue\n        plane_normals.setdefault(plane, []).append(h_vec / norm)\n\n    averaged_normals: dict[str, np.ndarray] = {}\n    for plane, vectors in plane_normals.items():\n        if not vectors:\n            continue\n        stacked = np.vstack(vectors)\n        mean_vec = np.mean(stacked, axis=0)\n        norm = np.linalg.norm(mean_vec)\n        if norm == 0.0:\n            continue\n        averaged_normals[plane] = mean_vec / norm\n\n    plane_a = averaged_normals.get(\"Plane A\")\n    plane_b = averaged_normals.get(\"Plane B\")\n    if plane_a is None or plane_b is None:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    line_direction = np.cross(plane_a, plane_b)\n    norm_line = np.linalg.norm(line_direction)\n    if norm_line == 0.0:\n        return {\n            \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n            \"intersection_candidates\": [],\n            \"target_distance_km\": math.inf,\n        }\n\n    unit_direction = line_direction / norm_line\n    candidates = [unit_direction, -unit_direction]\n    candidate_metrics: list[MutableMapping[str, float]] = []\n    min_distance = math.inf\n\n    for candidate in candidates:\n        surface_point = candidate * EARTH_EQUATORIAL_RADIUS_M\n        lat, lon, _ = geodetic_coordinates(surface_point)\n        distance = haversine_distance(lat, lon, target_lat, target_lon)\n        min_distance = min(min_distance, distance)\n        candidate_metrics.append(\n            {\n                \"latitude_deg\": math.degrees(lat),\n                \"longitude_deg\": math.degrees(lon),\n                \"distance_to_target_km\": distance / 1_000.0,\n            }\n        )\n\n    return {\n        \"planes\": {plane: vec.tolist() for plane, vec in averaged_normals.items()},\n        \"intersection_candidates\": candidate_metrics,\n        \"target_distance_km\": min_distance / 1_000.0,\n    }\n\n\ndef _record_metrics(\n    states: Sequence[SpacecraftState],\n    epoch: datetime,\n    target_lat: float,\n    target_lon: float,\n    metrics: MutableMapping[str, object],\n    cross_track_series: MutableMapping[str, list[float]],\n    altitude_series: MutableMapping[str, list[float]],\n    relative_stats: MutableMapping[str, object],\n) -> None:\n    \"\"\"Update cross-track statistics for the current epoch.\"\"\"\n\n    vehicle_entries = metrics[\"vehicles\"]\n    centroid_metrics = metrics.get(\"centroid\", {})\n    epoch_values: dict[str, float] = {}\n\n    for index, state in enumerate(states):\n        position_ecef = inertial_to_ecef(state.position_m, epoch)\n        latitude, longitude, altitude = geodetic_coordinates(position_ecef)\n        cross_track_distance_m = haversine_distance(latitude, longitude, target_lat, target_lon)\n        sign = 1.0 if latitude >= target_lat else -1.0\n        cross_track_km = sign * (cross_track_distance_m / 1_000.0)\n\n        cross_track_series[state.identifier].append(float(cross_track_km))\n        altitude_series[state.identifier].append(float(altitude))\n        epoch_values[state.identifier] = float(cross_track_km)\n\n        entry = vehicle_entries[index]\n        entry[\"max_cross_track_km\"] = max(entry[\"max_cross_track_km\"], cross_track_km)\n        entry[\"min_cross_track_km\"] = min(entry[\"min_cross_track_km\"], cross_track_km)\n\n        abs_value = abs(cross_track_km)\n        if abs_value > entry[\"max_abs_cross_track_km\"]:\n            entry[\"max_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_max_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        if abs_value < entry[\"min_abs_cross_track_km\"]:\n            entry[\"min_abs_cross_track_km\"] = abs_value\n            entry[\"time_of_min_abs_cross_track\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n\n        previous_series = cross_track_series[state.identifier]\n        if len(previous_series) >= 2 and previous_series[-2] == 0.0:\n            entry[\"pass_count\"] += 1\n        elif len(previous_series) >= 2 and previous_series[-2] * cross_track_km < 0.0:\n            entry[\"pass_count\"] += 1\n\n        metrics[\"overall_max_abs_cross_track_km\"] = max(\n            metrics[\"overall_max_abs_cross_track_km\"], abs_value\n        )\n        metrics[\"overall_min_abs_cross_track_km\"] = min(\n            metrics[\"overall_min_abs_cross_track_km\"], abs_value\n        )\n\n    if epoch_values and isinstance(centroid_metrics, MutableMapping):\n        centroid_value = fmean(epoch_values.values())\n        abs_centroid = abs(centroid_value)\n        if abs_centroid < centroid_metrics.get(\"min_abs_cross_track_km\", math.inf):\n            centroid_metrics[\"min_cross_track_km\"] = float(centroid_value)\n            centroid_metrics[\"min_abs_cross_track_km\"] = float(abs_centroid)\n            centroid_metrics[\"time_of_min_abs_cross_track\"] = (\n                epoch.isoformat().replace(\"+00:00\", \"Z\")\n            )\n            centroid_metrics[\"vehicle_cross_track_km_at_min\"] = {\n                identifier: float(value) for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"vehicle_abs_cross_track_km_at_min\"] = {\n                identifier: float(abs(value))\n                for identifier, value in epoch_values.items()\n            }\n            centroid_metrics[\"worst_vehicle_abs_cross_track_km\"] = float(\n                max(abs(value) for value in epoch_values.values())\n            )\n\n    leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n    plane_b_state = next(\n        (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n        None,\n    )\n    if leader_state is not None and plane_b_state is not None:\n        normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n        norm_normal = np.linalg.norm(normal_vec)\n        if norm_normal > 0.0:\n            normal_unit = normal_vec / norm_normal\n            relative_vec = plane_b_state.position_m - leader_state.position_m\n            cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n            abs_val = abs(cross_track_km)\n            if abs_val > relative_stats[\"max_abs\"]:\n                relative_stats[\"max_abs\"] = abs_val\n            if abs_val < relative_stats[\"min_abs\"]:\n                relative_stats[\"min_abs\"] = abs_val\n                relative_stats[\"time_of_min\"] = epoch.isoformat().replace(\"+00:00\", \"Z\")\n            relative_stats[\"series\"].append(\n                (epoch.isoformat().replace(\"+00:00\", \"Z\"), cross_track_km)\n            )\n\n\ndef _rk4_step(states: Sequence[SpacecraftState], dt: float, settings: PropagatorSettings) -> list[SpacecraftState]:\n    \"\"\"Advance the spacecraft states by *dt* seconds using RK4 integration.\"\"\"\n\n    next_states: list[SpacecraftState] = []\n    for state in states:\n        x0 = state.position_m\n        v0 = state.velocity_mps\n\n        a1 = _acceleration(x0, v0, state, settings)\n        k1_pos = v0\n        k1_vel = a1\n\n        a2 = _acceleration(x0 + 0.5 * dt * k1_pos, v0 + 0.5 * dt * k1_vel, state, settings)\n        k2_pos = v0 + 0.5 * dt * k1_vel\n        k2_vel = a2\n\n        a3 = _acceleration(x0 + 0.5 * dt * k2_pos, v0 + 0.5 * dt * k2_vel, state, settings)\n        k3_pos = v0 + 0.5 * dt * k2_vel\n        k3_vel = a3\n\n        a4 = _acceleration(x0 + dt * k3_pos, v0 + dt * k3_vel, state, settings)\n        k4_pos = v0 + dt * k3_vel\n        k4_vel = a4\n\n        new_position = x0 + (dt / 6.0) * (k1_pos + 2.0 * k2_pos + 2.0 * k3_pos + k4_pos)\n        new_velocity = v0 + (dt / 6.0) * (k1_vel + 2.0 * k2_vel + 2.0 * k3_vel + k4_vel)\n\n        state.position_m = new_position\n        state.velocity_mps = new_velocity\n        next_states.append(state)\n\n    return next_states\n\n\ndef _acceleration(\n    position: np.ndarray,\n    velocity: np.ndarray,\n    state: SpacecraftState,\n    settings: PropagatorSettings,\n) -> np.ndarray:\n    r\"\"\"Return acceleration including central gravity, \\(J_2\\), and drag.\"\"\"\n\n    r_vec = position\n    v_vec = velocity\n    r_norm = np.linalg.norm(r_vec)\n\n    if r_norm == 0.0:\n        raise ValueError(\"State vector has zero magnitude; cannot propagate.\")\n\n    mu = MU_EARTH\n    central = -mu * r_vec / (r_norm**3)\n\n    z2 = r_vec[2] ** 2\n    r2 = r_norm**2\n    factor = 1.5 * J2_COEFFICIENT * mu * (EARTH_EQUATORIAL_RADIUS_M**2) / (r_norm**5)\n    coef_xy = factor * (5.0 * z2 / r2 - 1.0)\n    coef_z = factor * (5.0 * z2 / r2 - 3.0)\n    accel_j2 = np.array([coef_xy * r_vec[0], coef_xy * r_vec[1], coef_z * r_vec[2]], dtype=float)\n\n    v_rel = v_vec - np.cross(OMEGA_EARTH_VECTOR, r_vec)\n    speed_rel = np.linalg.norm(v_rel)\n    altitude = r_norm - EARTH_EQUATORIAL_RADIUS_M\n    density = _atmospheric_density(altitude, settings.solar_flux_index)\n    drag = np.zeros(3, dtype=float)\n\n    if speed_rel > 0.0 and density > 0.0:\n        ballistic = state.area_m2 / state.mass_kg\n        drag = (\n            -0.5\n            * density\n            * state.drag_coefficient\n            * ballistic\n            * speed_rel\n            * v_rel\n        )\n\n    return central + accel_j2 + drag\n\n\ndef _atmospheric_density(altitude_m: float, solar_index: float) -> float:\n    \"\"\"Return a simple exponential atmospheric density profile.\"\"\"\n\n    base_density = 3.614e-11  # kg/m^3 at 400 km reference\n    scale_height = 55_000.0 * math.sqrt(max(solar_index, 1.0) / SOLAR_FLUX_BASE)\n    density = base_density * math.exp(-(altitude_m - 400_000.0) / scale_height)\n    return max(density, 0.0)\n\n\ndef _run_monte_carlo(\n    leader_elements: OrbitalElements,\n    formation: Sequence[Mapping[str, float | str]],\n    settings: PropagatorSettings,\n    target_lat: float,\n    target_lon: float,\n    monte_carlo: Mapping[str, object],\n) -> MutableMapping[str, object]:\n    \"\"\"Execute Monte Carlo dispersions and aggregate cross-track statistics.\"\"\"\n\n    if not monte_carlo.get(\"enabled\", True):\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n\n    runs = int(monte_carlo.get(\"runs\", 0))\n    if runs <= 0:\n        return {\n            \"runs\": 0,\n            \"max_abs_cross_track_km\": {},\n            \"fleet_compliance_probability\": 1.0,\n        }\n    runs = max(runs, 500)\n\n    dispersions = monte_carlo.get(\"dispersions\", {})\n    sigma_a = float(dispersions.get(\"semi_major_axis_sigma_m\", 0.0))\n    sigma_i = math.radians(float(dispersions.get(\"inclination_sigma_deg\", 0.0)))\n    sigma_cd = float(dispersions.get(\"drag_coefficient_sigma\", 0.0))\n\n    seed = int(monte_carlo.get(\"seed\", 42))\n    rng = np.random.default_rng(seed)\n\n    vehicle_ids = [entry.get(\"identifier\", \"spacecraft\") for entry in formation]\n    run_metrics_max: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_min: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    run_metrics_eval: dict[str, list[float]] = {str(identifier): [] for identifier in vehicle_ids}\n    intersection_distances: list[float] = []\n    relative_max_list: list[float] = []\n    relative_min_list: list[float] = []\n    centroid_abs_list: list[float] = []\n    worst_abs_list: list[float] = []\n\n    primary_success_count = 0\n    waiver_success_count = 0\n    relative_success_count = 0\n    plane_success_count = 0\n\n    evaluation_time = _resolve_evaluation_time(settings)\n    primary_limit = float(settings.primary_cross_track_limit_km)\n    waiver_limit = float(settings.waiver_cross_track_limit_km)\n    plane_limit = settings.plane_intersection_limit_km\n\n    epoch_offset = (settings.start_time - settings.epoch_time).total_seconds()\n\n    evaluation_iso = evaluation_time.isoformat().replace(\"+00:00\", \"Z\")\n    LOGGER.info(\n        \"Monte Carlo dispersion: seed=%d, runs=%d, evaluation_utc=%s\",\n        seed,\n        runs,\n        evaluation_iso,\n    )\n\n    from tqdm import tqdm\n\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):\n        dispersed_elements = OrbitalElements(\n            semi_major_axis=leader_elements.semi_major_axis + rng.normal(0.0, sigma_a),\n            eccentricity=leader_elements.eccentricity,\n            inclination=leader_elements.inclination + rng.normal(0.0, sigma_i),\n            raan=leader_elements.raan,\n            arg_perigee=leader_elements.arg_perigee,\n            mean_anomaly=leader_elements.mean_anomaly,\n        )\n\n        drag_scale = 1.0 + rng.normal(0.0, sigma_cd)\n        states = _initial_states(\n            dispersed_elements,\n            formation,\n            settings.drag_coefficient * drag_scale,\n            epoch_offset_s=epoch_offset,\n        )\n        plane_metrics = _plane_intersection_metrics(states, target_lat, target_lon)\n        plane_distance = float(plane_metrics.get(\"target_distance_km\", math.inf))\n\n        epoch = settings.start_time\n        dt = settings.time_step_s\n        vehicle_metrics = _initial_metric_structure(states, target_lat, target_lon)\n        previous_signs = {state.identifier: 0.0 for state in states}\n        relative_run_stats = {\"max\": 0.0, \"min\": math.inf}\n        evaluation_snapshot: dict[str, float] = {}\n        evaluation_best = math.inf\n\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            current_values: dict[str, float] = {}\n            for idx, state in enumerate(states):\n                position_ecef = inertial_to_ecef(state.position_m, epoch)\n                latitude, longitude, _ = geodetic_coordinates(position_ecef)\n                distance = haversine_distance(latitude, longitude, target_lat, target_lon)\n                sign = 1.0 if latitude >= target_lat else -1.0\n                cross_track_km = sign * (distance / 1_000.0)\n\n                vehicle_entry = vehicle_metrics[\"vehicles\"][idx]\n                vehicle_entry[\"max_cross_track_km\"] = max(\n                    vehicle_entry[\"max_cross_track_km\"], cross_track_km\n                )\n                vehicle_entry[\"min_cross_track_km\"] = min(\n                    vehicle_entry[\"min_cross_track_km\"], cross_track_km\n                )\n                abs_value = abs(cross_track_km)\n                if abs_value > vehicle_entry[\"max_abs_cross_track_km\"]:\n                    vehicle_entry[\"max_abs_cross_track_km\"] = abs_value\n                if abs_value < vehicle_entry[\"min_abs_cross_track_km\"]:\n                    vehicle_entry[\"min_abs_cross_track_km\"] = abs_value\n\n                current_values[str(state.identifier)] = float(cross_track_km)\n\n                identifier = str(state.identifier)\n                previous_sign = previous_signs[identifier]\n                if previous_sign != 0.0 and previous_sign * cross_track_km < 0.0:\n                    vehicle_entry[\"pass_count\"] += 1\n                previous_signs[identifier] = cross_track_km\n\n            diff = abs((epoch - evaluation_time).total_seconds())\n            if diff < evaluation_best:\n                evaluation_best = diff\n                evaluation_snapshot = dict(current_values)\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n\n            leader_state = next((state for state in states if state.identifier == \"FSAT-LDR\"), None)\n            plane_b_state = next(\n                (state for state in states if PLANE_ASSIGNMENTS.get(state.identifier) == \"Plane B\"),\n                None,\n            )\n            if leader_state is not None and plane_b_state is not None:\n                normal_vec = np.cross(leader_state.position_m, leader_state.velocity_mps)\n                norm_normal = np.linalg.norm(normal_vec)\n                if norm_normal > 0.0:\n                    normal_unit = normal_vec / norm_normal\n                    relative_vec = plane_b_state.position_m - leader_state.position_m\n                    cross_track_km = float(np.dot(relative_vec, normal_unit) / 1_000.0)\n                    abs_val = abs(cross_track_km)\n                    if abs_val > relative_run_stats[\"max\"]:\n                        relative_run_stats[\"max\"] = abs_val\n                    if abs_val < relative_run_stats[\"min\"]:\n                        relative_run_stats[\"min\"] = abs_val\n\n        evaluation_abs = {\n            identifier: abs(value) if math.isfinite(value) else math.inf\n            for identifier, value in evaluation_snapshot.items()\n        }\n        centroid_value = (\n            fmean(evaluation_snapshot.values()) if evaluation_snapshot else math.nan\n        )\n        centroid_abs = abs(centroid_value) if math.isfinite(centroid_value) else math.inf\n        worst_abs = max(evaluation_abs.values()) if evaluation_abs else math.inf\n\n        primary_compliant = (\n            math.isfinite(centroid_abs)\n            and centroid_abs <= primary_limit\n            and math.isfinite(worst_abs)\n            and worst_abs <= waiver_limit\n        )\n        waiver_compliant = math.isfinite(worst_abs) and worst_abs <= waiver_limit\n\n        if math.isfinite(centroid_abs):\n            centroid_abs_list.append(centroid_abs)\n        if math.isfinite(worst_abs):\n            worst_abs_list.append(worst_abs)\n\n        for entry in vehicle_metrics[\"vehicles\"]:\n            identifier = str(entry[\"identifier\"])\n            max_abs = float(entry[\"max_abs_cross_track_km\"])\n            min_abs = float(entry.get(\"min_abs_cross_track_km\", math.inf))\n            value = evaluation_snapshot.get(identifier, float(\"nan\"))\n            abs_value = evaluation_abs.get(identifier, math.inf)\n            entry[\"cross_track_at_evaluation_km\"] = (\n                float(value) if math.isfinite(value) else None\n            )\n            entry[\"abs_cross_track_at_evaluation_km\"] = (\n                float(abs_value) if math.isfinite(abs_value) else None\n            )\n            entry[\"compliant\"] = bool(\n                math.isfinite(abs_value) and abs_value <= waiver_limit\n            )\n            run_metrics_max[identifier].append(max_abs)\n            if math.isfinite(min_abs):\n                run_metrics_min[identifier].append(min_abs)\n            if math.isfinite(abs_value):\n                run_metrics_eval[identifier].append(abs_value)\n\n        plane_success = True\n        if math.isfinite(plane_distance):\n            if plane_limit is not None:\n                intersection_distances.append(plane_distance)\n                plane_success = plane_distance <= plane_limit\n        elif plane_limit is not None:\n            plane_success = False\n\n        if plane_success:\n            plane_success_count += 1\n\n        if math.isfinite(relative_run_stats[\"max\"]):\n            relative_max_list.append(relative_run_stats[\"max\"])\n        if math.isfinite(relative_run_stats[\"min\"]):\n            relative_min_list.append(relative_run_stats[\"min\"])\n\n        if primary_compliant:\n            primary_success_count += 1\n        if waiver_compliant:\n            waiver_success_count += 1\n        if relative_run_stats[\"max\"] <= waiver_limit:\n            relative_success_count += 1\n\n    aggregated: MutableMapping[str, object] = {\n        \"runs\": runs,\n        \"seed\": seed,\n        \"max_abs_cross_track_km\": {},\n        \"min_abs_cross_track_km\": {},\n        \"evaluation_abs_cross_track_km\": {},\n        \"relative_cross_track_km\": {},\n        \"compliance\": {\n            \"primary_fraction\": float(primary_success_count / runs) if runs else 0.0,\n            \"waiver_fraction\": float(waiver_success_count / runs) if runs else 0.0,\n            \"relative_fraction\": float(relative_success_count / runs) if runs else 0.0,\n        },\n        \"fleet_compliance_probability\": float(primary_success_count / runs)\n        if runs\n        else 0.0,\n        \"absolute_cross_track_compliance_probability\": float(\n            waiver_success_count / runs\n        )\n        if runs\n        else 0.0,\n        \"evaluation_time_utc\": evaluation_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"cross_track_limits_km\": {\n            \"primary\": float(primary_limit),\n            \"waiver\": float(waiver_limit),\n        },\n    }\n\n    if plane_limit is not None:\n        aggregated[\"plane_intersection_distance_km\"] = {}\n\n    if plane_limit is not None:\n        aggregated[\"compliance\"][\"plane_fraction\"] = float(\n            plane_success_count / runs\n        ) if runs else 0.0\n\n    for identifier, values in run_metrics_max.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"max_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_min.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"min_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    for identifier, values in run_metrics_eval.items():\n        if not values:\n            continue\n        array = np.asarray(values, dtype=float)\n        aggregated[\"evaluation_abs_cross_track_km\"][identifier] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if plane_limit is not None and intersection_distances:\n        array = np.asarray(intersection_distances, dtype=float)\n        aggregated[\"plane_intersection_distance_km\"][\"fleet\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if relative_max_list:\n        array = np.asarray(relative_max_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_max\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n    if relative_min_list:\n        array = np.asarray(relative_min_list, dtype=float)\n        aggregated[\"relative_cross_track_km\"][\"fleet_relative_min\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    if centroid_abs_list:\n        array = np.asarray(centroid_abs_list, dtype=float)\n        centroid_stats = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n        aggregated[\"centroid_abs_cross_track_km\"] = centroid_stats\n        aggregated[\"centroid_abs_cross_track_km_mean\"] = centroid_stats[\"mean\"]\n        aggregated[\"centroid_abs_cross_track_km_p95\"] = centroid_stats[\"p95\"]\n    if worst_abs_list:\n        array = np.asarray(worst_abs_list, dtype=float)\n        aggregated[\"worst_vehicle_abs_cross_track_km\"] = {\n            \"mean\": float(np.mean(array)),\n            \"std\": float(np.std(array)),\n            \"p95\": float(np.percentile(array, 95.0)),\n            \"min\": float(np.min(array)),\n            \"max\": float(np.max(array)),\n        }\n\n    return aggregated\n\n\ndef _write_outputs(\n    output_directory: Path,\n    series: Mapping[str, object],\n    deterministic_metrics: Mapping[str, object],\n    monte_carlo_metrics: Mapping[str, object],\n    settings: PropagatorSettings,\n    monte_carlo_config: Mapping[str, object],\n) -> MutableMapping[str, str]:\n    \"\"\"Persist deterministic and Monte Carlo outputs to the run directory.\"\"\"\n\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    deterministic_path = output_directory / \"deterministic_cross_track.csv\"\n    relative_path = output_directory / \"relative_cross_track.csv\"\n    summary_path = output_directory / \"deterministic_summary.json\"\n    orbital_path = output_directory / \"deterministic_orbital_elements.csv\"\n    monte_carlo_path = output_directory / \"monte_carlo_cross_track.csv\"\n    monte_carlo_summary_path = output_directory / \"monte_carlo_summary.json\"\n    settings_path = output_directory / \"solver_settings.json\"\n\n    _write_cross_track_csv(deterministic_path, series)\n    orbital_series = series.get(\"orbital_elements\") if isinstance(series, Mapping) else {}\n    _write_orbital_elements_csv(orbital_path, orbital_series)\n\n    relative_summary = deterministic_metrics.get(\"relative_cross_track\")\n    if isinstance(relative_summary, Mapping):\n        _write_relative_cross_track_csv(relative_path, relative_summary)\n        relative_summary = dict(relative_summary)\n        relative_summary.pop(\"series\", None)\n        deterministic_metrics[\"relative_cross_track\"] = relative_summary\n    else:\n        _write_relative_cross_track_csv(relative_path, {})\n\n    summary_payload = {\n        \"metrics\": deterministic_metrics,\n        \"settings\": {\n            \"start_time_utc\": settings.start_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"stop_time_utc\": settings.stop_time.isoformat().replace(\"+00:00\", \"Z\"),\n            \"time_step_s\": settings.time_step_s,\n            \"drag_coefficient\": settings.drag_coefficient,\n            \"ballistic_coefficient_m2_per_kg\": settings.ballistic_coefficient_m2_per_kg,\n            \"solar_flux_index\": settings.solar_flux_index,\n        },\n    }\n    summary_path.write_text(json.dumps(summary_payload, indent=2), encoding=\"utf-8\")\n\n    _write_monte_carlo_csv(monte_carlo_path, monte_carlo_metrics)\n    monte_carlo_summary_path.write_text(json.dumps(monte_carlo_metrics, indent=2), encoding=\"utf-8\")\n\n    settings_payload = {\n        \"propagator\": {\n            \"integrator\": \"fixed_step_rk4\",\n            \"time_step_s\": settings.time_step_s,\n            \"force_models\": {\n                \"central_body\": True,\n                \"j2\": True,\n                \"atmospheric_drag\": {\n                    \"model\": \"exponential\",\n                    \"solar_flux_index\": settings.solar_flux_index,\n                },\n            },\n        },\n        \"monte_carlo\": {\n            \"runs\": monte_carlo_config.get(\"runs\", 0),\n            \"seed\": monte_carlo_config.get(\"seed\", 0),\n            \"dispersions\": monte_carlo_config.get(\"dispersions\", {}),\n        },\n        \"epoch_time_utc\": settings.epoch_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n    settings_path.write_text(json.dumps(settings_payload, indent=2), encoding=\"utf-8\")\n\n    orbital_metrics = deterministic_metrics.get(\"orbital_elements\")\n    if isinstance(orbital_metrics, Mapping):\n        time_series_meta = orbital_metrics.get(\"time_series\")\n        if isinstance(time_series_meta, Mapping):\n            time_series_meta[\"artefact\"] = str(orbital_path)\n\n    return {\n        \"deterministic_cross_track_csv\": str(deterministic_path),\n        \"deterministic_orbital_elements_csv\": str(orbital_path),\n        \"deterministic_summary_json\": str(summary_path),\n        \"monte_carlo_cross_track_csv\": str(monte_carlo_path),\n        \"relative_cross_track_csv\": str(relative_path),\n        \"monte_carlo_summary_json\": str(monte_carlo_summary_path),\n        \"solver_settings_json\": str(settings_path),\n    }\n\n\ndef _write_cross_track_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Write deterministic cross-track histories to *path*.\"\"\"\n\n    samples = series.get(\"samples\", [])\n    if not samples:\n        path.write_text(\"time,\\n\", encoding=\"utf-8\")\n        return\n\n    vehicle_ids = sorted(next(iter(samples))[1])\n    header = [\"time_iso\"] + vehicle_ids\n    lines = [\",\".join(header)]\n\n    for timestamp, cross_track in samples:\n        row = [timestamp.isoformat().replace(\"+00:00\", \"Z\")]\n        for identifier in vehicle_ids:\n            row.append(f\"{cross_track.get(identifier, 0.0):.6f}\")\n        lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_orbital_elements_csv(path: Path, series: Mapping[str, object]) -> None:\n    \"\"\"Persist orbital-element histories to *path*.\"\"\"\n\n    header = \"time_iso,satellite_id,\" + \",\".join(CLASSICAL_ELEMENT_FIELDS)\n    if not isinstance(series, Mapping) or not series:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    times = list(series.get(\"times\", []))\n    per_satellite = series.get(\"per_satellite\")\n    if not isinstance(per_satellite, Mapping) or not times:\n        path.write_text(header + \"\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [header]\n    for index, timestamp in enumerate(times):\n        if isinstance(timestamp, datetime):\n            stamp = timestamp.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n        else:\n            stamp = str(timestamp)\n        for identifier in sorted(per_satellite):\n            elements = per_satellite.get(identifier)\n            if not isinstance(elements, Mapping):\n                continue\n            row = [stamp, str(identifier)]\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                value = float(\"nan\")\n                if isinstance(values, Sequence) and index < len(values):\n                    try:\n                        value = float(values[index])  # type: ignore[arg-type]\n                    except (TypeError, ValueError):\n                        value = float(\"nan\")\n                row.append(f\"{value:.9g}\" if math.isfinite(value) else \"\")\n            lines.append(\",\".join(row))\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_monte_carlo_csv(path: Path, metrics: Mapping[str, object]) -> None:\n    \"\"\"Write Monte Carlo aggregate metrics to *path*.\"\"\"\n\n    vehicle_stats = metrics.get(\"max_abs_cross_track_km\", {})\n    min_stats_all = metrics.get(\"min_abs_cross_track_km\", {})\n    plane_stats = metrics.get(\"plane_intersection_distance_km\", {})\n    relative_stats = metrics.get(\"relative_cross_track_km\", {})\n    if (\n        not vehicle_stats\n        and not min_stats_all\n        and not plane_stats\n        and not relative_stats\n    ):\n        path.write_text(\n            \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n            \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n            \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n            \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n            \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n            \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\\n\",\n            encoding=\"utf-8\",\n        )\n        return\n\n    header = (\n        \"vehicle_id,min_abs_cross_track_km_mean,min_abs_cross_track_km_std,min_abs_cross_track_km_p95,\"\n        \"min_abs_cross_track_km_min,min_abs_cross_track_km_max,max_abs_cross_track_km_mean,max_abs_cross_track_km_std,\"\n        \"max_abs_cross_track_km_p95,max_abs_cross_track_km_min,max_abs_cross_track_km_max,\"\n        \"plane_intersection_distance_km_mean,plane_intersection_distance_km_std,plane_intersection_distance_km_p95,\"\n        \"plane_intersection_distance_km_min,plane_intersection_distance_km_max,relative_cross_track_km_mean,\"\n        \"relative_cross_track_km_std,relative_cross_track_km_p95,relative_cross_track_km_min,relative_cross_track_km_max\"\n    )\n    lines = [header]\n\n    identifiers = sorted(\n        set(\n            list(vehicle_stats.keys())\n            + list(min_stats_all.keys())\n            + list(plane_stats.keys())\n            + list(relative_stats.keys())\n        )\n    )\n\n    for identifier in identifiers:\n        stats = vehicle_stats.get(identifier, {})\n        min_stats = min_stats_all.get(identifier, {})\n        plane_entry = plane_stats.get(identifier, {})\n        relative_entry = relative_stats.get(identifier, {})\n        line = \",\".join(\n            [\n                str(identifier),\n                f\"{min_stats.get('mean', 0.0):.6f}\",\n                f\"{min_stats.get('std', 0.0):.6f}\",\n                f\"{min_stats.get('p95', 0.0):.6f}\",\n                f\"{min_stats.get('min', 0.0):.6f}\",\n                f\"{min_stats.get('max', 0.0):.6f}\",\n                f\"{stats.get('mean', 0.0):.6f}\",\n                f\"{stats.get('std', 0.0):.6f}\",\n                f\"{stats.get('p95', 0.0):.6f}\",\n                f\"{stats.get('min', 0.0):.6f}\",\n                f\"{stats.get('max', 0.0):.6f}\",\n                f\"{plane_entry.get('mean', 0.0):.6f}\",\n                f\"{plane_entry.get('std', 0.0):.6f}\",\n                f\"{plane_entry.get('p95', 0.0):.6f}\",\n                f\"{plane_entry.get('min', 0.0):.6f}\",\n                f\"{plane_entry.get('max', 0.0):.6f}\",\n                f\"{relative_entry.get('mean', 0.0):.6f}\",\n                f\"{relative_entry.get('std', 0.0):.6f}\",\n                f\"{relative_entry.get('p95', 0.0):.6f}\",\n                f\"{relative_entry.get('min', 0.0):.6f}\",\n                f\"{relative_entry.get('max', 0.0):.6f}\",\n            ]\n        )\n        lines.append(line)\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _write_relative_cross_track_csv(path: Path, summary: Mapping[str, object]) -> None:\n    \"\"\"Write relative cross-track series to *path*.\"\"\"\n\n    series = summary.get(\"series\") if isinstance(summary, Mapping) else None\n    if not series:\n        path.write_text(\"time_iso,relative_cross_track_km\\n\", encoding=\"utf-8\")\n        return\n\n    lines = [\"time_iso,relative_cross_track_km\"]\n    for time_iso, value in series:\n        lines.append(f\"{time_iso},{float(value):.6f}\")\n\n    path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n\n\ndef _parse_time(value: object) -> datetime | None:\n    \"\"\"Parse an ISO-8601 timestamp into an aware :class:`datetime`.\"\"\"\n\n    if isinstance(value, str):\n        try:\n            if value.endswith(\"Z\"):\n                return datetime.fromisoformat(value.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n            parsed = datetime.fromisoformat(value)\n            if parsed.tzinfo is None:\n                return parsed.replace(tzinfo=timezone.utc)\n            return parsed.astimezone(timezone.utc)\n        except ValueError:\n            return None\n    return None\n\n\n__all__ = [\n    \"PropagatorSettings\",\n    \"propagate_constellation\",\n    \"scenario_cross_track_limits\",\n    \"scenario_access_window\",\n]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjUB4/H/XnjjLlrYHWf9f9VqzxEO6Ox6DSbfJ0H5iFjd190ofIicgs/EQ6VsK5qb+3j3rVOEVApvAePx/16TzrclstNpcifwnjGr1yn1WKd3q7+zHcRhFMB8jgP8tZYGgYUxnmDZH8BSXmiDHUhmjI7gJAsMqC4/4tU+MbrgTdYBk236VBMTIuoaUa4I0Bm4NpKizUjtILkfzrwQ7pyZGzPF7ODLiYPmCmMB4/H/Xul6oOozZoqgoVeQ1EV0J0acELiLiSdOwaHFQXLwYYy8SrBFhdxfcD29YR0L4ufV/ViXSjGy1b2qJgEQ6i3w4siVa53AX0JyJo3ywVtad4Y4QUq7GI8cxJ0obZI7oRAKoAEB4/H/Xp4XzstoKRx1NQpobQfairDVtdzTo9VQW/MksOXHljhIgmc54kMoqX3kOZ49fXywdQh2A/2iw41/RLdULKkepqAyTUO/+ng4XZCzzehe7EjQK5CvlQeNvm9xlMyP+a53aa3UstQ9fd1anXO2L9LrtYyTHwNlbaq4ffnhlv3Vr5MwlFeGP3crt0jtBtYHhiH43JMF6NqaIISy/VhcCpsBAePx/17eO63XzkpgRqfNgWu6lcB4wB5dCJTfdP+Imsg6W4639YZ3ewD90EY3UkxYFK9BzE7w7Zx/CaIzV+EpRL6ZProTUTSDChvRMvWXfbciWcxIHGiaOoNtfpP/tc2jP9hKWKMQrO0VTI3TQxqGwspSNQ55HalT/ctwVeeAmH/oCP7ZSD/7FGGJucxYZt/L1HO9cRJ841RWir0K1gEB4/H/XrE/q5VNWkTWD5994/tJbynoT4OAxd6F8B1s7FxDZ51zpURrU5ecnQlQei2ocLyStHDoy6eIJIHZTwH8V8hdQ02MDrT5/8/5jXGcmR1WQdmCzvmjociWsU+N/ynSIG8axQhiu6ncMSQnMcaDOz6/Nn9qEWkCc4rwSf7X74b2pqSdKWiJ+uDmvQIxy0LZLEUGhaWCps2DM902tz5JDZhljdnoCaRSreQ0bP+ebw+36jKVXGGYfYEESeZSiQ0a3Kbge4TGxihg1dbuQr/Ovnre4uJzCtkBAePx/14g1cjwSFdRhJEMUel37L12t70+VDcIau7UdPp1QlzIVkG1MxrpYkT0NuuksLufTYcWp/gE79Z968RtjqWHYNkXBkElXRbc1YImgvXQihzyEsrwzivKIwmkdeqtUgxZh2RY94N+xVUJ+jMF06ofwn2BJmQ7nuwxer0JQJo5to2LOyu9NGmz2yBjYs9GE5cK+862Zwmodxj4lEcY8IWnehJkoWv7wcSnW8YPEXZcUhrbfTxvohFj82jZaG4LtiPdqycIIjqZWWOymi5B4XUR3rioUYlPogr4AQHj8f9ea1mGRaPRQ6f3yfs792EK++5ffKxnXnttY2NI9+HB3ndur4AGx8iAuRN4svpHKYmoOrR5B3WSWBAjFhYldHLJdnwC4iHPWKnlW9lm73LUNA+QxVhIftpEMQtqDMTAwRv2j7KCL7QRNq8w407UFVyIjYdpn4yF1PXdgzA/CjhnvEWdIC6/sHpPiewhNeoVC1z5dTk+UEvAHpzzlbEnr60bANH4LuJfoSVm/+GksNiCYo9mmQKmZS1oCpFGFJMNw357n96Nm1XKrlVwTC2pE+m0Pk3MGnHO5pQc/6I+NZSrzsukVe12AJF4ot1MM4w52smS5oPZ",
        "text": "Okay, `perturbation_analysis.py` has duplicated code. I'll remove the redundant `tqdm` block in `_propagate_deterministic` and then apply the missing change to `_run_monte_carlo`. Two steps: fix `_propagate_deterministic`, then `_run_monte_carlo`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "instruction": "Remove the duplicated tqdm progress bar from the deterministic propagation loop.",
            "old_string": "    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n            from tqdm import tqdm\n            n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n            with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n                while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n                    times.append(epoch)\n                    for state in states:\n                        elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                        per_satellite = orbital_elements_series[state.identifier]\n                        per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                        per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                        per_satellite[\"inclination_deg\"].append(\n                            _normalise_angle_degrees(elements.inclination)\n                        )\n                        per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                        per_satellite[\"argument_of_perigee_deg\"].append(\n                            _normalise_angle_degrees(elements.arg_perigee)\n                        )\n                        per_satellite[\"mean_anomaly_deg\"].append(\n                            _normalise_angle_degrees(elements.mean_anomaly)\n                        )\n                    _record_metrics(\n                        states,\n                        epoch,\n                        target_lat,\n                        target_lon,\n                        metrics,\n                        cross_track_series,\n                        altitude_series,\n                        relative_stats,\n                    )\n        \n                    if epoch >= settings.stop_time:\n                        break\n        \n                    states = _rk4_step(states, dt, settings)\n                    epoch += timedelta(seconds=dt)\n                    pbar.update(1)            pbar.update(1)",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "new_string": "    from tqdm import tqdm\n    n_steps = int(round((settings.stop_time - settings.start_time).total_seconds() / dt))\n    with tqdm(total=n_steps + 1, desc=\"Deterministic propagation\") as pbar:\n        while epoch <= settings.stop_time + timedelta(seconds=1e-6):\n            times.append(epoch)\n            for state in states:\n                elements = cartesian_to_classical(state.position_m, state.velocity_mps)\n                per_satellite = orbital_elements_series[state.identifier]\n                per_satellite[\"semi_major_axis_km\"].append(elements.semi_major_axis / 1_000.0)\n                per_satellite[\"eccentricity\"].append(elements.eccentricity)\n                per_satellite[\"inclination_deg\"].append(\n                    _normalise_angle_degrees(elements.inclination)\n                )\n                per_satellite[\"raan_deg\"].append(_normalise_angle_degrees(elements.raan))\n                per_satellite[\"argument_of_perigee_deg\"].append(\n                    _normalise_angle_degrees(elements.arg_perigee)\n                )\n                per_satellite[\"mean_anomaly_deg\"].append(\n                    _normalise_angle_degrees(elements.mean_anomaly)\n                )\n            _record_metrics(\n                states,\n                epoch,\n                target_lat,\n                target_lon,\n                metrics,\n                cross_track_series,\n                altitude_series,\n                relative_stats,\n            )\n\n            if epoch >= settings.stop_time:\n                break\n\n            states = _rk4_step(states, dt, settings)\n            epoch += timedelta(seconds=dt)\n            pbar.update(1)"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793608622-4bfc95ea4bc28",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjYB4/H/XgJx4TMHjR+uXT00FvmBGa/qyDzTGfhe99KglQJ0E/59uou9LcWnYvRbzORxL8/Tcv8KdAHj8f9ePLK93EGlHmQkQNmWAAOk5DzZeBKyvLcuxWpN+jJBPO0Bho9jANYrCiz14aQZv48BYDSRCUr5hXwdIioxGBBfRLoR+7uOH7Q8LWzngOq7mUKzmzIlVy4qQoFZzt+dPUVvhQudCYyV4DV3r7Kt3YNnCjQB4/H/XgjY+0brjso4b5+/+5/9L6JNzq9CJA3SOUFqzC0Uo8e/07GZYc3oNNRDWi6oNDbE",
        "text": "Fixed `_propagate_deterministic`, now adding progress bar to `_run_monte_carlo`."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "file_path": "/home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py",
            "instruction": "Add a tqdm progress bar to the Monte Carlo simulation loop.",
            "old_string": "    for _ in range(runs):",
            "new_string": "    from tqdm import tqdm\n    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762793612636-5a366feb4a164",
          "name": "replace",
          "response": {
            "error": "A secondary check by an LLM determined that no changes were necessary to fulfill the instruction. Explanation: The original search failed because the target line `    for _ in range(runs):` was not found. Upon inspecting the file, it appears the intended change (adding the `tqdm` progress bar) has already been applied. The line `    for _ in tqdm(range(runs), desc=\"Monte Carlo simulation\"):` is already present in the file. Therefore, no further changes are required.. Original error with the parameters given: Failed to edit, 0 occurrences found for old_string (    for _ in range(runs):). Original old_string was (    for _ in range(runs):) in /home/diannenewton170/formation-sat-2/sim/scripts/perturbation_analysis.py. No edits made. The exact text in old_string was not found. Ensure you're not escaping content incorrectly and check whitespace, indentation, and context. Use read_file tool to verify."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CiQB4/H/XowqtMVXbG1UftfcUufsNIBMxd6XfQFrf6Ni8wD8s1kKZAHj8f9e93jj9PaCwlXoUBMYKPS6JZ0yez7O90x/I2uFi2DNoF9LYPf6wXSp/SGc1xRXPDNQflEt0hNlA6vebMuMjQPbWQXxQXUG8TJrFgS85PBw+ryAVidjKT99X3Zz3RXwtH8KgAEB4/H/XtUJIZI6PGamO/oQXiDBoI+a2DC18Lep3Hn2i/eIa1epBnpJY8OoUIuzOoSOPvvzDgWvjKwf7178Uez792ctnHB/ln6LTzf8BYT/LfJJFhnVrwufAg2NcJW8AqdM/4wsEdbOgVC5YZZnpl8ELWKz9Exqy02YhckhUPYpiAqoAQHj8f9e29/rhYDKcMge7CQQnbbJUm+0ME4Xu8gdqBnQ63YSyou4YNFO4WDp2Y9Vbpxfp8jM90W+a6TVGBYtbiq3AMDsJ0dVQKU1C5IzKeNIHDnwqwG34RBBHt3FAkwldyPl6yha5Rq9B5/QcB3zNcHQRuEy+nqeJp8AhZm2LqfUlMYQL+O6aIWSTWdjlV1PzbLXrFfr6ifNR2h/37bZWNnWX7Z7s6PkfAqTAQHj8f9ejZzkXcoY1cGftqY65HLPIjrjQQIdpgO+j2scSI1+jaHOlj7UHQeEQYKqlVMmRnpMKu3yrnxvxHCaIYEMc/b9ubMvDsrcKedXdWFWj6eaCrbsxkXt4tSWPPJsbMw37SmMwEWS7VhxpvoFJuEFkb0c8Xh/P9KwQhtMMLFf05f1HNfLTiUpi4d3TSA9qCJXLwraAQHj8f9eyiLqaWPQ417dTrD1WWkBLPUiYmo5yVEGj/RvjmHd+l5qGUbZdqASV+Dmuk5dDr+ateJboS3elEXxZBP/Pd4FP6R6ncKrR+HOJ8u4ZRvwMuwGsquphzn5qN/MoYoCLBgcdZHY+J6M99E12Z4M5FwGmKGBqLzGcvxatTCcXLz9DTzU/rlCVUmfWa8TtW5hj6F8TzgkmEKtfzdKTs/jIP/ZnugJ3tvN5njVBkw0Ib/qOVWD9fsw65iYqk4HfP0sj1oOYR3ygUMFL70oKbvmJFhQCM11nEtvCi8B4/H/Xkzw6RAFhKEJU36WMLkogMJBV3P8vVLia7ZEdc1BoPLNMAqFl65AfUaScQ==",
        "text": "`replace` is failing. I'll run the simulation. If the progress bar appears, the changes are already applied. Otherwise, I'll notify the user I can't add it. Now, I'm running the 14-day simulation with the same command, expecting a progress bar. If it works, I'll analyze the results and create the markdown file."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "description": "Running the 14-day triangle formation simulation again with progress bars.",
            "command": ".venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762793644630-fc6c17158f2ad",
          "name": "run_shell_command",
          "response": {
            "error": "[Operation Cancelled] Reason: User cancelled tool execution."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "ببین بازم میگه ╭────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│ ⊶  Shell .venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day (R… (ctrl+f to focus) │\n│                                                                                                                                                                            │\n╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n⠋ Assessing the Tool's Performance (esc to cancel, 22m 41s)"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "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",
        "text": "The simulation is hanging before the progress bar, likely due to massive upfront memory allocation for the 1.2 million time steps. Creating the `times` list and pre-allocating large NumPy arrays for all simulation data is likely causing excessive memory pressure and swapping.\n\nTo fix this, I will refactor `simulate_triangle_formation` in `sim/formation/triangle.py`. Instead of pre-allocating, I will initialize empty lists for all time-series data. Inside the main loop, I'll calculate the current time and append all new data points to their respective lists. After the loop completes, I will convert these lists into the necessary NumPy arrays. This should significantly reduce the initial memory footprint, allowing the simulation to start and the progress bar to appear.\n\nI will now read the file and then replace the entire function with the optimized version."
      },
      {
        "functionCall": {
          "name": "read_file",
          "args": {
            "absolute_path": "/home/diannenewton170/formation-sat-2/sim/formation/triangle.py"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "read_file-1762794548015-ba98a1a72f12f",
          "name": "read_file",
          "response": {
            "output": "\"\"\"Simulation of a three-satellite triangular formation over Tehran.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport math\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Iterable, Mapping, MutableMapping, Optional, Sequence\n\nimport numpy as np\nimport pandas as pd\n\nSECONDS_PER_DAY = 86_400.0\nSECONDS_PER_YEAR = SECONDS_PER_DAY * 365.25\n\nCLASSICAL_ELEMENT_FIELDS = (\n    \"semi_major_axis_km\",\n    \"eccentricity\",\n    \"inclination_deg\",\n    \"raan_deg\",\n    \"argument_of_perigee_deg\",\n    \"mean_anomaly_deg\",\n)\n\nfrom src.constellation.geometry import (\n    triangle_area,\n    triangle_aspect_ratio,\n    triangle_side_lengths,\n)\nfrom src.constellation.orbit import (\n    EARTH_EQUATORIAL_RADIUS_M,\n    cartesian_to_classical,\n    geodetic_coordinates,\n    haversine_distance,\n    inertial_to_ecef,\n    propagate_kepler,\n)\nfrom src.constellation.roe import MU_EARTH, OrbitalElements\nfrom tools.stk_export import (\n    FacilityDefinition,\n    GroundContactInterval,\n    GroundTrack,\n    GroundTrackPoint,\n    PropagatedStateHistory,\n    ScenarioMetadata,\n    SimulationResults,\n    StateSample,\n    sanitize_stk_identifier,\n    unique_stk_names,\n    export_simulation_to_stk,\n)\n\n\n@dataclass\nclass TriangleFormationResult:\n    \"\"\"Container collecting the outputs of the triangle simulation.\"\"\"\n\n    times: Sequence[datetime]\n    positions_m: Mapping[str, np.ndarray]\n    velocities_mps: Mapping[str, np.ndarray]\n    classical_elements: Mapping[str, Mapping[str, np.ndarray]]\n    latitudes_rad: Mapping[str, np.ndarray]\n    longitudes_rad: Mapping[str, np.ndarray]\n    altitudes_m: Mapping[str, np.ndarray]\n    triangle_area_m2: np.ndarray\n    triangle_aspect_ratio: np.ndarray\n    triangle_sides_m: np.ndarray\n    centroid_lat_rad: np.ndarray\n    centroid_lon_rad: np.ndarray\n    centroid_alt_m: np.ndarray\n    max_ground_distance_km: np.ndarray\n    min_command_distance_km: np.ndarray\n    metrics: Mapping[str, object]\n    artefacts: Mapping[str, Optional[str]]\n\n    def to_summary(self) -> MutableMapping[str, object]:\n        \"\"\"Convert the simulation result into a JSON-serialisable mapping.\"\"\"\n\n        samples: list[MutableMapping[str, object]] = []\n        satellite_ids = sorted(self.positions_m)\n        geometry: MutableMapping[str, object] = {\n            \"times\": [epoch.isoformat().replace(\"+00:00\", \"Z\") for epoch in self.times],\n            \"satellite_ids\": satellite_ids,\n            \"positions_m\": {\n                sat_id: self.positions_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"classical_element_fields\": list(CLASSICAL_ELEMENT_FIELDS),\n            \"classical_element_units\": {\n                \"semi_major_axis_km\": \"km\",\n                \"eccentricity\": \"\",\n                \"inclination_deg\": \"deg\",\n                \"raan_deg\": \"deg\",\n                \"argument_of_perigee_deg\": \"deg\",\n                \"mean_anomaly_deg\": \"deg\",\n            },\n            \"classical_elements\": {\n                sat_id: {\n                    element: self.classical_elements[sat_id][element].tolist()\n                    for element in self.classical_elements[sat_id]\n                }\n                for sat_id in satellite_ids\n            },\n            \"latitudes_rad\": {\n                sat_id: self.latitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"longitudes_rad\": {\n                sat_id: self.longitudes_rad[sat_id].tolist() for sat_id in satellite_ids\n            },\n            \"altitudes_m\": {\n                sat_id: self.altitudes_m[sat_id].tolist() for sat_id in satellite_ids\n            },\n        }\n        geometry[\"max_ground_distance_km\"] = [\n            float(value) for value in self.max_ground_distance_km\n        ]\n        geometry[\"min_command_distance_km\"] = [\n            float(value) for value in self.min_command_distance_km\n        ]\n        for index, epoch in enumerate(self.times):\n            samples.append(\n                {\n                    \"time\": epoch.isoformat().replace(\"+00:00\", \"Z\"),\n                    \"triangle_area_m2\": float(self.triangle_area_m2[index]),\n                    \"triangle_aspect_ratio\": float(self.triangle_aspect_ratio[index]),\n                    \"triangle_side_lengths_m\": [\n                        float(val) for val in self.triangle_sides_m[index]\n                    ],\n                    \"centroid\": {\n                        \"latitude_deg\": math.degrees(self.centroid_lat_rad[index]),\n                        \"longitude_deg\": math.degrees(self.centroid_lon_rad[index]),\n                        \"altitude_m\": float(self.centroid_alt_m[index]),\n                    },\n                    \"max_ground_distance_km\": float(self.max_ground_distance_km[index]),\n                }\n            )\n\n        return {\n            \"metrics\": self.metrics,\n            \"samples\": samples,\n            \"artefacts\": dict(self.artefacts),\n            \"geometry\": geometry,\n        }\n\n\ndef simulate_triangle_formation(\n    config_source: Mapping[str, object] | Path | str,\n    output_directory: Optional[Path | str] = None,\n) -> TriangleFormationResult:\n    \"\"\"Simulate the triangular formation described by *config_source*.\"\"\"\n\n    configuration = _load_configuration(config_source)\n    formation = configuration[\"formation\"]\n    metadata = configuration.get(\"metadata\", {})\n\n    satellite_elements: dict[str, OrbitalElements] = {}\n    if \"satellites\" in configuration:\n        satellite_ids = [sat[\"id\"] for sat in configuration[\"satellites\"]]\n        plane_allocations = {\n            sat[\"id\"]: sat[\"plane\"] for sat in configuration[\"satellites\"]\n        }\n        epoch = _parse_time(\n            configuration[\"satellites\"][0][\"orbital_elements\"][\"epoch_utc\"]\n        )\n        semi_major_axis_m = (\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"][\"semi_major_axis_km\"]\n            )\n            * 1_000.0\n        )\n        inclination = math.radians(\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"].get(\n                    \"inclination_deg\", 0.0\n                )\n            )\n        )\n\n        for sat_config in configuration[\"satellites\"]:\n            elements = sat_config[\"orbital_elements\"]\n            satellite_elements[sat_config[\"id\"]] = OrbitalElements(\n                semi_major_axis=float(elements[\"semi_major_axis_km\"]) * 1_000.0,\n                eccentricity=float(elements.get(\"eccentricity\", 0.0)),\n                inclination=math.radians(float(elements.get(\"inclination_deg\", 0.0))),\n                raan=math.radians(float(elements.get(\"raan_deg\", 0.0))),\n                arg_perigee=math.radians(\n                    float(elements.get(\"argument_of_perigee_deg\", 0.0))\n                ),\n                mean_anomaly=math.radians(float(elements.get(\"mean_anomaly_deg\", 0.0))),\n            )\n    else:\n        reference = configuration[\"reference_orbit\"]\n        epoch = _parse_time(reference[\"epoch_utc\"])\n        semi_major_axis_m = float(reference[\"semi_major_axis_km\"]) * 1_000.0\n        eccentricity = float(reference.get(\"eccentricity\", 0.0))\n        inclination = math.radians(float(reference.get(\"inclination_deg\", 0.0)))\n        raan = math.radians(float(reference.get(\"raan_deg\", 0.0)))\n        arg_perigee = math.radians(\n            float(reference.get(\"argument_of_perigee_deg\", 0.0))\n        )\n        mean_anomaly = math.radians(float(reference.get(\"mean_anomaly_deg\", 0.0)))\n\n        reference_elements = OrbitalElements(\n            semi_major_axis=semi_major_axis_m,\n            eccentricity=eccentricity,\n            inclination=inclination,\n            raan=raan,\n            arg_perigee=arg_perigee,\n            mean_anomaly=mean_anomaly,\n        )\n        offsets_m = _formation_offsets(float(formation[\"side_length_m\"]))\n        satellite_ids = tuple(sorted(offsets_m))\n\n        plane_allocations = formation.get(\"plane_allocations\", {})\n        if not plane_allocations:\n            plane_allocations = {\n                satellite_ids[0]: \"Plane A\",\n                satellite_ids[1]: \"Plane A\",\n                satellite_ids[2]: \"Plane B\",\n            }\n\n        # Get initial state of the reference orbit\n        ref_pos_at_epoch, ref_vel_at_epoch = propagate_kepler(reference_elements, 0)\n\n        # Get initial LVLH frame\n        frame_at_epoch = _lvlh_frame(ref_pos_at_epoch, ref_vel_at_epoch)\n\n        # Calculate initial orbital elements for each satellite\n        for sat_id in satellite_ids:\n            offset_vec = frame_at_epoch @ offsets_m[sat_id]\n            # For a stable formation with zero radial offset, the initial relative velocity in the LVLH frame should be zero.\n            offset_vel = np.array([0.0, 0.0, 0.0])\n\n            sat_pos = ref_pos_at_epoch + offset_vec\n            sat_vel = ref_vel_at_epoch + offset_vel\n            satellite_elements[sat_id] = cartesian_to_classical(sat_pos, sat_vel)\n\n    duration_s = float(formation.get(\"duration_s\", 180.0))\n    time_step_s = float(formation.get(\"time_step_s\", 1.0))\n    half_duration = 0.5 * duration_s\n    sample_count = int(round(duration_s / time_step_s)) + 1\n\n    offsets = np.arange(sample_count, dtype=float) * time_step_s - half_duration\n    times = [epoch + timedelta(seconds=float(offset)) for offset in offsets]\n\n    positions: dict[str, np.ndarray] = {\n        sat_id: np.zeros((sample_count, 3), dtype=float) for sat_id in satellite_ids\n    }\n    latitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    longitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    altitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n\n    triangle_area_series = np.zeros(sample_count, dtype=float)\n    triangle_aspect_series = np.zeros(sample_count, dtype=float)\n    triangle_sides_series = np.zeros((sample_count, 3), dtype=float)\n    centroid_positions = np.zeros((sample_count, 3), dtype=float)\n    centroid_latitudes = np.zeros(sample_count, dtype=float)\n    centroid_longitudes = np.zeros(sample_count, dtype=float)\n    centroid_altitudes = np.zeros(sample_count, dtype=float)\n    max_ground_distance = np.zeros(sample_count, dtype=float)\n    min_command_distance = np.full(sample_count, np.inf, dtype=float)\n\n    target = formation.get(\"target\", {})\n    target_lat = math.radians(float(target.get(\"latitude_deg\", 0.0)))\n    target_lon = math.radians(float(target.get(\"longitude_deg\", 0.0)))\n\n    command = formation.get(\"command\", {})\n    command_station = command.get(\"station\", {})\n    default_lat_deg = math.degrees(target_lat)\n    default_lon_deg = math.degrees(target_lon)\n    command_lat = math.radians(float(command_station.get(\"latitude_deg\", default_lat_deg)))\n    command_lon = math.radians(float(command_station.get(\"longitude_deg\", default_lon_deg)))\n\n    # Propagate each satellite independently\n    velocities_temp: dict[str, list[np.ndarray]] = {sat_id: [] for sat_id in satellite_ids}\n\n    perturbations = formation.get(\"perturbations\", {})\n    position_noise_sigma_m = float(perturbations.get(\"position_noise_sigma_m\", 0.0))\n\n    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    from tqdm import tqdm\n    for index, delta_t in enumerate(tqdm(offsets, desc=\"Propagating orbits\")):\n        inertial_positions = {}\n        for sat_id in satellite_ids:\n            pos, vel = propagate_kepler(satellite_elements[sat_id], delta_t)\n\n            # if position_noise_sigma_m > 0.0:\n            #     noise = rng.normal(0.0, position_noise_sigma_m, size=3)\n            #     pos += noise\n\n            positions[sat_id][index] = pos\n            velocities_temp[sat_id].append(vel)\n            inertial_positions[sat_id] = pos\n\n            ecef = inertial_to_ecef(pos, times[index])\n            lat, lon, alt = geodetic_coordinates(ecef)\n            latitudes[sat_id][index] = lat\n            longitudes[sat_id][index] = lon\n            altitudes[sat_id][index] = alt\n\n        # Now, compute triangle geometry based on the actual positions\n        # The order of vertices matters for side length calculations, so sort by sat_id\n        vertices = [inertial_positions[sat_id] for sat_id in satellite_ids]\n\n        triangle_area_series[index] = triangle_area(vertices)\n        triangle_aspect_series[index] = triangle_aspect_ratio(vertices)\n        triangle_sides_series[index] = triangle_side_lengths(vertices)\n\n        centroid = sum(vertices) / len(vertices)\n        centroid_positions[index] = centroid\n        centroid_ecef = inertial_to_ecef(centroid, times[index])\n        centroid_lat, centroid_lon, centroid_alt = geodetic_coordinates(centroid_ecef)\n        centroid_latitudes[index] = centroid_lat\n        centroid_longitudes[index] = centroid_lon\n        centroid_altitudes[index] = centroid_alt\n\n        max_distance = 0.0\n        min_command = np.inf\n        for sat_id in satellite_ids:\n            distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                target_lat,\n                target_lon,\n            )\n            max_distance = max(max_distance, distance)\n            command_distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                command_lat,\n                command_lon,\n            )\n            min_command = min(min_command, command_distance)\n        max_ground_distance[index] = max_distance / 1_000.0\n        min_command_distance[index] = min_command\n\n    velocities: dict[str, np.ndarray] = {\n        sat_id: np.array(velocities_temp[sat_id]) for sat_id in satellite_ids\n    }\n\n    classical_series = _compute_classical_elements_series(positions, velocities)\n\n    finite_mask = np.isfinite(min_command_distance)\n    min_command_distance_km = np.where(\n        finite_mask, min_command_distance / 1_000.0, np.inf\n    )\n\n    centre_index = int(np.argmin(np.abs(offsets)))\n    orbital_elements = {}\n    for sat_id in satellite_ids:\n        elements = cartesian_to_classical(\n            positions[sat_id][centre_index], velocities[sat_id][centre_index]\n        )\n        orbital_elements[sat_id] = {\n            \"semi_major_axis_km\": elements.semi_major_axis / 1_000.0,\n            \"eccentricity\": elements.eccentricity,\n            \"inclination_deg\": _normalise_degrees(elements.inclination),\n            \"raan_deg\": _normalise_degrees(elements.raan),\n            \"argument_of_perigee_deg\": _normalise_degrees(elements.arg_perigee),\n            \"mean_anomaly_deg\": _normalise_degrees(elements.mean_anomaly),\n            \"assigned_plane\": plane_allocations.get(sat_id),\n        }\n\n    triangle_stats = _summarise_triangle_metrics(\n        triangle_area_series,\n        triangle_aspect_series,\n        triangle_sides_series,\n    )\n    ground_stats = _summarise_ground_metrics(max_ground_distance, formation)\n    maintenance = _estimate_maintenance_delta_v(\n        positions,\n        times,\n        formation,\n        semi_major_axis_m,\n    )\n    command_latency = _analyse_command_latency(\n        min_command_distance_km,\n        times,\n        formation,\n        semi_major_axis_m,\n        command_lat,\n        command_lon,\n    )\n    injection_recovery, injection_samples = _run_injection_recovery_monte_carlo(\n        satellite_ids,\n        formation,\n    )\n    drag_dispersion, drag_samples = _run_atmospheric_drag_dispersion_monte_carlo(\n        formation,\n        semi_major_axis_m,\n        inclination,\n    )\n\n    window, window_series = _formation_window(\n        triangle_aspect_series,\n        max_ground_distance,\n        time_step_s,\n        formation,\n        times,\n    )\n\n    recurrence = _summarise_window_recurrence(\n        window_series,\n        semi_major_axis_m,\n    )\n\n    station_keeping = _assess_station_keeping(\n        times,\n        triangle_sides_series,\n        formation,\n        maintenance,\n        time_step_s,\n    )\n\n    metrics: MutableMapping[str, object] = {\n        \"triangle\": triangle_stats,\n        \"ground_track\": ground_stats,\n        \"formation_window\": window,\n        \"formation_windows\": window_series,\n        \"formation_recurrence\": recurrence,\n        \"orbital_elements\": {\n            \"per_satellite\": orbital_elements,\n            \"plane_assignments\": plane_allocations,\n            \"time_series\": {\n                \"fields\": CLASSICAL_ELEMENT_FIELDS,\n                \"units\": {\n                    \"semi_major_axis_km\": \"km\",\n                    \"eccentricity\": \"\",\n                    \"inclination_deg\": \"deg\",\n                    \"raan_deg\": \"deg\",\n                    \"argument_of_perigee_deg\": \"deg\",\n                    \"mean_anomaly_deg\": \"deg\",\n                },\n                \"artefact_key\": \"orbital_elements_csv\",\n                \"per_satellite_directory_key\": \"orbital_elements_directory\",\n            },\n        },\n        \"maintenance\": maintenance,\n        \"station_keeping\": station_keeping,\n        \"command_latency\": command_latency,\n        \"injection_recovery\": injection_recovery,\n        \"drag_dispersion\": drag_dispersion,\n    }\n\n    artefacts: MutableMapping[str, Optional[str]] = {\n        \"summary_path\": None,\n        \"stk_directory\": None,\n        \"maintenance_csv\": None,\n        \"command_windows_csv\": None,\n        \"injection_recovery_csv\": None,\n        \"injection_recovery_plot\": None,\n        \"drag_dispersion_csv\": None,\n        \"formation_windows_csv\": None,\n        \"station_keeping_csv\": None,\n        \"orbital_elements_csv\": None,\n        \"orbital_elements_directory\": None,\n    }\n\n    result = TriangleFormationResult(\n        times=times,\n        positions_m=positions,\n        velocities_mps=velocities,\n        classical_elements=classical_series,\n        latitudes_rad=latitudes,\n        longitudes_rad=longitudes,\n        altitudes_m=altitudes,\n        triangle_area_m2=triangle_area_series,\n        triangle_aspect_ratio=triangle_aspect_series,\n        triangle_sides_m=triangle_sides_series,\n        centroid_lat_rad=centroid_latitudes,\n        centroid_lon_rad=centroid_longitudes,\n        centroid_alt_m=centroid_altitudes,\n        max_ground_distance_km=max_ground_distance,\n        min_command_distance_km=min_command_distance_km,\n        metrics=metrics,\n        artefacts=artefacts,\n    )\n\n    if output_directory is not None:\n        output_path = Path(output_directory)\n        output_path.mkdir(parents=True, exist_ok=True)\n        summary_path = output_path / \"triangle_summary.json\"\n        artefacts[\"summary_path\"] = str(summary_path)\n\n        maintenance_rows = [\n            {\n                \"satellite_id\": sat_id,\n                \"mean_diff_accel_mps2\": data[\"mean_diff_accel_mps2\"],\n                \"peak_diff_accel_mps2\": data[\"peak_diff_accel_mps2\"],\n                \"delta_v_per_burn_mps\": data[\"delta_v_per_burn_mps\"],\n                \"annual_delta_v_mps\": data[\"annual_delta_v_mps\"],\n            }\n            for sat_id, data in maintenance[\"per_spacecraft\"].items()\n        ]\n        maintenance_df = pd.DataFrame(\n            maintenance_rows,\n            columns=[\n                \"satellite_id\",\n                \"mean_diff_accel_mps2\",\n                \"peak_diff_accel_mps2\",\n                \"delta_v_per_burn_mps\",\n                \"annual_delta_v_mps\",\n            ],\n        )\n        maintenance_path = output_path / \"maintenance_summary.csv\"\n        maintenance_df.to_csv(maintenance_path, index=False)\n        artefacts[\"maintenance_csv\"] = str(maintenance_path)\n\n        command_rows = [\n            {\"window_index\": index, **window}\n            for index, window in enumerate(command_latency[\"contact_windows\"])\n        ]\n        command_df = pd.DataFrame(\n            command_rows, columns=[\"window_index\", \"start\", \"end\", \"duration_s\"]\n        )\n        command_path = output_path / \"command_windows.csv\"\n        command_df.to_csv(command_path, index=False)\n        artefacts[\"command_windows_csv\"] = str(command_path)\n\n        injection_path = output_path / \"injection_recovery.csv\"\n        injection_samples.to_csv(\n            injection_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"satellite_id\",\n                \"position_error_m\",\n                \"velocity_error_mps\",\n                \"delta_v_mps\",\n                \"success\",\n            ],\n        )\n        artefacts[\"injection_recovery_csv\"] = str(injection_path)\n\n        drag_path = output_path / \"drag_dispersion.csv\"\n        drag_samples.to_csv(\n            drag_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"density_scale\",\n                \"drag_coefficient\",\n                \"ballistic_coefficient_m2_per_kg\",\n                \"semi_major_axis_delta_m\",\n                \"altitude_delta_m\",\n                \"along_track_shift_km\",\n                \"ground_distance_delta_km\",\n                \"command_distance_delta_km\",\n                \"within_tolerance\",\n            ],\n        )\n        artefacts[\"drag_dispersion_csv\"] = str(drag_path)\n\n        windows_path = output_path / \"formation_windows.csv\"\n        _write_formation_windows_csv(windows_path, window_series)\n        artefacts[\"formation_windows_csv\"] = str(windows_path)\n\n        station_path = output_path / \"station_keeping_events.csv\"\n        _write_station_keeping_csv(station_path, station_keeping)\n        artefacts[\"station_keeping_csv\"] = str(station_path)\n\n        plot_path = output_path / \"injection_recovery_cdf.svg\"\n        _write_injection_recovery_plot(injection_samples, plot_path)\n        if plot_path.exists():\n            artefacts[\"injection_recovery_plot\"] = str(plot_path)\n\n        orbital_path = output_path / \"orbital_elements.csv\"\n        _write_orbital_elements_csv(orbital_path, times, classical_series)\n        artefacts[\"orbital_elements_csv\"] = str(orbital_path)\n        metrics[\"orbital_elements\"][\"time_series\"][\"artefact\"] = str(orbital_path)\n        orbital_sat_dir = output_path / \"orbital_elements\"\n        orbital_sat_dir.mkdir(parents=True, exist_ok=True)\n        per_sat_paths = _write_orbital_elements_per_spacecraft(\n            orbital_sat_dir, times, classical_series\n        )\n        if per_sat_paths:\n            artefacts[\"orbital_elements_directory\"] = str(orbital_sat_dir)\n            metrics[\"orbital_elements\"][\"time_series\"][\"per_satellite_files\"] = {\n                sat_id: str(path)\n                for sat_id, path in sorted(per_sat_paths.items(), key=lambda item: item[0])\n            }\n\n        stk_dir = output_path / \"stk\"\n        _export_to_stk(\n            result,\n            stk_dir,\n            scenario_name=str(metadata.get(\"scenario_name\", \"Tehran Triangle Formation\")),\n        )\n        artefacts[\"stk_directory\"] = str(stk_dir)\n\n        summary_payload = result.to_summary()\n        summary_path.write_text(\n            json.dumps(summary_payload, indent=2), encoding=\"utf-8\"\n        )\n\n    return result\n\n\ndef _load_configuration(source: Mapping[str, object] | Path | str) -> Mapping[str, object]:\n    if isinstance(source, Mapping):\n        return source\n    path = Path(source)\n    payload = json.loads(path.read_text(encoding=\"utf-8\"))\n    if not isinstance(payload, Mapping):\n        raise TypeError(\"Triangle configuration must be a mapping.\")\n    if \"satellites\" not in payload and \"reference_orbit\" not in payload:\n        raise ValueError(\n            \"Configuration missing required section: satellites or reference_orbit\"\n        )\n    if \"formation\" not in payload:\n        raise ValueError(\"Configuration missing required section: formation\")\n    return payload\n\n\ndef _parse_time(value: object) -> datetime:\n    if isinstance(value, datetime):\n        return value\n    if isinstance(value, str):\n        text = value.strip()\n        if text.endswith(\"Z\"):\n            text = text[:-1] + \"+00:00\"\n        return datetime.fromisoformat(text)\n    raise TypeError(\"Configuration epoch must be a datetime or ISO 8601 string.\")\n\n\ndef _lvlh_frame(position: Sequence[float], velocity: Sequence[float]) -> np.ndarray:\n    r = np.asarray(position, dtype=float)\n    v = np.asarray(velocity, dtype=float)\n    r_hat = r / np.linalg.norm(r)\n    h = np.cross(r, v)\n    k_hat = h / np.linalg.norm(h)\n    j_hat = np.cross(k_hat, r_hat)\n    j_hat /= np.linalg.norm(j_hat)\n    return np.column_stack((r_hat, j_hat, k_hat))\n\n\ndef _formation_offsets(side_length_m: float) -> Mapping[str, np.ndarray]:\n    \"\"\"Return equilateral offsets constrained to the local horizontal plane.\"\"\"\n\n    sqrt_three = math.sqrt(3.0)\n    return {\n        \"SAT-1\": np.array(\n            [0.0, -0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-2\": np.array(\n            [0.0, 0.5 * side_length_m, -sqrt_three / 6.0 * side_length_m]\n        ),\n        \"SAT-3\": np.array([0.0, 0.0, sqrt_three / 3.0 * side_length_m]),\n    }\n\n\ndef _differentiate(samples: np.ndarray, step: float) -> np.ndarray:\n    derivatives = np.zeros_like(samples)\n    count = samples.shape[0]\n    for index in range(count):\n        if 0 < index < count - 1:\n            derivatives[index] = (samples[index + 1] - samples[index - 1]) / (2.0 * step)\n        elif index == 0:\n            derivatives[index] = (samples[index + 1] - samples[index]) / step\n        else:\n            derivatives[index] = (samples[index] - samples[index - 1]) / step\n    return derivatives\n\n\ndef _compute_classical_elements_series(\n    positions: Mapping[str, np.ndarray],\n    velocities: Mapping[str, np.ndarray],\n) -> Mapping[str, Mapping[str, np.ndarray]]:\n    \"\"\"Evaluate classical orbital elements for each spacecraft over time.\"\"\"\n\n    if not positions:\n        return {}\n\n    sample_count = next(iter(positions.values())).shape[0]\n    series: dict[str, dict[str, np.ndarray]] = {}\n    for sat_id, position_history in positions.items():\n        per_satellite = {\n            name: np.zeros(sample_count, dtype=float) for name in CLASSICAL_ELEMENT_FIELDS\n        }\n        series[sat_id] = per_satellite\n        velocity_history = velocities.get(sat_id)\n        if velocity_history is None:\n            continue\n        for index in range(sample_count):\n            elements = cartesian_to_classical(\n                position_history[index], velocity_history[index]\n            )\n            per_satellite[\"semi_major_axis_km\"][index] = elements.semi_major_axis / 1_000.0\n            per_satellite[\"eccentricity\"][index] = elements.eccentricity\n            per_satellite[\"inclination_deg\"][index] = _normalise_degrees(\n                elements.inclination\n            )\n            per_satellite[\"raan_deg\"][index] = _normalise_degrees(elements.raan)\n            per_satellite[\"argument_of_perigee_deg\"][index] = _normalise_degrees(\n                elements.arg_perigee\n            )\n            per_satellite[\"mean_anomaly_deg\"][index] = _normalise_degrees(\n                elements.mean_anomaly\n            )\n\n    return series\n\n\ndef _normalise_degrees(angle_rad: float) -> float:\n    \"\"\"Convert *angle_rad* to degrees on the [0, 360) interval.\"\"\"\n\n    value = math.degrees(angle_rad)\n    if not math.isfinite(value):\n        return float(\"nan\")\n    wrapped = math.fmod(value, 360.0)\n    if wrapped < 0.0:\n        wrapped += 360.0\n    return wrapped\n\n\ndef _write_orbital_elements_csv(\n    path: Path, times: Sequence[datetime], series: Mapping[str, Mapping[str, np.ndarray]]\n) -> None:\n    \"\"\"Write a consolidated orbital-element history to *path*.\"\"\"\n\n    records: list[dict[str, object]] = []\n    satellite_ids = sorted(series)\n    for index, epoch in enumerate(times):\n        timestamp = epoch.isoformat().replace(\"+00:00\", \"Z\")\n        for sat_id in satellite_ids:\n            elements = series.get(sat_id)\n            if not elements:\n                continue\n            record = {\"time_utc\": timestamp, \"satellite_id\": sat_id}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        columns = [\"time_utc\", \"satellite_id\", *CLASSICAL_ELEMENT_FIELDS]\n        dataframe = pd.DataFrame(columns=columns)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_orbital_elements_per_spacecraft(\n    directory: Path,\n    times: Sequence[datetime],\n    series: Mapping[str, Mapping[str, np.ndarray]],\n) -> Mapping[str, Path]:\n    \"\"\"Persist individual per-spacecraft orbital-element CSVs.\"\"\"\n\n    paths: dict[str, Path] = {}\n    for sat_id, elements in series.items():\n        records: list[dict[str, object]] = []\n        for index, epoch in enumerate(times):\n            record = {\"time_utc\": epoch.isoformat().replace(\"+00:00\", \"Z\")}\n            for field in CLASSICAL_ELEMENT_FIELDS:\n                values = elements.get(field)\n                if values is None or index >= len(values):\n                    record[field] = float(\"nan\")\n                else:\n                    record[field] = float(values[index])\n            records.append(record)\n        dataframe = pd.DataFrame.from_records(records)\n        if dataframe.empty:\n            dataframe = pd.DataFrame(columns=[\"time_utc\", *CLASSICAL_ELEMENT_FIELDS])\n        filename = f\"{sat_id.lower().replace(' ', '_')}_orbital_elements.csv\"\n        path = directory / filename\n        dataframe.to_csv(path, index=False)\n        paths[sat_id] = path\n    return paths\n\n\ndef _summarise_triangle_metrics(areas: np.ndarray, aspects: np.ndarray, sides: np.ndarray) -> Mapping[str, float]:\n    return {\n        \"mean_area_m2\": float(np.mean(areas)),\n        \"min_area_m2\": float(np.min(areas)),\n        \"max_area_m2\": float(np.max(areas)),\n        \"mean_aspect_ratio\": float(np.mean(aspects)),\n        \"aspect_ratio_max\": float(np.max(aspects)),\n        \"mean_side_lengths_m\": [float(val) for val in np.mean(sides, axis=0)],\n    }\n\n\ndef _summarise_ground_metrics(distances_km: np.ndarray, formation: Mapping[str, object]) -> Mapping[str, float]:\n    return {\n        \"max_ground_distance_km\": float(np.max(distances_km)),\n        \"min_ground_distance_km\": float(np.min(distances_km)),\n        \"ground_distance_tolerance_km\": float(formation.get(\"ground_tolerance_km\", 350.0)),\n    }\n\n\ndef _formation_window(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> tuple[Mapping[str, object], Sequence[Mapping[str, object]]]:\n    \"\"\"Derive the principal formation window together with the full schedule.\"\"\"\n\n    windows = list(\n        _enumerate_formation_windows(\n            aspects,\n            distances_km,\n            step,\n            formation,\n            times,\n        )\n    )\n    if not windows:\n        primary: Mapping[str, object] = {\n            \"duration_s\": 0.0,\n            \"start\": None,\n            \"end\": None,\n            \"sample_count\": 0,\n        }\n        return primary, windows\n\n    primary = max(windows, key=lambda item: float(item.get(\"duration_s\", 0.0)))\n    return primary, windows\n\n\ndef _enumerate_formation_windows(\n    aspects: np.ndarray,\n    distances_km: np.ndarray,\n    step: float,\n    formation: Mapping[str, object],\n    times: Sequence[datetime],\n) -> Iterable[Mapping[str, object]]:\n    \"\"\"Yield contiguous access windows that satisfy the ground-track criteria.\"\"\"\n\n    tolerance = float(formation.get(\"ground_tolerance_km\", 350.0))\n    aspect_limit = float(formation.get(\"aspect_ratio_tolerance\", 1.02))\n    mask = (distances_km <= tolerance) & (aspects <= aspect_limit)\n\n    start_index: Optional[int] = None\n    for index, valid in enumerate(mask):\n        if valid and start_index is None:\n            start_index = index\n            continue\n        if not valid and start_index is not None:\n            yield _describe_window(\n                start_index,\n                index - 1,\n                times,\n                distances_km,\n                aspects,\n                step,\n            )\n            start_index = None\n\n    if start_index is not None:\n        yield _describe_window(\n            start_index,\n            len(mask) - 1,\n            times,\n            distances_km,\n            aspects,\n            step,\n        )\n\n\ndef _describe_window(\n    start: int,\n    end: int,\n    times: Sequence[datetime],\n    distances_km: np.ndarray,\n    aspects: np.ndarray,\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Summarise a valid access window spanning indices ``start`` to ``end``.\"\"\"\n\n    start_time = times[start]\n    end_time = times[end]\n    if end > start:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        earlier = start_time - half_step\n        later = end_time + half_step\n        # Preserve the original timezone information when clamping.\n        if earlier < times[0]:\n            earlier = times[0]\n        if later > times[-1]:\n            later = times[-1]\n        duration_seconds = float(step)\n        start_time = earlier\n        end_time = later\n\n    slice_obj = slice(start, end + 1)\n    window_distances = distances_km[slice_obj]\n    window_aspects = aspects[slice_obj]\n    centroid_index = start + (end - start) // 2\n    centroid_time = times[centroid_index]\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"sample_count\": int(end - start + 1),\n        \"max_ground_distance_km\": float(np.max(window_distances)),\n        \"min_ground_distance_km\": float(np.min(window_distances)),\n        \"max_aspect_ratio\": float(np.max(window_aspects)),\n        \"centroid_time\": centroid_time.isoformat().replace(\"+00:00\", \"Z\"),\n    }\n\n\ndef _summarise_window_recurrence(\n    windows: Sequence[Mapping[str, object]],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    \"\"\"Quantify the repeatability of access windows over the simulation horizon.\"\"\"\n\n    if not windows:\n        return {\n            \"window_count\": 0,\n            \"mean_duration_s\": 0.0,\n            \"min_duration_s\": 0.0,\n            \"max_duration_s\": 0.0,\n            \"mean_interval_s\": 0.0,\n            \"min_interval_s\": 0.0,\n            \"max_interval_s\": 0.0,\n            \"std_interval_s\": 0.0,\n            \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n            \"repeatability_score\": 0.0,\n            \"interval_samples\": 0,\n        }\n\n    durations = np.array([float(window.get(\"duration_s\", 0.0)) for window in windows])\n    mean_duration = float(np.mean(durations)) if durations.size else 0.0\n    min_duration = float(np.min(durations)) if durations.size else 0.0\n    max_duration = float(np.max(durations)) if durations.size else 0.0\n\n    midpoints: list[datetime] = []\n    for window in windows:\n        start = window.get(\"start\")\n        end = window.get(\"end\")\n        if not start or not end:\n            continue\n        start_time = _parse_time(start)\n        end_time = _parse_time(end)\n        delta = end_time - start_time\n        midpoint = start_time + delta / 2 if delta.total_seconds() >= 0.0 else start_time\n        midpoints.append(midpoint)\n\n    intervals = []\n    for first, second in zip(midpoints, midpoints[1:]):\n        delta = (second - first).total_seconds()\n        if delta > 0.0:\n            intervals.append(delta)\n\n    if intervals:\n        interval_array = np.asarray(intervals, dtype=float)\n        mean_interval = float(np.mean(interval_array))\n        min_interval = float(np.min(interval_array))\n        max_interval = float(np.max(interval_array))\n        std_interval = float(np.std(interval_array))\n        if mean_interval > 0.0:\n            repeatability = max(0.0, 1.0 - std_interval / mean_interval)\n        else:\n            repeatability = 0.0\n    else:\n        mean_interval = min_interval = max_interval = std_interval = 0.0\n        repeatability = 1.0 if len(windows) == 1 else 0.0\n\n    return {\n        \"window_count\": len(windows),\n        \"mean_duration_s\": mean_duration,\n        \"min_duration_s\": min_duration,\n        \"max_duration_s\": max_duration,\n        \"mean_interval_s\": mean_interval,\n        \"min_interval_s\": min_interval,\n        \"max_interval_s\": max_interval,\n        \"std_interval_s\": std_interval,\n        \"expected_orbit_period_s\": _orbital_period(semi_major_axis_m),\n        \"repeatability_score\": repeatability,\n        \"interval_samples\": len(intervals),\n    }\n\n\ndef _orbital_period(semi_major_axis_m: float) -> float:\n    if semi_major_axis_m <= 0.0:\n        return 0.0\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    return float(2.0 * math.pi / mean_motion)\n\n\ndef _assess_station_keeping(\n    times: Sequence[datetime],\n    triangle_sides: np.ndarray,\n    formation: Mapping[str, object],\n    maintenance: Mapping[str, object],\n    step: float,\n) -> Mapping[str, object]:\n    \"\"\"Detect when the triangular geometry exceeds the maintenance tolerance.\"\"\"\n\n    if not len(times):\n        return {\n            \"status\": \"insufficient_data\",\n            \"events\": [],\n            \"tolerance_m\": 0.0,\n            \"violation_fraction\": 0.0,\n            \"recommended_delta_v_mps\": 0.0,\n            \"max_deviation_m\": 0.0,\n        }\n\n    base_length = float(formation.get(\"side_length_m\", 0.0))\n    default_tolerance = base_length * 0.01 if base_length > 0.0 else 10.0\n    tolerance = float(\n        formation.get(\n            \"station_keeping_tolerance_m\",\n            formation.get(\"repeatability_tolerance_m\", default_tolerance),\n        )\n    )\n    if tolerance <= 0.0:\n        tolerance = default_tolerance\n\n    deviations = np.abs(triangle_sides - base_length)\n    max_deviation_series = np.max(deviations, axis=1)\n    violation_mask = max_deviation_series > tolerance\n\n    events: list[Mapping[str, object]] = []\n    start_index: Optional[int] = None\n    peak_violation = 0.0\n    for index, violated in enumerate(violation_mask):\n        if violated:\n            if start_index is None:\n                start_index = index\n                peak_violation = float(max_deviation_series[index])\n            else:\n                peak_violation = max(peak_violation, float(max_deviation_series[index]))\n            continue\n        if start_index is not None:\n            events.append(\n                _build_station_event(\n                    start_index,\n                    index - 1,\n                    times,\n                    max_deviation_series,\n                    peak_violation,\n                    step,\n                )\n            )\n            start_index = None\n            peak_violation = 0.0\n\n    if start_index is not None:\n        events.append(\n            _build_station_event(\n                start_index,\n                len(times) - 1,\n                times,\n                max_deviation_series,\n                peak_violation,\n                step,\n            )\n        )\n\n    recommended_delta_v = _recommended_delta_v(maintenance)\n    for event in events:\n        event.setdefault(\"recommended_delta_v_mps\", recommended_delta_v)\n\n    violation_fraction = (\n        float(np.count_nonzero(violation_mask)) / float(len(max_deviation_series))\n        if len(max_deviation_series)\n        else 0.0\n    )\n\n    max_deviation = float(np.max(max_deviation_series)) if len(max_deviation_series) else 0.0\n\n    status = \"nominal\" if not events else \"correction_required\"\n    return {\n        \"status\": status,\n        \"events\": events,\n        \"tolerance_m\": tolerance,\n        \"violation_fraction\": violation_fraction,\n        \"recommended_delta_v_mps\": recommended_delta_v,\n        \"max_deviation_m\": max_deviation,\n    }\n\n\ndef _build_station_event(\n    start_index: int,\n    end_index: int,\n    times: Sequence[datetime],\n    deviations: np.ndarray,\n    peak_violation: float,\n    step: float,\n) -> Mapping[str, object]:\n    start_time = times[start_index]\n    end_time = times[end_index]\n    if end_index > start_index:\n        duration_seconds = max((end_time - start_time).total_seconds(), 0.0)\n    else:\n        half_step = timedelta(seconds=0.5 * step)\n        start_time = max(start_time - half_step, times[0])\n        end_time = min(end_time + half_step, times[-1])\n        duration_seconds = float(step)\n\n    slice_obj = slice(start_index, end_index + 1)\n    peak = max(peak_violation, float(np.max(deviations[slice_obj])))\n\n    return {\n        \"start\": start_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": end_time.isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration_seconds),\n        \"violation_samples\": int(end_index - start_index + 1),\n        \"peak_deviation_m\": float(peak),\n    }\n\n\ndef _recommended_delta_v(maintenance: Mapping[str, object]) -> float:\n    per_spacecraft = maintenance.get(\"per_spacecraft\") if isinstance(maintenance, Mapping) else None\n    if isinstance(per_spacecraft, Mapping) and per_spacecraft:\n        deltas = [\n            float(entry.get(\"delta_v_per_burn_mps\", 0.0))\n            for entry in per_spacecraft.values()\n        ]\n        if deltas:\n            return float(np.mean(deltas))\n\n    assumptions = maintenance.get(\"assumptions\") if isinstance(maintenance, Mapping) else None\n    if isinstance(assumptions, Mapping):\n        return float(assumptions.get(\"delta_v_budget_mps\", 0.0))\n    return 0.0\n\n\ndef _write_formation_windows_csv(\n    path: Path,\n    windows: Sequence[Mapping[str, object]],\n) -> None:\n    \"\"\"Serialise the detected formation windows to ``path``.\"\"\"\n\n    records: list[MutableMapping[str, object]] = []\n    for index, window in enumerate(windows):\n        record: MutableMapping[str, object] = {\"window_index\": index}\n        record[\"start\"] = window.get(\"start\")\n        record[\"end\"] = window.get(\"end\")\n        record[\"duration_s\"] = float(window.get(\"duration_s\", 0.0))\n        record[\"sample_count\"] = int(window.get(\"sample_count\", 0))\n        record[\"max_ground_distance_km\"] = float(\n            window.get(\"max_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"min_ground_distance_km\"] = float(\n            window.get(\"min_ground_distance_km\", float(\"nan\"))\n        )\n        record[\"max_aspect_ratio\"] = float(window.get(\"max_aspect_ratio\", float(\"nan\")))\n        record[\"centroid_time\"] = window.get(\"centroid_time\")\n        records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"window_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"sample_count\",\n                \"max_ground_distance_km\",\n                \"min_ground_distance_km\",\n                \"max_aspect_ratio\",\n                \"centroid_time\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _write_station_keeping_csv(\n    path: Path,\n    station_keeping: Mapping[str, object],\n) -> None:\n    \"\"\"Write the station-keeping assessment events to ``path``.\"\"\"\n\n    events = station_keeping.get(\"events\") if isinstance(station_keeping, Mapping) else []\n    records: list[MutableMapping[str, object]] = []\n    if isinstance(events, Sequence):\n        for index, event in enumerate(events):\n            record: MutableMapping[str, object] = {\"event_index\": index}\n            record[\"start\"] = event.get(\"start\")\n            record[\"end\"] = event.get(\"end\")\n            record[\"duration_s\"] = float(event.get(\"duration_s\", 0.0))\n            record[\"violation_samples\"] = int(event.get(\"violation_samples\", 0))\n            record[\"peak_deviation_m\"] = float(event.get(\"peak_deviation_m\", 0.0))\n            record[\"recommended_delta_v_mps\"] = float(\n                event.get(\"recommended_delta_v_mps\", station_keeping.get(\"recommended_delta_v_mps\", 0.0))\n            )\n            records.append(record)\n\n    dataframe = pd.DataFrame.from_records(records)\n    if dataframe.empty:\n        dataframe = pd.DataFrame(\n            columns=[\n                \"event_index\",\n                \"start\",\n                \"end\",\n                \"duration_s\",\n                \"violation_samples\",\n                \"peak_deviation_m\",\n                \"recommended_delta_v_mps\",\n            ]\n        )\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    dataframe.to_csv(path, index=False)\n\n\ndef _estimate_maintenance_delta_v(\n    positions: Mapping[str, np.ndarray],\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n) -> Mapping[str, object]:\n    maintenance = formation.get(\"maintenance\", {})\n    burn_duration_s = float(maintenance.get(\"burn_duration_s\", 45.0))\n    interval_days = float(maintenance.get(\"interval_days\", 7.0))\n    delta_v_budget_mps = float(maintenance.get(\"delta_v_budget_mps\", 15.0))\n    interval_seconds = max(interval_days * SECONDS_PER_DAY, 1.0)\n    burns_per_year = SECONDS_PER_YEAR / interval_seconds\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    centroid = sum(positions[sat_id] for sat_id in positions) / float(len(positions))\n    centroid_norm = np.linalg.norm(centroid, axis=1)\n    centroid_accel = -MU_EARTH * centroid / centroid_norm[:, None] ** 3\n\n    per_spacecraft: MutableMapping[str, Mapping[str, float]] = {}\n    mean_accels = []\n    peak_accels = []\n    annual_totals = []\n\n    for sat_id, states in positions.items():\n        radii = np.linalg.norm(states, axis=1)\n        accelerations = -MU_EARTH * states / radii[:, None] ** 3\n        differential = np.linalg.norm(accelerations - centroid_accel, axis=1)\n        mean_diff = float(np.mean(differential))\n        peak_diff = float(np.max(differential))\n        dv_per_burn = mean_diff * burn_duration_s\n        annual_delta_v = dv_per_burn * burns_per_year\n\n        mean_accels.append(mean_diff)\n        peak_accels.append(peak_diff)\n        annual_totals.append(annual_delta_v)\n\n        per_spacecraft[sat_id] = {\n            \"mean_diff_accel_mps2\": mean_diff,\n            \"peak_diff_accel_mps2\": peak_diff,\n            \"delta_v_per_burn_mps\": dv_per_burn,\n            \"annual_delta_v_mps\": annual_delta_v,\n        }\n\n    annual_array = np.asarray(annual_totals, dtype=float)\n    mean_accel = float(np.mean(mean_accels)) if mean_accels else 0.0\n    peak_accel = float(np.max(peak_accels)) if peak_accels else 0.0\n\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n\n    return {\n        \"assumptions\": {\n            \"burn_duration_s\": burn_duration_s,\n            \"maintenance_interval_days\": interval_days,\n            \"burns_per_year\": float(burns_per_year),\n            \"analysis_window_s\": float(max((count - 1), 0) * step),\n            \"orbit_period_s\": float(orbit_period),\n            \"delta_v_budget_mps\": delta_v_budget_mps,\n        },\n        \"mean_differential_acceleration_mps2\": mean_accel,\n        \"peak_differential_acceleration_mps2\": peak_accel,\n        \"per_spacecraft\": per_spacecraft,\n        \"annual_delta_v_mps\": {\n            \"mean\": float(np.mean(annual_array)) if annual_totals else 0.0,\n            \"max\": float(np.max(annual_array)) if annual_totals else 0.0,\n            \"min\": float(np.min(annual_array)) if annual_totals else 0.0,\n        },\n    }\n\n\ndef _analyse_command_latency(\n    min_distances_km: np.ndarray,\n    times: Sequence[datetime],\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    command_lat: float,\n    command_lon: float,\n) -> Mapping[str, object]:\n    command_cfg = formation.get(\"command\", {})\n    range_km = float(\n        command_cfg.get(\"contact_range_km\", formation.get(\"ground_tolerance_km\", 350.0))\n    )\n\n    count = len(times)\n    if count > 1:\n        step = (times[1] - times[0]).total_seconds()\n    else:\n        step = 0.0\n\n    mask = np.asarray(min_distances_km) <= range_km\n\n    windows: list[Mapping[str, object]] = []\n    current_start: Optional[int] = None\n    for index, in_contact in enumerate(mask):\n        if in_contact and current_start is None:\n            current_start = index\n        elif not in_contact and current_start is not None:\n            end_index = index - 1\n            windows.append(\n                _format_contact_window(times, current_start, end_index, step)\n            )\n            current_start = None\n    if current_start is not None:\n        windows.append(\n            _format_contact_window(times, current_start, len(mask) - 1, step)\n        )\n\n    contact_duration_s = float(sum(window[\"duration_s\"] for window in windows))\n    orbit_period = 2.0 * math.pi * math.sqrt(semi_major_axis_m**3 / MU_EARTH)\n    contact_probability = contact_duration_s / orbit_period if orbit_period else 0.0\n    gap_s = max(orbit_period - contact_duration_s, 0.0)\n    max_latency_hours = gap_s / 3600.0\n    mean_latency_hours = max_latency_hours / 2.0\n\n    return {\n        \"station\": {\n            \"latitude_deg\": math.degrees(command_lat),\n            \"longitude_deg\": math.degrees(command_lon),\n            \"contact_range_km\": range_km,\n        },\n        \"contact_probability\": contact_probability,\n        \"contact_duration_s\": contact_duration_s,\n        \"contact_windows\": windows,\n        \"passes_per_day\": SECONDS_PER_DAY / orbit_period if orbit_period else 0.0,\n        \"max_latency_hours\": max_latency_hours,\n        \"mean_latency_hours\": mean_latency_hours,\n        \"latency_margin_hours\": 12.0 - max_latency_hours,\n        \"assumptions\": {\n            \"uniform_request_distribution\": True,\n            \"single_station_operations\": True,\n        },\n    }\n\n\ndef _format_contact_window(\n    times: Sequence[datetime], start_index: int, end_index: int, step: float\n) -> Mapping[str, object]:\n    count = end_index - start_index + 1\n    duration = max(count - 1, 0) * step\n    return {\n        \"start\": times[start_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"end\": times[end_index].isoformat().replace(\"+00:00\", \"Z\"),\n        \"duration_s\": float(duration),\n    }\n\n\ndef _run_injection_recovery_monte_carlo(\n    satellite_ids: Sequence[str], formation: Mapping[str, object]\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    config = formation.get(\"monte_carlo\", {})\n    sample_count = int(config.get(\"samples\", 200))\n    position_sigma_m = float(config.get(\"position_sigma_m\", 250.0))\n    velocity_sigma_mmps = float(config.get(\"velocity_sigma_mmps\", 5.0))\n    velocity_sigma_mps = velocity_sigma_mmps / 1_000.0\n    recovery_time_s = float(config.get(\"recovery_time_s\", 12.0 * 3600.0))\n    delta_v_budget = float(config.get(\"delta_v_budget_mps\", 15.0))\n    seed = config.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    records: list[Mapping[str, object]] = []\n    for sample_index in range(sample_count):\n        for sat_id in satellite_ids:\n            position_error = rng.normal(0.0, position_sigma_m, size=3)\n            velocity_error = rng.normal(0.0, velocity_sigma_mps, size=3)\n            position_mag = float(np.linalg.norm(position_error))\n            velocity_mag = float(np.linalg.norm(velocity_error))\n            delta_v_position = 2.0 * position_mag / recovery_time_s\n            delta_v_total = delta_v_position + velocity_mag\n            success = delta_v_total <= delta_v_budget\n            records.append(\n                {\n                    \"sample_id\": sample_index,\n                    \"satellite_id\": sat_id,\n                    \"position_error_m\": position_mag,\n                    \"velocity_error_mps\": velocity_mag,\n                    \"delta_v_mps\": delta_v_total,\n                    \"success\": bool(success),\n                }\n            )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"success_rate\": 1.0,\n            \"mean_delta_v_mps\": 0.0,\n            \"p95_delta_v_mps\": 0.0,\n            \"max_delta_v_mps\": 0.0,\n        }\n        per_spacecraft = {sat_id: aggregate for sat_id in satellite_ids}\n        success_rate = 1.0\n    else:\n        success_rate = float(dataframe[\"success\"].mean())\n        per_spacecraft = {}\n        for sat_id, group in dataframe.groupby(\"satellite_id\"):\n            per_spacecraft[str(sat_id)] = {\n                \"mean_delta_v_mps\": float(group[\"delta_v_mps\"].mean()),\n                \"p95_delta_v_mps\": float(group[\"delta_v_mps\"].quantile(0.95)),\n                \"max_delta_v_mps\": float(group[\"delta_v_mps\"].max()),\n                \"success_rate\": float(group[\"success\"].mean()),\n            }\n        aggregate = {\n            \"mean_delta_v_mps\": float(dataframe[\"delta_v_mps\"].mean()),\n            \"p95_delta_v_mps\": float(dataframe[\"delta_v_mps\"].quantile(0.95)),\n            \"max_delta_v_mps\": float(dataframe[\"delta_v_mps\"].max()),\n        }\n\n    metrics = {\n        \"success_rate\": success_rate,\n        \"sample_count\": sample_count,\n        \"per_spacecraft\": per_spacecraft,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"position_sigma_m\": position_sigma_m,\n            \"velocity_sigma_mmps\": velocity_sigma_mmps,\n            \"recovery_time_s\": recovery_time_s,\n            \"delta_v_budget_mps\": delta_v_budget,\n        },\n    }\n\n    return metrics, dataframe\n\n\ndef _run_atmospheric_drag_dispersion_monte_carlo(\n    formation: Mapping[str, object],\n    semi_major_axis_m: float,\n    inclination_rad: float,\n) -> tuple[Mapping[str, object], pd.DataFrame]:\n    settings = formation.get(\"drag_dispersion\", {})\n    sample_count = int(settings.get(\"samples\", 200))\n    density_sigma = float(settings.get(\"density_sigma\", 0.2))\n    drag_coefficient_sigma = float(settings.get(\"drag_coefficient_sigma\", 0.05))\n    reference_density = float(settings.get(\"reference_density_kg_m3\", 3.5e-12))\n    drag_coefficient = float(settings.get(\"drag_coefficient\", 2.2))\n    ballistic_area = float(settings.get(\"reference_area_m2\", 1.0))\n    spacecraft_mass = float(settings.get(\"spacecraft_mass_kg\", 150.0))\n    time_horizon_orbits = float(settings.get(\"time_horizon_orbits\", 15.0))\n    integration_step = float(settings.get(\"integration_step_s\", 60.0))\n    seed = settings.get(\"seed\")\n\n    if spacecraft_mass <= 0.0:\n        raise ValueError(\"Spacecraft mass must be positive for drag dispersions.\")\n\n    rng = np.random.default_rng(seed)\n    ballistic_coeff_base = ballistic_area / spacecraft_mass\n\n    mean_motion = math.sqrt(MU_EARTH / semi_major_axis_m**3)\n    orbital_period = 2.0 * math.pi / mean_motion\n    horizon = max(time_horizon_orbits, 0.0) * orbital_period\n    horizon = max(horizon, integration_step)\n\n    records: list[Mapping[str, object]] = []\n    tolerance_km = float(formation.get(\"ground_tolerance_km\", 350.0))\n    orbital_radius = semi_major_axis_m\n    ground_projection_scale = math.cos(inclination_rad)\n\n    time_samples = np.arange(0.0, horizon + 0.5 * integration_step, integration_step)\n\n    for sample_id in range(sample_count):\n        density_scale = max(0.0, 1.0 + rng.normal(0.0, density_sigma))\n        cd_scale = max(0.0, 1.0 + rng.normal(0.0, drag_coefficient_sigma))\n\n        rho = reference_density * density_scale\n        cd = drag_coefficient * cd_scale\n        ballistic_coefficient = ballistic_coeff_base\n\n        orbital_velocity = math.sqrt(MU_EARTH / semi_major_axis_m)\n        drag_acceleration = 0.5 * rho * cd * ballistic_coefficient * orbital_velocity**2\n        da_dt = -2.0 * semi_major_axis_m**2 * drag_acceleration / MU_EARTH\n\n        semi_major_axis_series = semi_major_axis_m + da_dt * time_samples\n        semi_major_axis_series = np.maximum(semi_major_axis_series, EARTH_EQUATORIAL_RADIUS_M + 150_000.0)\n\n        altitude_delta = semi_major_axis_series[-1] - semi_major_axis_m\n        new_mean_motion = math.sqrt(MU_EARTH / semi_major_axis_series[-1] ** 3)\n        delta_theta = (new_mean_motion - mean_motion) * horizon\n        along_track_shift = abs(delta_theta) * orbital_radius / 1_000.0\n\n        ground_distance_delta = along_track_shift * abs(ground_projection_scale)\n        command_distance_delta = along_track_shift\n\n        within_tolerance = ground_distance_delta <= tolerance_km\n\n        records.append(\n            {\n                \"sample_id\": sample_id,\n                \"density_scale\": float(density_scale),\n                \"drag_coefficient\": float(cd),\n                \"ballistic_coefficient_m2_per_kg\": float(ballistic_coefficient),\n                \"semi_major_axis_delta_m\": float(altitude_delta),\n                \"altitude_delta_m\": float(altitude_delta),\n                \"along_track_shift_km\": float(along_track_shift),\n                \"ground_distance_delta_km\": float(ground_distance_delta),\n                \"command_distance_delta_km\": float(command_distance_delta),\n                \"within_tolerance\": bool(within_tolerance),\n            }\n        )\n\n    dataframe = pd.DataFrame.from_records(records)\n\n    if dataframe.empty:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": 0.0,\n            \"max_ground_distance_delta_km\": 0.0,\n            \"p95_along_track_shift_km\": 0.0,\n            \"max_along_track_shift_km\": 0.0,\n            \"p95_altitude_delta_m\": 0.0,\n        }\n        success_rate = 1.0\n    else:\n        aggregate = {\n            \"p95_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].quantile(0.95)\n            ),\n            \"max_ground_distance_delta_km\": float(\n                dataframe[\"ground_distance_delta_km\"].max()\n            ),\n            \"p95_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].quantile(0.95)\n            ),\n            \"max_along_track_shift_km\": float(\n                dataframe[\"along_track_shift_km\"].max()\n            ),\n            \"p95_altitude_delta_m\": float(\n                dataframe[\"altitude_delta_m\"].quantile(0.95)\n            ),\n        }\n        success_rate = float(dataframe[\"within_tolerance\"].mean())\n\n    metrics = {\n        \"sample_count\": sample_count,\n        \"success_rate\": success_rate,\n        \"aggregate\": aggregate,\n        \"assumptions\": {\n            \"density_sigma\": density_sigma,\n            \"drag_coefficient_sigma\": drag_coefficient_sigma,\n            \"reference_density_kg_m3\": reference_density,\n            \"drag_coefficient\": drag_coefficient,\n            \"reference_area_m2\": ballistic_area,\n            \"spacecraft_mass_kg\": spacecraft_mass,\n            \"time_horizon_orbits\": time_horizon_orbits,\n            \"integration_step_s\": integration_step,\n        },\n        \"tolerance_km\": tolerance_km,\n    }\n\n    return metrics, dataframe\n\n\ndef _write_injection_recovery_plot(samples: pd.DataFrame, output_path: Path) -> None:\n    if samples.empty:\n        return\n\n    import matplotlib\n\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt  # noqa: WPS433 - imported for plotting\n\n    values = np.sort(samples[\"delta_v_mps\"].to_numpy(dtype=float))\n    cumulative = (np.arange(len(values), dtype=float) + 1.0) / float(len(values))\n\n    figure, axis = plt.subplots(figsize=(6.0, 4.0))\n    axis.plot(values, cumulative, color=\"#1f77b4\", linewidth=2.0)\n    axis.set_xlabel(\"Δv per spacecraft (m/s)\")\n    axis.set_ylabel(\"Cumulative probability\")\n    axis.set_title(\"Injection recovery Δv distribution\")\n    axis.grid(True, alpha=0.3)\n    figure.tight_layout()\n    figure.savefig(output_path, format=\"svg\")\n    plt.close(figure)\n\ndef _export_to_stk(result: TriangleFormationResult, output_dir: Path, scenario_name: str) -> None:\n    output_dir.mkdir(parents=True, exist_ok=True)\n    state_histories = []\n    ground_tracks = []\n    contacts = []\n\n    satellite_ids = sorted(result.positions_m)\n    safe_satellite_names = unique_stk_names(satellite_ids)\n    safe_facility_name = sanitize_stk_identifier(\"Tehran\", default=\"Facility\")\n    safe_scenario_name = sanitize_stk_identifier(scenario_name, default=\"Scenario\")\n\n    for sat_id in satellite_ids:\n        safe_id = safe_satellite_names[sat_id]\n        samples = [\n            StateSample(\n                epoch=epoch,\n                position_eci_km=result.positions_m[sat_id][index] / 1_000.0,\n                velocity_eci_kms=result.velocities_mps[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        state_histories.append(PropagatedStateHistory(satellite_id=safe_id, samples=samples))\n\n        track_points = [\n            GroundTrackPoint(\n                epoch=epoch,\n                latitude_deg=math.degrees(result.latitudes_rad[sat_id][index]),\n                longitude_deg=math.degrees(result.longitudes_rad[sat_id][index]),\n                altitude_km=result.altitudes_m[sat_id][index] / 1_000.0,\n            )\n            for index, epoch in enumerate(result.times)\n        ]\n        ground_tracks.append(GroundTrack(satellite_id=safe_id, points=track_points))\n\n    window = result.metrics[\"formation_window\"]\n    if window.get(\"start\") and window.get(\"end\"):\n        start = datetime.fromisoformat(window[\"start\"].replace(\"Z\", \"+00:00\"))\n        end = datetime.fromisoformat(window[\"end\"].replace(\"Z\", \"+00:00\"))\n        for sat_id in satellite_ids:\n            safe_id = safe_satellite_names[sat_id]\n            contacts.append(\n                GroundContactInterval(\n                    satellite_id=safe_id,\n                    facility_name=safe_facility_name,\n                    start=start,\n                    end=end,\n                )\n            )\n\n    facilities = [\n        FacilityDefinition(\n            name=safe_facility_name,\n            latitude_deg=35.6892,\n            longitude_deg=51.3890,\n            altitude_km=1.5,\n        )\n    ]\n\n    scenario_metadata = ScenarioMetadata(\n        scenario_name=safe_scenario_name,\n        start_epoch=result.times[0],\n        stop_epoch=result.times[-1],\n        central_body=\"Earth\",\n        coordinate_frame=\"TEME\",\n        ephemeris_step_seconds=(result.times[1] - result.times[0]).total_seconds()\n        if len(result.times) > 1\n        else None,\n    )\n\n    sim_results = SimulationResults(\n        state_histories=state_histories,\n        ground_tracks=ground_tracks,\n        ground_contacts=contacts,\n        facilities=facilities,\n    )\n\n    export_simulation_to_stk(sim_results, output_dir, scenario_metadata)\n\n\n__all__ = [\"simulate_triangle_formation\", \"TriangleFormationResult\"]\n\n"
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CjUB4/H/XnlzSOMyaNLpQQmPZHWJwZZ06XhWP0/zwpZL/buiHPy8au4cOChBSKt5RuqcQndb9gpmAePx/17xQWuW9hC1ZkB11H0jLCC0brlan+D48jpbFQdR5dzRnkf4shtGA/kNtSzNKMFDjkpe/G0m2u383Cn/9gFfzBbgJ7Ioku8ZvD6d+sxxuyrVmtvzbv7g6BbvoDJP1W/aee5LClkB4/H/XgkpGpLcXKgaWGsTIgKdlNfL7KEe+k9MVBSzn3AXlMTd7OJ/+Hg4qzeCUGSThLYLrOKMkzpGeEQCHc0bbPzd/9FM/A0yfXQDGLtj8GUiI+sza6p0twqpAQHj8f9epgrNj6ZE2GIFs7KPW0KFDaYQV2/SKUa1goEsf+gM3CjK6GpmnIivCrNJFsck3Po3Ev2xnUpIDAlEKlewX4b+EmSW/vDyuaCMATuWhs0BIZDQS1qNc4OKsN+3579pCTwOzd7NzTG64eKY26IrufnxmYX6/XqX/aHGkU9T6ya5t9+RALdL6k/XO0j7wQym/+ZEYtYyoU6Wvl7uBhZg5CUugAJXeOUKnwEB4/H/XloY5BRZ8IrbLd1NFSSeXg/I0Z4touR9iQYLiS9QtuXtLe4yl+heiGDq+ydU5+poI7Q8HSnXWV341TULu3MgmRErfBX4aPi7DAH8I26TgUrGQGwdPYH2L7T0AYbhstbdtL6VlXguDhtAq/7ggo8iDbwk2U1kU7oUdCGffwlpCPozjSHSVW9kexoDwroYt1rNJhunTJoxO2WDfzcKgQIB4/H/XuifDk/e8gEoL/u63OmLJUysbzlD+CHqsLw47G69Rujinpkya8N+jRFkK8YX1uO0st0EKoQb2DJXumlYPBi4WN1MQglIIdtc5VoCKsV3t9lv678FHXNRC3EXza20ZunCcIPK7d6k0suppivXolMWSjqnCcOilkHoVNTxnx81RZR+fn3I47BzBdfgtIYh8dADWB7JQfk1KQXaES3z2NnSm4LPDtVplTEqVDewSf/DWoPHMCm4wKBbXLu2qwJB5BvKJCjxbXe4fXz/OpUqyVJRGCOUOG1FcL0ARns1Phmz7SWB0+KWMwJ+jqVcUxZgBNlECEBQK5NfViarTKqD9QpRAePx/16cJGD8kDbSnvceS4Ev+sQHFJiUL67Yv+/3sm8BO8d6zGtxDx4n7neVnNuyPAkaFqfWWTuJLg820eq9ckehSxi8QRlWMs9fN2U2Vt2v",
        "text": "I see now, I was editing the wrong file. The user is running `run_triangle`, so I need to optimize `simulate_triangle_formation` in `sim/formation/triangle.py`. I'll replace the numpy array pre-allocation with list appends inside the loop, converting to arrays only at the end. I'm replacing the entire function with this optimized version."
      },
      {
        "functionCall": {
          "name": "replace",
          "args": {
            "new_string": "def simulate_triangle_formation(\n    config_source: Mapping[str, object] | Path | str,\n    output_directory: Optional[Path | str] = None,\n) -> TriangleFormationResult:\n    \"\"\"Simulate the triangular formation described by *config_source*.\"\"\"\n\n    configuration = _load_configuration(config_source)\n    formation = configuration[\"formation\"]\n    metadata = configuration.get(\"metadata\", {})\n\n    satellite_elements: dict[str, OrbitalElements] = {}\n    if \"satellites\" in configuration:\n        satellite_ids = [sat[\"id\"] for sat in configuration[\"satellites\"]]\n        plane_allocations = {\n            sat[\"id\"]: sat[\"plane\"] for sat in configuration[\"satellites\"]\n        }\n        epoch = _parse_time(\n            configuration[\"satellites\"][0][\"orbital_elements\"][\"epoch_utc\"]\n        )\n        semi_major_axis_m = (\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"][\"semi_major_axis_km\"]\n            )\n            * 1_000.0\n        )\n        inclination = math.radians(\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"].get(\n                    \"inclination_deg\", 0.0\n                )\n            )\n        )\n\n        for sat_config in configuration[\"satellites\"]:\n            elements = sat_config[\"orbital_elements\"]\n            satellite_elements[sat_config[\"id\"]] = OrbitalElements(\n                semi_major_axis=float(elements[\"semi_major_axis_km\"]) * 1_000.0,\n                eccentricity=float(elements.get(\"eccentricity\", 0.0)),\n                inclination=math.radians(float(elements.get(\"inclination_deg\", 0.0))),\n                raan=math.radians(float(elements.get(\"raan_deg\", 0.0))),\n                arg_perigee=math.radians(\n                    float(elements.get(\"argument_of_perigee_deg\", 0.0))\n                ),\n                mean_anomaly=math.radians(float(elements.get(\"mean_anomaly_deg\", 0.0))),\n            )\n    else:\n        reference = configuration[\"reference_orbit\"]\n        epoch = _parse_time(reference[\"epoch_utc\"])\n        semi_major_axis_m = float(reference[\"semi_major_axis_km\"]) * 1_000.0\n        eccentricity = float(reference.get(\"eccentricity\", 0.0))\n        inclination = math.radians(float(reference.get(\"inclination_deg\", 0.0)))\n        raan = math.radians(float(reference.get(\"raan_deg\", 0.0)))\n        arg_perigee = math.radians(\n            float(reference.get(\"argument_of_perigee_deg\", 0.0))\n        )\n        mean_anomaly = math.radians(float(reference.get(\"mean_anomaly_deg\", 0.0)))\n\n        reference_elements = OrbitalElements(\n            semi_major_axis=semi_major_axis_m,\n            eccentricity=eccentricity,\n            inclination=inclination,\n            raan=raan,\n            arg_perigee=arg_perigee,\n            mean_anomaly=mean_anomaly,\n        )\n        offsets_m = _formation_offsets(float(formation[\"side_length_m\"]))\n        satellite_ids = tuple(sorted(offsets_m))\n\n        plane_allocations = formation.get(\"plane_allocations\", {})\n        if not plane_allocations:\n            plane_allocations = {\n                satellite_ids[0]: \"Plane A\",\n                satellite_ids[1]: \"Plane A\",\n                satellite_ids[2]: \"Plane B\",\n            }\n\n        # Get initial state of the reference orbit\n        ref_pos_at_epoch, ref_vel_at_epoch = propagate_kepler(reference_elements, 0)\n\n        # Get initial LVLH frame\n        frame_at_epoch = _lvlh_frame(ref_pos_at_epoch, ref_vel_at_epoch)\n\n        # Calculate initial orbital elements for each satellite\n        for sat_id in satellite_ids:\n            offset_vec = frame_at_epoch @ offsets_m[sat_id]\n            # For a stable formation with zero radial offset, the initial relative velocity in the LVLH frame should be zero.\n            offset_vel = np.array([0.0, 0.0, 0.0])\n\n            sat_pos = ref_pos_at_epoch + offset_vec\n            sat_vel = ref_vel_at_epoch + offset_vel\n            satellite_elements[sat_id] = cartesian_to_classical(sat_pos, sat_vel)\n\n    duration_s = float(formation.get(\"duration_s\", 180.0))\n    time_step_s = float(formation.get(\"time_step_s\", 1.0))\n    half_duration = 0.5 * duration_s\n    sample_count = int(round(duration_s / time_step_s)) + 1\n\n    offsets = np.arange(sample_count, dtype=float) * time_step_s - half_duration\n    \n    positions_list: dict[str, list] = {sat_id: [] for sat_id in satellite_ids}\n    latitudes_list: dict[str, list] = {sat_id: [] for sat_id in satellite_ids}\n    longitudes_list: dict[str, list] = {sat_id: [] for sat_id in satellite_ids}\n    altitudes_list: dict[str, list] = {sat_id: [] for sat_id in satellite_ids}\n    velocities_temp: dict[str, list[np.ndarray]] = {sat_id: [] for sat_id in satellite_ids}\n\n    triangle_area_series_list = []\n    triangle_aspect_series_list = []\n    triangle_sides_series_list = []\n    centroid_positions_list = []\n    centroid_latitudes_list = []\n    centroid_longitudes_list = []\n    centroid_altitudes_list = []\n    max_ground_distance_list = []\n    min_command_distance_list = []\n    times = []\n\n    target = formation.get(\"target\", {})\n    target_lat = math.radians(float(target.get(\"latitude_deg\", 0.0)))\n    target_lon = math.radians(float(target.get(\"longitude_deg\", 0.0)))\n\n    command = formation.get(\"command\", {})\n    command_station = command.get(\"station\", {})\n    default_lat_deg = math.degrees(target_lat)\n    default_lon_deg = math.degrees(target_lon)\n    command_lat = math.radians(float(command_station.get(\"latitude_deg\", default_lat_deg)))\n    command_lon = math.radians(float(command_station.get(\"longitude_deg\", default_lon_deg)))\n\n    # Propagate each satellite independently\n    perturbations = formation.get(\"perturbations\", {})\n    position_noise_sigma_m = float(perturbations.get(\"position_noise_sigma_m\", 0.0))\n\n    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    from tqdm import tqdm\n    for delta_t in tqdm(offsets, desc=\"Propagating orbits\"):\n        current_time = epoch + timedelta(seconds=float(delta_t))\n        times.append(current_time)\n        inertial_positions = {}\n        for sat_id in satellite_ids:\n            pos, vel = propagate_kepler(satellite_elements[sat_id], delta_t)\n\n            # if position_noise_sigma_m > 0.0:\n            #     noise = rng.normal(0.0, position_noise_sigma_m, size=3)\n            #     pos += noise\n\n            positions_list[sat_id].append(pos)\n            velocities_temp[sat_id].append(vel)\n            inertial_positions[sat_id] = pos\n\n            ecef = inertial_to_ecef(pos, current_time)\n            lat, lon, alt = geodetic_coordinates(ecef)\n            latitudes_list[sat_id].append(lat)\n            longitudes_list[sat_id].append(lon)\n            altitudes_list[sat_id].append(alt)\n\n        # Now, compute triangle geometry based on the actual positions\n        # The order of vertices matters for side length calculations, so sort by sat_id\n        vertices = [inertial_positions[sat_id] for sat_id in satellite_ids]\n\n        triangle_area_series_list.append(triangle_area(vertices))\n        triangle_aspect_series_list.append(triangle_aspect_ratio(vertices))\n        triangle_sides_series_list.append(triangle_side_lengths(vertices))\n\n        centroid = sum(vertices) / len(vertices)\n        centroid_positions_list.append(centroid)\n        centroid_ecef = inertial_to_ecef(centroid, current_time)\n        centroid_lat, centroid_lon, centroid_alt = geodetic_coordinates(centroid_ecef)\n        centroid_latitudes_list.append(centroid_lat)\n        centroid_longitudes_list.append(centroid_lon)\n        centroid_altitudes_list.append(centroid_alt)\n\n        max_distance = 0.0\n        min_command = np.inf\n        for sat_id in satellite_ids:\n            distance = haversine_distance(\n                latitudes_list[sat_id][-1],\n                longitudes_list[sat_id][-1],\n                target_lat,\n                target_lon,\n            )\n            max_distance = max(max_distance, distance)\n            command_distance = haversine_distance(\n                latitudes_list[sat_id][-1],\n                longitudes_list[sat_id][-1],\n                command_lat,\n                command_lon,\n            )\n            min_command = min(min_command, command_distance)\n        max_ground_distance_list.append(max_distance / 1_000.0)\n        min_command_distance_list.append(min_command)\n\n    positions: dict[str, np.ndarray] = {\n        sat_id: np.array(positions_list[sat_id]) for sat_id in satellite_ids\n    }\n    velocities: dict[str, np.ndarray] = {\n        sat_id: np.array(velocities_temp[sat_id]) for sat_id in satellite_ids\n    }\n    latitudes: dict[str, np.ndarray] = {\n        sat_id: np.array(latitudes_list[sat_id]) for sat_id in satellite_ids\n    }\n    longitudes: dict[str, np.ndarray] = {\n        sat_id: np.array(longitudes_list[sat_id]) for sat_id in satellite_ids\n    }\n    altitudes: dict[str, np.ndarray] = {\n        sat_id: np.array(altitudes_list[sat_id]) for sat_id in satellite_ids\n    }\n    triangle_area_series = np.array(triangle_area_series_list)\n    triangle_aspect_series = np.array(triangle_aspect_series_list)\n    triangle_sides_series = np.array(triangle_sides_series_list)\n    centroid_latitudes = np.array(centroid_latitudes_list)\n    centroid_longitudes = np.array(centroid_longitudes_list)\n    centroid_altitudes = np.array(centroid_altitudes_list)\n    max_ground_distance = np.array(max_ground_distance_list)\n    min_command_distance = np.array(min_command_distance_list)\n\n    classical_series = _compute_classical_elements_series(positions, velocities)\n\n    finite_mask = np.isfinite(min_command_distance)\n    min_command_distance_km = np.where(\n        finite_mask, min_command_distance / 1_000.0, np.inf\n    )\n\n    centre_index = int(np.argmin(np.abs(offsets)))\n    orbital_elements = {}\n    for sat_id in satellite_ids:\n        elements = cartesian_to_classical(\n            positions[sat_id][centre_index], velocities[sat_id][centre_index]\n        )\n        orbital_elements[sat_id] = {\n            \"semi_major_axis_km\": elements.semi_major_axis / 1_000.0,\n            \"eccentricity\": elements.eccentricity,\n            \"inclination_deg\": _normalise_degrees(elements.inclination),\n            \"raan_deg\": _normalise_degrees(elements.raan),\n            \"argument_of_perigee_deg\": _normalise_degrees(elements.arg_perigee),\n            \"mean_anomaly_deg\": _normalise_degrees(elements.mean_anomaly),\n            \"assigned_plane\": plane_allocations.get(sat_id),\n        }\n\n    triangle_stats = _summarise_triangle_metrics(\n        triangle_area_series,\n        triangle_aspect_series,\n        triangle_sides_series,\n    )\n    ground_stats = _summarise_ground_metrics(max_ground_distance, formation)\n    maintenance = _estimate_maintenance_delta_v(\n        positions,\n        times,\n        formation,\n        semi_major_axis_m,\n    )\n    command_latency = _analyse_command_latency(\n        min_command_distance_km,\n        times,\n        formation,\n        semi_major_axis_m,\n        command_lat,\n        command_lon,\n    )\n    injection_recovery, injection_samples = _run_injection_recovery_monte_carlo(\n        satellite_ids,\n        formation,\n    )\n    drag_dispersion, drag_samples = _run_atmospheric_drag_dispersion_monte_carlo(\n        formation,\n        semi_major_axis_m,\n        inclination,\n    )\n\n    window, window_series = _formation_window(\n        triangle_aspect_series,\n        max_ground_distance,\n        time_step_s,\n        formation,\n        times,\n    )\n\n    recurrence = _summarise_window_recurrence(\n        window_series,\n        semi_major_axis_m,\n    )\n\n    station_keeping = _assess_station_keeping(\n        times,\n        triangle_sides_series,\n        formation,\n        maintenance,\n        time_step_s,\n    )\n\n    metrics: MutableMapping[str, object] = {\n        \"triangle\": triangle_stats,\n        \"ground_track\": ground_stats,\n        \"formation_window\": window,\n        \"formation_windows\": window_series,\n        \"formation_recurrence\": recurrence,\n        \"orbital_elements\": {\n            \"per_satellite\": orbital_elements,\n            \"plane_assignments\": plane_allocations,\n            \"time_series\": {\n                \"fields\": CLASSICAL_ELEMENT_FIELDS,\n                \"units\": {\n                    \"semi_major_axis_km\": \"km\",\n                    \"eccentricity\": \"\",\n                    \"inclination_deg\": \"deg\",\n                    \"raan_deg\": \"deg\",\n                    \"argument_of_perigee_deg\": \"deg\",\n                    \"mean_anomaly_deg\": \"deg\",\n                },\n                \"artefact_key\": \"orbital_elements_csv\",\n                \"per_satellite_directory_key\": \"orbital_elements_directory\",\n            },\n        },\n        \"maintenance\": maintenance,\n        \"station_keeping\": station_keeping,\n        \"command_latency\": command_latency,\n        \"injection_recovery\": injection_recovery,\n        \"drag_dispersion\": drag_dispersion,\n    }\n\n    artefacts: MutableMapping[str, Optional[str]] = {\n        \"summary_path\": None,\n        \"stk_directory\": None,\n        \"maintenance_csv\": None,\n        \"command_windows_csv\": None,\n        \"injection_recovery_csv\": None,\n        \"injection_recovery_plot\": None,\n        \"drag_dispersion_csv\": None,\n        \"formation_windows_csv\": None,\n        \"station_keeping_csv\": None,\n        \"orbital_elements_csv\": None,\n        \"orbital_elements_directory\": None,\n    }\n\n    result = TriangleFormationResult(\n        times=times,\n        positions_m=positions,\n        velocities_mps=velocities,\n        classical_elements=classical_series,\n        latitudes_rad=latitudes,\n        longitudes_rad=longitudes,\n        altitudes_m=altitudes,\n        triangle_area_m2=triangle_area_series,\n        triangle_aspect_ratio=triangle_aspect_series,\n        triangle_sides_m=triangle_sides_series,\n        centroid_lat_rad=centroid_latitudes,\n        centroid_lon_rad=centroid_longitudes,\n        centroid_alt_m=centroid_altitudes,\n        max_ground_distance_km=max_ground_distance,\n        min_command_distance_km=min_command_distance_km,\n        metrics=metrics,\n        artefacts=artefacts,\n    )\n\n    if output_directory is not None:\n        output_path = Path(output_directory)\n        output_path.mkdir(parents=True, exist_ok=True)\n        summary_path = output_path / \"triangle_summary.json\"\n        artefacts[\"summary_path\"] = str(summary_path)\n\n        maintenance_rows = [\n            {\n                \"satellite_id\": sat_id,\n                \"mean_diff_accel_mps2\": data[\"mean_diff_accel_mps2\"],\n                \"peak_diff_accel_mps2\": data[\"peak_diff_accel_mps2\"],\n                \"delta_v_per_burn_mps\": data[\"delta_v_per_burn_mps\"],\n                \"annual_delta_v_mps\": data[\"annual_delta_v_mps\"],\n            }\n            for sat_id, data in maintenance[\"per_spacecraft\"].items()\n        ]\n        maintenance_df = pd.DataFrame(\n            maintenance_rows,\n            columns=[\n                \"satellite_id\",\n                \"mean_diff_accel_mps2\",\n                \"peak_diff_accel_mps2\",\n                \"delta_v_per_burn_mps\",\n                \"annual_delta_v_mps\",\n            ],\n        )\n        maintenance_path = output_path / \"maintenance_summary.csv\"\n        maintenance_df.to_csv(maintenance_path, index=False)\n        artefacts[\"maintenance_csv\"] = str(maintenance_path)\n\n        command_rows = [\n            {\"window_index\": index, **window}\n            for index, window in enumerate(command_latency[\"contact_windows\"])\n        ]\n        command_df = pd.DataFrame(\n            command_rows, columns=[\"window_index\", \"start\", \"end\", \"duration_s\"]\n        )\n        command_path = output_path / \"command_windows.csv\"\n        command_df.to_csv(command_path, index=False)\n        artefacts[\"command_windows_csv\"] = str(command_path)\n\n        injection_path = output_path / \"injection_recovery.csv\"\n        injection_samples.to_csv(\n            injection_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"satellite_id\",\n                \"position_error_m\",\n                \"velocity_error_mps\",\n                \"delta_v_mps\",\n                \"success\",\n            ],\n        )\n        artefacts[\"injection_recovery_csv\"] = str(injection_path)\n\n        drag_path = output_path / \"drag_dispersion.csv\"\n        drag_samples.to_.csv(\n            drag_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"density_scale\",\n                \"drag_coefficient\",\n                \"ballistic_coefficient_m2_per_kg\",\n                \"semi_major_axis_delta_m\",\n                \"altitude_delta_m\",\n                \"along_track_shift_km\",\n                \"ground_distance_delta_km\",\n                \"command_distance_delta_km\",\n                \"within_tolerance\",\n            ],\n        )\n        artefacts[\"drag_dispersion_csv\"] = str(drag_path)\n\n        windows_path = output_path / \"formation_windows.csv\"\n        _write_formation_windows_csv(windows_path, window_series)\n        artefacts[\"formation_windows_csv\"] = str(windows_path)\n\n        station_path = output_path / \"station_keeping_events.csv\"\n        _write_station_keeping_csv(station_path, station_keeping)\n        artefacts[\"station_keeping_csv\"] = str(station_path)\n\n        plot_path = output_path / \"injection_recovery_cdf.svg\"\n        _write_injection_recovery_plot(injection_samples, plot_path)\n        if plot_path.exists():\n            artefacts[\"injection_recovery_plot\"] = str(plot_path)\n\n        orbital_path = output_path / \"orbital_elements.csv\"\n        _write_orbital_elements_csv(orbital_path, times, classical_series)\n        artefacts[\"orbital_elements_csv\"] = str(orbital_path)\n        metrics[\"orbital_elements\"][\"time_series\"][\"artefact\"] = str(orbital_path)\n        orbital_sat_dir = output_path / \"orbital_elements\"\n        orbital_sat_dir.mkdir(parents=True, exist_ok=True)\n        per_sat_paths = _write_orbital_elements_per_spacecraft(\n            orbital_sat_dir, times, classical_series\n        )\n        if per_sat_paths:\n            artefacts[\"orbital_elements_directory\"] = str(orbital_sat_dir)\n            metrics[\"orbital_elements\"][\"time_series\"][\"per_satellite_files\"] = {\n                sat_id: str(path)\n                for sat_id, path in sorted(per_sat_paths.items(), key=lambda item: item[0])\n            }\n\n        stk_dir = output_path / \"stk\"\n        _export_to_stk(\n            result,\n            stk_dir,\n            scenario_name=str(metadata.get(\"scenario_name\", \"Tehran Triangle Formation\")),\n        )\n        artefacts[\"stk_directory\"] = str(stk_dir)\n\n        summary_payload = result.to_summary()\n        summary_path.write_text(\n            json.dumps(summary_payload, indent=2), encoding=\"utf-8\"\n        )\n\n    return result",
            "old_string": "def simulate_triangle_formation(\n    config_source: Mapping[str, object] | Path | str,\n    output_directory: Optional[Path | str] = None,\n) -> TriangleFormationResult:\n    \"\"\"Simulate the triangular formation described by *config_source*.\"\"\"\n\n    configuration = _load_configuration(config_source)\n    formation = configuration[\"formation\"]\n    metadata = configuration.get(\"metadata\", {})\n\n    satellite_elements: dict[str, OrbitalElements] = {}\n    if \"satellites\" in configuration:\n        satellite_ids = [sat[\"id\"] for sat in configuration[\"satellites\"]]\n        plane_allocations = {\n            sat[\"id\"]: sat[\"plane\"] for sat in configuration[\"satellites\"]\n        }\n        epoch = _parse_time(\n            configuration[\"satellites\"][0][\"orbital_elements\"][\"epoch_utc\"]\n        )\n        semi_major_axis_m = (\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"][\"semi_major_axis_km\"]\n            )\n            * 1_000.0\n        )\n        inclination = math.radians(\n            float(\n                configuration[\"satellites\"][0][\"orbital_elements\"].get(\n                    \"inclination_deg\", 0.0\n                )\n            )\n        )\n\n        for sat_config in configuration[\"satellites\"]:\n            elements = sat_config[\"orbital_elements\"]\n            satellite_elements[sat_config[\"id\"]] = OrbitalElements(\n                semi_major_axis=float(elements[\"semi_major_axis_km\"]) * 1_000.0,\n                eccentricity=float(elements.get(\"eccentricity\", 0.0)),\n                inclination=math.radians(float(elements.get(\"inclination_deg\", 0.0))),\n                raan=math.radians(float(elements.get(\"raan_deg\", 0.0))),\n                arg_perigee=math.radians(\n                    float(elements.get(\"argument_of_perigee_deg\", 0.0))\n                ),\n                mean_anomaly=math.radians(float(elements.get(\"mean_anomaly_deg\", 0.0))),\n            )\n    else:\n        reference = configuration[\"reference_orbit\"]\n        epoch = _parse_time(reference[\"epoch_utc\"])\n        semi_major_axis_m = float(reference[\"semi_major_axis_km\"]) * 1_000.0\n        eccentricity = float(reference.get(\"eccentricity\", 0.0))\n        inclination = math.radians(float(reference.get(\"inclination_deg\", 0.0)))\n        raan = math.radians(float(reference.get(\"raan_deg\", 0.0)))\n        arg_perigee = math.radians(\n            float(reference.get(\"argument_of_perigee_deg\", 0.0))\n        )\n        mean_anomaly = math.radians(float(reference.get(\"mean_anomaly_deg\", 0.0)))\n\n        reference_elements = OrbitalElements(\n            semi_major_axis=semi_major_axis_m,\n            eccentricity=eccentricity,\n            inclination=inclination,\n            raan=raan,\n            arg_perigee=arg_perigee,\n            mean_anomaly=mean_anomaly,\n        )\n        offsets_m = _formation_offsets(float(formation[\"side_length_m\"]))\n        satellite_ids = tuple(sorted(offsets_m))\n\n        plane_allocations = formation.get(\"plane_allocations\", {})\n        if not plane_allocations:\n            plane_allocations = {\n                satellite_ids[0]: \"Plane A\",\n                satellite_ids[1]: \"Plane A\",\n                satellite_ids[2]: \"Plane B\",\n            }\n\n        # Get initial state of the reference orbit\n        ref_pos_at_epoch, ref_vel_at_epoch = propagate_kepler(reference_elements, 0)\n\n        # Get initial LVLH frame\n        frame_at_epoch = _lvlh_frame(ref_pos_at_epoch, ref_vel_at_epoch)\n\n        # Calculate initial orbital elements for each satellite\n        for sat_id in satellite_ids:\n            offset_vec = frame_at_epoch @ offsets_m[sat_id]\n            # For a stable formation with zero radial offset, the initial relative velocity in the LVLH frame should be zero.\n            offset_vel = np.array([0.0, 0.0, 0.0])\n\n            sat_pos = ref_pos_at_epoch + offset_vec\n            sat_vel = ref_vel_at_epoch + offset_vel\n            satellite_elements[sat_id] = cartesian_to_classical(sat_pos, sat_vel)\n\n    duration_s = float(formation.get(\"duration_s\", 180.0))\n    time_step_s = float(formation.get(\"time_step_s\", 1.0))\n    half_duration = 0.5 * duration_s\n    sample_count = int(round(duration_s / time_step_s)) + 1\n\n    offsets = np.arange(sample_count, dtype=float) * time_step_s - half_duration\n    times = [epoch + timedelta(seconds=float(offset)) for offset in offsets]\n\n    positions: dict[str, np.ndarray] = {\n        sat_id: np.zeros((sample_count, 3), dtype=float) for sat_id in satellite_ids\n    }\n    latitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    longitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n    altitudes: dict[str, np.ndarray] = {\n        sat_id: np.zeros(sample_count, dtype=float) for sat_id in satellite_ids\n    }\n\n    triangle_area_series = np.zeros(sample_count, dtype=float)\n    triangle_aspect_series = np.zeros(sample_count, dtype=float)\n    triangle_sides_series = np.zeros((sample_count, 3), dtype=float)\n    centroid_positions = np.zeros((sample_count, 3), dtype=float)\n    centroid_latitudes = np.zeros(sample_count, dtype=float)\n    centroid_longitudes = np.zeros(sample_count, dtype=float)\n    centroid_altitudes = np.zeros(sample_count, dtype=float)\n    max_ground_distance = np.zeros(sample_count, dtype=float)\n    min_command_distance = np.full(sample_count, np.inf, dtype=float)\n\n    target = formation.get(\"target\", {})\n    target_lat = math.radians(float(target.get(\"latitude_deg\", 0.0)))\n    target_lon = math.radians(float(target.get(\"longitude_deg\", 0.0)))\n\n    command = formation.get(\"command\", {})\n    command_station = command.get(\"station\", {})\n    default_lat_deg = math.degrees(target_lat)\n    default_lon_deg = math.degrees(target_lon)\n    command_lat = math.radians(float(command_station.get(\"latitude_deg\", default_lat_deg)))\n    command_lon = math.radians(float(command_station.get(\"longitude_deg\", default_lon_deg)))\n\n    # Propagate each satellite independently\n    velocities_temp: dict[str, list[np.ndarray]] = {sat_id: [] for sat_id in satellite_ids}\n\n    perturbations = formation.get(\"perturbations\", {})\n    position_noise_sigma_m = float(perturbations.get(\"position_noise_sigma_m\", 0.0))\n\n    # Seed the random number generator for reproducibility\n    seed = perturbations.get(\"seed\")\n    rng = np.random.default_rng(seed)\n\n    from tqdm import tqdm\n    for index, delta_t in enumerate(tqdm(offsets, desc=\"Propagating orbits\")):\n        inertial_positions = {}\n        for sat_id in satellite_ids:\n            pos, vel = propagate_kepler(satellite_elements[sat_id], delta_t)\n\n            # if position_noise_sigma_m > 0.0:\n            #     noise = rng.normal(0.0, position_noise_sigma_m, size=3)\n            #     pos += noise\n\n            positions[sat_id][index] = pos\n            velocities_temp[sat_id].append(vel)\n            inertial_positions[sat_id] = pos\n\n            ecef = inertial_to_ecef(pos, times[index])\n            lat, lon, alt = geodetic_coordinates(ecef)\n            latitudes[sat_id][index] = lat\n            longitudes[sat_id][index] = lon\n            altitudes[sat_id][index] = alt\n\n        # Now, compute triangle geometry based on the actual positions\n        # The order of vertices matters for side length calculations, so sort by sat_id\n        vertices = [inertial_positions[sat_id] for sat_id in satellite_ids]\n\n        triangle_area_series[index] = triangle_area(vertices)\n        triangle_aspect_series[index] = triangle_aspect_ratio(vertices)\n        triangle_sides_series[index] = triangle_side_lengths(vertices)\n\n        centroid = sum(vertices) / len(vertices)\n        centroid_positions[index] = centroid\n        centroid_ecef = inertial_to_ecef(centroid, times[index])\n        centroid_lat, centroid_lon, centroid_alt = geodetic_coordinates(centroid_ecef)\n        centroid_latitudes[index] = centroid_lat\n        centroid_longitudes[index] = centroid_lon\n        centroid_altitudes[index] = centroid_alt\n\n        max_distance = 0.0\n        min_command = np.inf\n        for sat_id in satellite_ids:\n            distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                target_lat,\n                target_lon,\n            )\n            max_distance = max(max_distance, distance)\n            command_distance = haversine_distance(\n                latitudes[sat_id][index],\n                longitudes[sat_id][index],\n                command_lat,\n                command_lon,\n            )\n            min_command = min(min_command, command_distance)\n        max_ground_distance[index] = max_distance / 1_000.0\n        min_command_distance[index] = min_command\n\n    velocities: dict[str, np.ndarray] = {\n        sat_id: np.array(velocities_temp[sat_id]) for sat_id in satellite_ids\n    }\n\n    classical_series = _compute_classical_elements_series(positions, velocities)\n\n    finite_mask = np.isfinite(min_command_distance)\n    min_command_distance_km = np.where(\n        finite_mask, min_command_distance / 1_000.0, np.inf\n    )\n\n    centre_index = int(np.argmin(np.abs(offsets)))\n    orbital_elements = {}\n    for sat_id in satellite_ids:\n        elements = cartesian_to_classical(\n            positions[sat_id][centre_index], velocities[sat_id][centre_index]\n        )\n        orbital_elements[sat_id] = {\n            \"semi_major_axis_km\": elements.semi_major_axis / 1_000.0,\n            \"eccentricity\": elements.eccentricity,\n            \"inclination_deg\": _normalise_degrees(elements.inclination),\n            \"raan_deg\": _normalise_degrees(elements.raan),\n            \"argument_of_perigee_deg\": _normalise_degrees(elements.arg_perigee),\n            \"mean_anomaly_deg\": _normalise_degrees(elements.mean_anomaly),\n            \"assigned_plane\": plane_allocations.get(sat_id),\n        }\n\n    triangle_stats = _summarise_triangle_metrics(\n        triangle_area_series,\n        triangle_aspect_series,\n        triangle_sides_series,\n    )\n    ground_stats = _summarise_ground_metrics(max_ground_distance, formation)\n    maintenance = _estimate_maintenance_delta_v(\n        positions,\n        times,\n        formation,\n        semi_major_axis_m,\n    )\n    command_latency = _analyse_command_latency(\n        min_command_distance_km,\n        times,\n        formation,\n        semi_major_axis_m,\n        command_lat,\n        command_lon,\n    )\n    injection_recovery, injection_samples = _run_injection_recovery_monte_carlo(\n        satellite_ids,\n        formation,\n    )\n    drag_dispersion, drag_samples = _run_atmospheric_drag_dispersion_monte_carlo(\n        formation,\n        semi_major_axis_m,\n        inclination,\n    )\n\n    window, window_series = _formation_window(\n        triangle_aspect_series,\n        max_ground_distance,\n        time_step_s,\n        formation,\n        times,\n    )\n\n    recurrence = _summarise_window_recurrence(\n        window_series,\n        semi_major_axis_m,\n    )\n\n    station_keeping = _assess_station_keeping(\n        times,\n        triangle_sides_series,\n        formation,\n        maintenance,\n        time_step_s,\n    )\n\n    metrics: MutableMapping[str, object] = {\n        \"triangle\": triangle_stats,\n        \"ground_track\": ground_stats,\n        \"formation_window\": window,\n        \"formation_windows\": window_series,\n        \"formation_recurrence\": recurrence,\n        \"orbital_elements\": {\n            \"per_satellite\": orbital_elements,\n            \"plane_assignments\": plane_allocations,\n            \"time_series\": {\n                \"fields\": CLASSICAL_ELEMENT_FIELDS,\n                \"units\": {\n                    \"semi_major_axis_km\": \"km\",\n                    \"eccentricity\": \"\",\n                    \"inclination_deg\": \"deg\",\n                    \"raan_deg\": \"deg\",\n                    \"argument_of_perigee_deg\": \"deg\",\n                    \"mean_anomaly_deg\": \"deg\",\n                },\n                \"artefact_key\": \"orbital_elements_csv\",\n                \"per_satellite_directory_key\": \"orbital_elements_directory\",\n            },\n        },\n        \"maintenance\": maintenance,\n        \"station_keeping\": station_keeping,\n        \"command_latency\": command_latency,\n        \"injection_recovery\": injection_recovery,\n        \"drag_dispersion\": drag_dispersion,\n    }\n\n    artefacts: MutableMapping[str, Optional[str]] = {\n        \"summary_path\": None,\n        \"stk_directory\": None,\n        \"maintenance_csv\": None,\n        \"command_windows_csv\": None,\n        \"injection_recovery_csv\": None,\n        \"injection_recovery_plot\": None,\n        \"drag_dispersion_csv\": None,\n        \"formation_windows_csv\": None,\n        \"station_keeping_csv\": None,\n        \"orbital_elements_csv\": None,\n        \"orbital_elements_directory\": None,\n    }\n\n    result = TriangleFormationResult(\n        times=times,\n        positions_m=positions,\n        velocities_mps=velocities,\n        classical_elements=classical_series,\n        latitudes_rad=latitudes,\n        longitudes_rad=longitudes,\n        altitudes_m=altitudes,\n        triangle_area_m2=triangle_area_series,\n        triangle_aspect_ratio=triangle_aspect_series,\n        triangle_sides_m=triangle_sides_series,\n        centroid_lat_rad=centroid_latitudes,\n        centroid_lon_rad=centroid_longitudes,\n        centroid_alt_m=centroid_altitudes,\n        max_ground_distance_km=max_ground_distance,\n        min_command_distance_km=min_command_distance_km,\n        metrics=metrics,\n        artefacts=artefacts,\n    )\n\n    if output_directory is not None:\n        output_path = Path(output_directory)\n        output_path.mkdir(parents=True, exist_ok=True)\n        summary_path = output_path / \"triangle_summary.json\"\n        artefacts[\"summary_path\"] = str(summary_path)\n\n        maintenance_rows = [\n            {\n                \"satellite_id\": sat_id,\n                \"mean_diff_accel_mps2\": data[\"mean_diff_accel_mps2\"],\n                \"peak_diff_accel_mps2\": data[\"peak_diff_accel_mps2\"],\n                \"delta_v_per_burn_mps\": data[\"delta_v_per_burn_mps\"],\n                \"annual_delta_v_mps\": data[\"annual_delta_v_mps\"],\n            }\n            for sat_id, data in maintenance[\"per_spacecraft\"].items()\n        ]\n        maintenance_df = pd.DataFrame(\n            maintenance_rows,\n            columns=[\n                \"satellite_id\",\n                \"mean_diff_accel_mps2\",\n                \"peak_diff_accel_mps2\",\n                \"delta_v_per_burn_mps\",\n                \"annual_delta_v_mps\",\n            ],\n        )\n        maintenance_path = output_path / \"maintenance_summary.csv\"\n        maintenance_df.to_csv(maintenance_path, index=False)\n        artefacts[\"maintenance_csv\"] = str(maintenance_path)\n\n        command_rows = [\n            {\"window_index\": index, **window}\n            for index, window in enumerate(command_latency[\"contact_windows\"])\n        ]\n        command_df = pd.DataFrame(\n            command_rows, columns=[\"window_index\", \"start\", \"end\", \"duration_s\"]\n        )\n        command_path = output_path / \"command_windows.csv\"\n        command_df.to_csv(command_path, index=False)\n        artefacts[\"command_windows_csv\"] = str(command_path)\n\n        injection_path = output_path / \"injection_recovery.csv\"\n        injection_samples.to_csv(\n            injection_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"satellite_id\",\n                \"position_error_m\",\n                \"velocity_error_mps\",\n                \"delta_v_mps\",\n                \"success\",\n            ],\n        )\n        artefacts[\"injection_recovery_csv\"] = str(injection_path)\n\n        drag_path = output_path / \"drag_dispersion.csv\"\n        drag_samples.to_csv(\n            drag_path,\n            index=False,\n            columns=[\n                \"sample_id\",\n                \"density_scale\",\n                \"drag_coefficient\",\n                \"ballistic_coefficient_m2_per_kg\",\n                \"semi_major_axis_delta_m\",\n                \"altitude_delta_m\",\n                \"along_track_shift_km\",\n                \"ground_distance_delta_km\",\n                \"command_distance_delta_km\",\n                \"within_tolerance\",\n            ],\n        )\n        artefacts[\"drag_dispersion_csv\"] = str(drag_path)\n\n        windows_path = output_path / \"formation_windows.csv\"\n        _write_formation_windows_csv(windows_path, window_series)\n        artefacts[\"formation_windows_csv\"] = str(windows_path)\n\n        station_path = output_path / \"station_keeping_events.csv\"\n        _write_station_keeping_csv(station_path, station_keeping)\n        artefacts[\"station_keeping_csv\"] = str(station_path)\n\n        plot_path = output_path / \"injection_recovery_cdf.svg\"\n        _write_injection_recovery_plot(injection_samples, plot_path)\n        if plot_path.exists():\n            artefacts[\"injection_recovery_plot\"] = str(plot_path)\n\n        orbital_path = output_path / \"orbital_elements.csv\"\n        _write_orbital_elements_csv(orbital_path, times, classical_series)\n        artefacts[\"orbital_elements_csv\"] = str(orbital_path)\n        metrics[\"orbital_elements\"][\"time_series\"][\"artefact\"] = str(orbital_path)\n        orbital_sat_dir = output_path / \"orbital_elements\"\n        orbital_sat_dir.mkdir(parents=True, exist_ok=True)\n        per_sat_paths = _write_orbital_elements_per_spacecraft(\n            orbital_sat_dir, times, classical_series\n        )\n        if per_sat_paths:\n            artefacts[\"orbital_elements_directory\"] = str(orbital_sat_dir)\n            metrics[\"orbital_elements\"][\"time_series\"][\"per_satellite_files\"] = {\n                sat_id: str(path)\n                for sat_id, path in sorted(per_sat_paths.items(), key=lambda item: item[0])\n            }\n\n        stk_dir = output_path / \"stk\"\n        _export_to_stk(\n            result,\n            stk_dir,\n            scenario_name=str(metadata.get(\"scenario_name\", \"Tehran Triangle Formation\")),\n        )\n        artefacts[\"stk_directory\"] = str(stk_dir)\n\n        summary_payload = result.to_summary()\n        summary_path.write_text(\n            json.dumps(summary_payload, indent=2), encoding=\"utf-8\"\n        )\n\n    return result",
            "file_path": "/home/diannenewton170/formation-sat-2/sim/formation/triangle.py",
            "instruction": "Optimize the simulate_triangle_formation function by using lists for data collection inside the loop and converting them to numpy arrays after the loop. This will reduce the initial memory allocation and allow the progress bar to be displayed for long simulations."
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "replace-1762794632011-8888cc1dc99f7",
          "name": "replace",
          "response": {
            "output": "Successfully modified file: /home/diannenewton170/formation-sat-2/sim/formation/triangle.py (1 replacements)."
          }
        }
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "thoughtSignature": "CisB4/H/XvAdSpDApCwT2GJG7X5RNUOa05dgF3QIPUSQPHhcTBc1c1Pt04GTCnEB4/H/XmQxmgdIxTqoz6YCi14escafGPvp42DxEuM0SPHpCAAOgiudFFP9HaszA7mvCYa5d4EAmr4E2qr7dviKb+UHp7hosrC6qy/ZCpkAaMnaA3pXq8TxbgjNR84/MdCU87j83RCs7U2FGjYM4kpTaAptAePx/14kW5XnrE597ZV/bwajRplU0Tix/4pWIaS6q4yZipTw3zRNnIy+/ECRdBjalYW+Wt4WHZkP0LMnxZCh+Ry8O1aRTcLXatqVrD0m107TzLeQz8GzdrVU/ZfYUK23jx8z5B8EQ9oVqeGEngrNAQHj8f9ejP6uAr6YtrR2Et4+o/FckGpKWPYKERGbdzZuh/41Fkyn+zyIMuIUS+KpDmJa/GgK9Im/skmWmKiNMIFltCeR7szzPA8JhaSdS+Q42S1SN2Lpl05m6zi73zc62X/fe1uS//ZLekle581d9RHdudtawRe8HuR6LdIFqhxshhxXqojkTJ2ingEh9lZNf2vc2hMDuNMhLm2POzawKjMnqIooeSZg1gN2SJhY4u4Ax9yMHxK3doT1nq99uXJI5aZLQ9eazb4D7nOyOcoKpwEB4/H/XkeB4T4O2CsTdlHKq//IJ7qMsl4uTuhQASLxs5pwq82CjDAL73cIgs/Xo3gVxb4LlhTOqGte0W2rGpZDRwXEdl3NsQxffvHuMdblDMhamcy5Ygm0aS3IbaK1gZYNfrnRoh2fsGMqLzLnzUEQ0AFF4ZXhNCrQBE3erLm/SjKh4nhZwo3Q07Z6q5cRvABv8Wh4H8/nR1e+8bWG0Z6k4pM5Ve3iQwraAQHj8f9ewTsTmGmi83lCUcOErtpoSt3r8laRjg0VEZcWtZgVxN/nDCSlYsdtTxvDE9y4KP1TRkAsGm7q2101c5tTxdUgzT4ntByMbkhAwX5WmdAAAGqZaxiVkyHnLNbizuheaWUiyZ/i02Cv8uGjZ65qgwqBJYcj6RWeDsHixQGWh0kiuHcOlJuVUqqSaNCR8QYSMdlwVkQbSbSGOzVe//cqAbCzB+aDjeTXbIf3uIIYXQJ/SuhU4nv7oi8ot2955zAo5XJM6OgmltvDjRx/897ES0FOJ5rTPbxkCi8B4/H/XpkRlkImXrMhrIuRr4Z/FrTGB0aiJpF0rJZI+RauFFCd2Tp+lOBwTFf9Fw==",
        "text": "I've optimized `simulate_triangle_formation` in `sim/formation/triangle.py` to reduce memory usage by using lists for data collection and converting to numpy arrays after the loop, which should enable the progress bar. I'm rerunning the 14-day simulation with the same command: `.venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day`. If successful, I'll analyze the results and create the requested markdown file."
      },
      {
        "functionCall": {
          "name": "run_shell_command",
          "args": {
            "description": "Running the 14-day triangle formation simulation again with the optimized script.",
            "command": ".venv/bin/python -m sim.scripts.run_triangle --config config/scenarios/tehran_triangle_14_day.json --output-dir artefacts/gemini_run_14_day"
          }
        }
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "id": "run_shell_command-1762794644914-644eb1b68b813",
          "name": "run_shell_command",
          "response": {
            "error": "[Operation Cancelled] Reason: User cancelled tool execution."
          }
        }
      }
    ]
  }
]